#!/usr/bin/env python3
"""Translates a source file using a translation model."""
import argparse
import logging
import tensorflow as tf
import inference
import model_loader
import models
import numpy
import time
import util
from constants import DataType
from config import load_config_from_json_file
from settings import TranslationSettings
from sampling_utils import SamplingUtils
from data_iterator import SimpleFileWrapper



def translate_file(input_file, output_file, session, model, config,
                   beam_size=12, nbest=False, normalization_alpha=1.0):
    """Translates a source file using a translation model (or ensemble).
    Note that Batch translation is not supported here.
    Args:
        input_file: file object from which source sentences will be read.
        output_file: file object to which translations will be written.
        session: TensorFlow session.
        models: list of model objects to use for beam search.
        configs: model configs.
        beam_size: beam width.
        nbest: if True, produce n-best output with scores; otherwise 1-best.
        normalization_alpha: alpha parameter for length normalization.
    """

    def translate_sents(sents, model_set, num_to_target, n_prev_translated, add_p_tag=True):
        """Translates an individual maxibatch.

        Args:
            sents: a list of sentences
            model_set: an InferenceModelSet object.
            num_to_target: dictionary mapping target vocabulary IDs to strings.
        """
        x = util.read_all_lines(config, sents)
        y_dummy = numpy.zeros(shape=(len(x), len(x[0]), 1))
        flag = DataType.DATA_FOR_NMT if add_p_tag else DataType.DATA_FOR_VANILLA_NMT
        x, x_mask, _, _ = util.data_prepare_func(flag, x, y_dummy, config.factors, maxlen=None)
        sample = model_set.decode(session=session,
                                  x=x,
                                  x_mask=x_mask,
                                  beam_size=beam_size,
                                  normalization_alpha=normalization_alpha)
        n_prev_translated = n_prev_translated + len(sample)
        logging.info('Translated {} sents'.format(n_prev_translated))

        # Write the translation for each sentence to the output file.
        # all the sentecnes shall be wrappered in a section
        if add_p_tag:
            output_file.write('<p>' + '\n')
        for i, beam in enumerate(sample):
            # beam contains k-best result
            if nbest:
                num = n_prev_translated + i
                for sent, cost in beam:
                    translation = util.seq2words(sent, num_to_target)
                    line = "{} ||| {} ||| {}\n".format(num, translation,
                                                       str(cost))
                    output_file.write(line)
            else:
                best_hypo, cost = beam[0]
                line = util.seq2words(best_hypo, num_to_target) + '\n'
                output_file.write(line)
        if add_p_tag:
            output_file.write('</p>' + '\n')

    _, _, _, num_to_target = util.load_dictionaries(config)
    model_set = inference.InferenceModelSet(model, config)
    logging.info("NOTE: Length of translations is capped to {}".format(config.translation_maxlen))

    start_time = time.time()
    n_sent_translated = 0
    is_vanilla = config.mode == 'baseline-nmt'
    for sents in SimpleFileWrapper(input_file, seg_file=not is_vanilla):
        translate_sents(sents, model_set, num_to_target, n_sent_translated, add_p_tag=not is_vanilla)  # vanilla-nmt doesn't need p-tag
        n_sent_translated += len(sents)
    duration = time.time() - start_time
    logging.info('Translated {} sents in {} sec. Speed {} sents/sec'.format(
        n_sent_translated, duration, n_sent_translated/duration))


def main(settings):
    """
    Translates a source language file (or STDIN) into a target language file
    (or STDOUT).
    """
    # Start logging.
    level = logging.DEBUG if settings.verbose else logging.INFO
    logging.basicConfig(level=level, format='%(levelname)s: %(message)s')

    # Create the TensorFlow session.
    tf_config = tf.ConfigProto()
    tf_config.allow_soft_placement = True
    session = tf.Session(config=tf_config)

    config = load_config_from_json_file(settings.model)
    setattr(config, 'reload', settings.model)

    # Create the model graphs and restore their variables.
    logging.debug("Loading model\n")
    model = models.ContextAwareNMT(config)
    model_loader.init_or_restore_variables(config, session)
    model.sampling_utils = SamplingUtils(settings)

    # Translate the source file.
    translate_file(input_file=settings.input,
                   output_file=settings.output,
                   session=session,
                   model=model,
                   config=config,
                   beam_size=settings.beam_size,
                   nbest=settings.n_best,
                   normalization_alpha=settings.normalization_alpha)


if __name__ == "__main__":
    # Parse console arguments.
    settings = TranslationSettings(from_console_arguments=True)
    main(settings)
