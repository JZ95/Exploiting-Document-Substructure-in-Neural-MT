#!/usr/bin/env python3
'''
Build a neural machine translation model with soft attention
'''
from datetime import datetime
import json
import os
import logging
import sys
import time
import numpy
import tempfile
import tensorflow as tf
import model_loader
import util
from constants import DataType
from models import model_builder
from data_iterator import iter_builder
from model_updater import ModelUpdater
from validation_helper import validate_clf, validate_nmt, validate_with_script
from config import write_config_to_json_file, read_config_from_cmdline
from learning_schedule import ConstantSchedule

# Start logging.
level = logging.INFO
logging.basicConfig(level=level, format='%(levelname)s: %(message)s')

def load_data(config):
    logging.info('Reading data...')
    text_iterator = iter_builder(config)

    if config.valid_freq and config.valid_datasets:
        valid_text_iterator = iter_builder(config, is_valid_data=True)
    else:
        logging.info('no validation set loaded')
        valid_text_iterator = None

    logging.info('Done')
    return text_iterator, valid_text_iterator


def _train_clf(config, sess, text_iterator, valid_text_iterator, progress, updater, saver, replicas):
    clf_loss = 0.
    n_sents, n_words, n_batch = 0, 0, 0
    last_time = time.time()
    logging.info("Initial uidx={}".format(progress.uidx))

    for progress.eidx in range(progress.eidx, config.max_epochs):
        logging.info('Starting epoch {0}'.format(progress.eidx))
        for batch, flag in text_iterator:
            inputs, outputs = batch
            x_in, x_mask_in, y_in, y_mask_in = util.data_prepare_func(flag, inputs, outputs, config.factors, maxlen=None)

            write_summary_for_this_batch = config.summary_freq and ((progress.uidx % config.summary_freq == 0) or (config.finish_after and progress.uidx % config.finish_after == 0))
            loss = updater.update(sess, x_in, x_mask_in, y_in, y_mask_in, write_summary_for_this_batch, flag)

            clf_loss += loss
            n_words += int(numpy.sum(x_mask_in))
            n_sents += x_mask_in.shape[1]
            n_batch += x_mask_in.shape[-1]
            progress.uidx += 1

            if config.disp_freq and progress.uidx % config.disp_freq == 0:
                duration = time.time() - last_time
                disp_time = datetime.now().strftime('[%Y-%m-%d %H:%M:%S]')

                msg = '{0} Epoch: {1} Update: {2} ' \
                    'CLF-Loss ce/batch: {3:.5f} '\
                    'Words/sec: {4:.5f} Sents/sec: {5:.5f}'
            
                logging.info(msg.format(disp_time, progress.eidx, progress.uidx, 
                                        clf_loss/n_batch,
                                        n_words/duration, n_sents/duration))

                last_time = time.time()
                clf_loss = 0.
                n_sents, n_words, n_batch = 0, 0, 0

            if config.valid_freq and progress.uidx % config.valid_freq == 0:
                valid_score = validate_clf(sess, replicas[0], config, valid_text_iterator)
                valid_ce, valid_acc = valid_score
                if (len(progress.clf_errs) == 0 or
                    valid_ce < min(progress.clf_errs)):
                    progress.clf_errs.append(valid_ce)
                    progress.bad_counter = 0
                    save_non_checkpoint(sess, saver, config.saveto)
                    progress_path = '{0}.progress.json'.format(config.saveto)
                    progress.save_to_json(progress_path)
                else:
                    progress.clf_errs.append(valid_ce)
                    progress.bad_counter += 1
                    if progress.bad_counter > config.patience:
                        logging.info('Early Stop!')
                        progress.estop = True
                        break

                # decide wether save as the best-model or not
                need_to_save = ((len(progress.acc_scores) == 0 or 
                        valid_acc > max(progress.acc_scores)))
                progress.acc_scores.append(valid_acc)

                if need_to_save:
                    save_path = config.saveto + ".best-valid"
                    save_non_checkpoint(sess, saver, save_path)
                    write_config_to_json_file(config, save_path)

                    progress_path = '{}.progress.json'.format(save_path)
                    progress.save_to_json(progress_path)

            if config.save_freq and progress.uidx % config.save_freq == 0:
                saver.save(sess, save_path=config.saveto, global_step=progress.uidx)
                write_config_to_json_file(config, "%s-%s" % (config.saveto, progress.uidx))

                progress_path = '{0}-{1}.progress.json'.format(config.saveto, progress.uidx)
                progress.save_to_json(progress_path)

            if config.finish_after and progress.uidx % config.finish_after == 0:
                logging.info("Maximum number of updates reached")
                saver.save(sess, save_path=config.saveto, global_step=progress.uidx)
                write_config_to_json_file(config, "%s-%s" % (config.saveto, progress.uidx))

                progress.estop=True
                progress_path = '{0}-{1}.progress.json'.format(config.saveto, progress.uidx)
                progress.save_to_json(progress_path)
                break
        if progress.estop:
            break


def _train_nmt(config, sess, text_iterator, valid_text_iterator, progress, updater, saver, replicas):
    nmt_loss = 0.
    n_sents, n_words = 0, 0
    last_time = time.time()
    logging.info("Initial uidx={}".format(progress.uidx))

    for progress.eidx in range(progress.eidx, config.max_epochs):
        logging.info('Starting epoch {0}'.format(progress.eidx))
        for batch, flag in text_iterator:
            inputs, outputs = batch
            x_in, x_mask_in, y_in, y_mask_in = util.data_prepare_func(flag, inputs, outputs, config.factors, maxlen=None)
            write_summary_for_this_batch = config.summary_freq and ((progress.uidx % config.summary_freq == 0) or (config.finish_after and progress.uidx % config.finish_after == 0))
            loss = updater.update(sess, x_in, x_mask_in, y_in, y_mask_in, write_summary_for_this_batch, flag)

            nmt_loss += loss
            n_words += int(numpy.sum(y_mask_in))
            if flag == DataType.DATA_FOR_NMT:
                n_sents += y_mask_in.shape[0] * y_mask_in.shape[-1]
            elif flag == DataType.DATA_FOR_VANILLA_NMT:
                n_sents += y_mask_in.shape[-1]

            progress.uidx += 1

            if config.disp_freq and progress.uidx % config.disp_freq == 0:
                duration = time.time() - last_time
                disp_time = datetime.now().strftime('[%Y-%m-%d %H:%M:%S]')

                msg = '{0} Epoch: {1} Update: {2} ' \
                    'NMT-Loss ce/word: {3:.5f} '\
                    'Words/sec: {4:.5f} Sents/sec: {5:.5f}'

                logging.info(msg.format(disp_time, progress.eidx, progress.uidx, 
                                        nmt_loss/n_words,
                                        n_words/duration, n_sents/duration))

                last_time = time.time()
                nmt_loss = 0.
                n_sents, n_words = 0, 0

            if config.valid_freq and progress.uidx % config.valid_freq == 0:
                valid_ce = validate_nmt(sess, replicas[0], config, valid_text_iterator)
                if (len(progress.nmt_errs) == 0 or
                    valid_ce < min(progress.nmt_errs)):
                    progress.nmt_errs.append(valid_ce)
                    progress.bad_counter = 0
                    save_non_checkpoint(sess, saver, config.saveto)
                    progress_path = '{0}.progress.json'.format(config.saveto)
                    progress.save_to_json(progress_path)
                else:
                    progress.nmt_errs.append(valid_ce)
                    progress.bad_counter += 1
                    if progress.bad_counter > config.patience:
                        logging.info('Early Stop!')
                        progress.estop = True
                        break

                if config.valid_script is not None:
                    bleu_score = validate_with_script(sess, replicas[0], config)
                    need_to_save = (bleu_score is not None and
                        (len(progress.bleu_scores) == 0 or
                         bleu_score > max(progress.bleu_scores)))
                    if bleu_score is None:
                        bleu_score = 0.0  # ensure a valid value is written
                    progress.bleu_scores.append(bleu_score)

                    if need_to_save:
                        save_path = config.saveto + ".best-valid"
                        save_non_checkpoint(sess, saver, save_path)
                        write_config_to_json_file(config, save_path)

                        progress_path = '{}.progress.json'.format(save_path)
                        progress.save_to_json(progress_path)

            if config.save_freq and progress.uidx % config.save_freq == 0:
                saver.save(sess, save_path=config.saveto, global_step=progress.uidx)
                write_config_to_json_file(config, "%s-%s" % (config.saveto, progress.uidx))

                progress_path = '{0}-{1}.progress.json'.format(config.saveto, progress.uidx)
                progress.save_to_json(progress_path)

            if config.finish_after and progress.uidx % config.finish_after == 0:
                logging.info("Maximum number of updates reached")
                saver.save(sess, save_path=config.saveto, global_step=progress.uidx)
                write_config_to_json_file(config, "%s-%s" % (config.saveto, progress.uidx))

                progress.estop=True
                progress_path = '{0}-{1}.progress.json'.format(config.saveto, progress.uidx)
                progress.save_to_json(progress_path)
                break
        if progress.estop:
            break


def _train_joint(config, sess, text_iterator, valid_text_iterator, progress, updater, aux_updater, saver, replicas):
    nmt_loss, clf_loss = 0., 0.
    n_sents, n_words_for_loss, n_words, n_batch = 0, 0, 0, 0
    last_time = time.time()
    logging.info("Initial uidx={}".format(progress.uidx))

    for progress.eidx in range(progress.eidx, config.max_epochs):
        logging.info('Starting epoch {0}'.format(progress.eidx))
        for batch, flag in text_iterator:
            inputs, outputs = batch
            x_in, x_mask_in, y_in, y_mask_in = util.data_prepare_func(flag, inputs, outputs, config.factors, maxlen=None)
            
            write_summary_for_this_batch = config.summary_freq and ((progress.uidx % config.summary_freq == 0) or (config.finish_after and progress.uidx % config.finish_after == 0))
            
            if flag == DataType.DATA_FOR_CLF:
                loss = aux_updater.update(sess, x_in, x_mask_in, y_in, y_mask_in, write_summary_for_this_batch, flag)
                clf_loss += loss
                n_batch += x_mask_in.shape[-1]
            else:
                loss = updater.update(sess, x_in, x_mask_in, y_in, y_mask_in, write_summary_for_this_batch, flag)
                nmt_loss += loss
                n_words_for_loss += int(numpy.sum(y_mask_in))
            
            n_sents += y_mask_in.shape[0] * y_mask_in.shape[-1]
            n_words += int(numpy.sum(x_mask_in[0]))
            progress.uidx += 1

            if config.disp_freq and progress.uidx % config.disp_freq == 0:
                duration = time.time() - last_time
                disp_time = datetime.now().strftime('[%Y-%m-%d %H:%M:%S]')

                msg = '{0} Epoch: {1} Update: {2} ' \
                    'NMT-Loss ce/word: {3:.5f} CLF-Loss ce/batch: {4:.5f} '\
                    'Words/sec: {5:.5f} Sents/sec: {6:.5f}'
            
                logging.info(msg.format(disp_time, progress.eidx, progress.uidx, 
                                        nmt_loss/n_words_for_loss, clf_loss/n_batch,
                                        n_words/duration, n_sents/duration))

                last_time = time.time()
                nmt_loss, clf_loss = 0., 0.
                n_sents, n_words, n_words_for_loss, n_batch = 0, 0, 0, 0

            if config.valid_freq and progress.uidx % config.valid_freq == 0:
                clf_ce, clf_acc = validate_clf(sess, replicas[0].clf_model, config, valid_text_iterator.clf_iter)
                nmt_ce = validate_nmt(sess, replicas[0].nmt_model, config, valid_text_iterator.nmt_iter)

                progress.clf_errs.append(clf_ce)
                progress.acc_scores.append(clf_acc)
                if (len(progress.nmt_errs) == 0 or
                    nmt_ce < min(progress.nmt_errs)):
                    progress.nmt_errs.append(nmt_ce)
                    progress.bad_counter = 0
                    save_non_checkpoint(sess, saver, config.saveto)
                    progress_path = '{0}.progress.json'.format(config.saveto)
                    progress.save_to_json(progress_path)
                else:
                    progress.nmt_errs.append(nmt_ce)
                    progress.bad_counter += 1
                    if progress.bad_counter > config.patience:
                        logging.info('Early Stop!')
                        progress.estop = True
                        break

                if config.valid_script is not None:
                    bleu_score = validate_with_script(sess, replicas[0].nmt_model, config)
                    need_to_save = (bleu_score is not None and
                        (len(progress.bleu_scores) == 0 or
                         bleu_score > max(progress.bleu_scores)))
                    if bleu_score is None:
                        bleu_score = 0.0  # ensure a valid value is written
                    progress.bleu_scores.append(bleu_score)

                    if need_to_save:
                        save_path = config.saveto + ".best-valid"
                        save_non_checkpoint(sess, saver, save_path)
                        write_config_to_json_file(config, save_path)

                        progress_path = '{}.progress.json'.format(save_path)
                        progress.save_to_json(progress_path)

            if config.save_freq and progress.uidx % config.save_freq == 0:
                saver.save(sess, save_path=config.saveto, global_step=progress.uidx)
                write_config_to_json_file(config, "%s-%s" % (config.saveto, progress.uidx))

                progress_path = '{0}-{1}.progress.json'.format(config.saveto, progress.uidx)
                progress.save_to_json(progress_path)

            if config.finish_after and progress.uidx % config.finish_after == 0:
                logging.info("Maximum number of updates reached")
                saver.save(sess, save_path=config.saveto, global_step=progress.uidx)
                write_config_to_json_file(config, "%s-%s" % (config.saveto, progress.uidx))

                progress.estop=True
                progress_path = '{0}-{1}.progress.json'.format(config.saveto, progress.uidx)
                progress.save_to_json(progress_path)
                break
        if progress.estop:
            break


def train(config, sess):
    # Construct the graph, with one model replica per GPU
    num_gpus = len(util.get_available_gpus())
    num_replicas = max(1, num_gpus)

    logging.info('Building model...')
    replicas = []
    for i in range(num_replicas):
        device_type = "GPU" if num_gpus > 0 else "CPU"
        device_spec = tf.DeviceSpec(device_type=device_type, device_index=i)
        with tf.device(device_spec):
            with tf.variable_scope(tf.get_variable_scope(), reuse=(i>0)):
                model = model_builder(config)
                replicas.append(model)

    init = tf.zeros_initializer(dtype=tf.int32)
    global_step = tf.get_variable('time', [], initializer=init, trainable=False)

    if config.summary_freq:
        summary_dir = (config.summary_dir if config.summary_dir is not None
                       else os.path.abspath(os.path.dirname(config.saveto)))
        writer = tf.summary.FileWriter(summary_dir, sess.graph)
    else:
        writer = None
    
    schedule = ConstantSchedule(config.learning_rate)
    optimizer = tf.train.AdamOptimizer(learning_rate=schedule.learning_rate,
                                        beta1=config.adam_beta1,
                                        beta2=config.adam_beta2,
                                        epsilon=config.adam_epsilon)

    updater = ModelUpdater(config, num_gpus, replicas, optimizer, global_step, writer)
    if config.mode == 'joint':
        aux_optimizer = tf.train.AdamOptimizer(learning_rate=schedule.learning_rate,
                                        beta1=config.adam_beta1,
                                        beta2=config.adam_beta2,
                                        epsilon=config.adam_epsilon)
        aux_updater = ModelUpdater(config, num_gpus, replicas, aux_optimizer, global_step, writer, is_aux_updater=True)
    
    saver, progress = model_loader.init_or_restore_variables(config, sess, train=True)
    global_step.load(progress.uidx, sess)

    #save model options
    write_config_to_json_file(config, config.saveto)
    text_iterator, valid_text_iterator = load_data(config)

    if config.mode == 'clf':
        _train_clf(config, sess, text_iterator, valid_text_iterator, progress, updater, saver, replicas)
    elif config.mode in ('baseline-nmt', 'nmt'):
        _train_nmt(config, sess, text_iterator, valid_text_iterator, progress, updater, saver, replicas)
    elif config.mode == 'joint':
        _train_joint(config, sess, text_iterator, valid_text_iterator, progress, updater, aux_updater, saver, replicas)
    else:
        logging.info('invalid mode %s' % config.mode)
        exit(-1)

def save_non_checkpoint(session, saver, save_path):
    """Saves the model to a temporary directory then moves it to save_path.

    Rationale: we use TensorFlow's standard tf.train.Saver mechanism for saving
    training checkpoints and also for saving the current best model according
    to validation metrics. Since these are all stored in the same directory,
    their paths would normally all get written to the same 'checkpoint' file,
    with the file containing whichever one was last saved. That creates a
    problem if training is interrupted after a best-so-far model is saved but
    before a regular checkpoint is saved, since Nematus will try to load the
    best-so-far model instead of the last checkpoint when it is restarted. To
    avoid this, we save the best-so-far models to a temporary directory, then
    move them to their desired location. The 'checkpoint' file that is written
    to the temporary directory can safely be deleted along with the directory.

    Args:
        session: a TensorFlow session.
        saver: a tf.train.Saver
        save_path: string containing the path to save the model to.

    Returns:
        None.
    """
    head, tail = os.path.split(save_path)
    assert tail != ""
    base_dir = "." if head == "" else head
    with tempfile.TemporaryDirectory(dir=base_dir) as tmp_dir:
        tmp_save_path = os.path.join(tmp_dir, tail)
        saver.save(session, save_path=tmp_save_path)
        for filename in os.listdir(tmp_dir):
            if filename == 'checkpoint':
                continue
            new = os.path.join(tmp_dir, filename)
            old = os.path.join(base_dir, filename)
            os.replace(src=new, dst=old)


if __name__ == "__main__":
    # Parse command-line arguments.
    config = read_config_from_cmdline()
    logging.info(config)

    # Create the TensorFlow session.
    tf_config = tf.ConfigProto()
    tf_config.allow_soft_placement = True

    # Train.
    with tf.Session(config=tf_config) as sess:
        train(config, sess)
