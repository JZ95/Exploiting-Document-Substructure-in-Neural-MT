#!/usr/bin/env python3
'''
Build a neural machine translation model with soft attention
'''
import collections
from datetime import datetime
import json
import os
import locale
import logging
import subprocess
import sys
import tempfile
import time
import numpy
import tensorflow as tf
from config import write_config_to_json_file, read_config_from_cmdline
from learning_schedule import ConstantSchedule
import model_loader
import translate
import util
from constants import DataType
from models import model_builder
from data_iterator import iter_builder
from model_updater import ModelUpdater

# Start logging.
level = logging.INFO
logging.basicConfig(level=level, format='%(levelname)s: %(message)s')

def load_data(config):
    logging.info('Reading data...')
    text_iterator = iter_builder(config)

    if config.valid_freq:
        valid_text_iterator = iter_builder(config, is_valid_data=True)
    else:
        logging.info('no validation set loaded')
        valid_text_iterator = None

    logging.info('Done')
    return text_iterator, valid_text_iterator


def train(config, sess):
    # Construct the graph, with one model replica per GPU
    num_gpus = len(util.get_available_gpus())
    num_replicas = max(1, num_gpus)

    logging.info('Building model...')
    replicas = []
    for i in range(num_replicas):
        device_type = "GPU" if num_gpus > 0 else "CPU"
        device_spec = tf.DeviceSpec(device_type=device_type, device_index=i)
        with tf.device(device_spec):
            with tf.variable_scope(tf.get_variable_scope(), reuse=(i>0)):
                model = model_builder(config)
                replicas.append(model)

    init = tf.zeros_initializer(dtype=tf.int32)
    global_step = tf.get_variable('time', [], initializer=init, trainable=False)

    schedule = ConstantSchedule(config.learning_rate)
    optimizer = tf.train.AdamOptimizer(learning_rate=schedule.learning_rate,
                                        beta1=config.adam_beta1,
                                        beta2=config.adam_beta2,
                                        epsilon=config.adam_epsilon)

    if config.summary_freq:
        summary_dir = (config.summary_dir if config.summary_dir is not None
                       else os.path.abspath(os.path.dirname(config.saveto)))
        writer = tf.summary.FileWriter(summary_dir, sess.graph)
    else:
        writer = None

    main_updater = ModelUpdater(config, num_gpus, replicas, optimizer, global_step, writer)

    if config.mode == 'joint':
        aux_optimizer = tf.train.AdamOptimizer(learning_rate=schedule.learning_rate,
                                                beta1=config.adam_beta1,
                                                beta2=config.adam_beta2,
                                                epsilon=config.adam_epsilon)
        aux_updater = ModelUpdater(config, num_gpus, replicas, aux_optimizer, global_step, writer, True)

    saver, progress = model_loader.init_or_restore_variables(config, sess, train=True)
    global_step.load(progress.uidx, sess)

    #save model options
    write_config_to_json_file(config, config.saveto)

    text_iterator, valid_text_iterator = load_data(config)
    main_loss, aux_loss = 0., 0.
    n_sents, n_words = 0, 0
    last_time = time.time()
    logging.info("Initial uidx={}".format(progress.uidx))
    
    for progress.eidx in range(progress.eidx, config.max_epochs):
        logging.info('Starting epoch {0}'.format(progress.eidx))
        for batch, flag in text_iterator:
            inputs, outputs = batch
            x_in, x_mask_in, y_in, y_mask_in = util.data_prepare_func(flag, inputs, outputs, config.factors, maxlen=None)
            assert x_in is not None

            write_summary_for_this_batch = config.summary_freq and ((progress.uidx % config.summary_freq == 0) or (config.finish_after and progress.uidx % config.finish_after == 0))

            if len(x_in.shape) == 4:
                (factors, sentLen, wordLen, batch_size) = x_in.shape
                n_sents += sentLen
                n_words += sentLen * wordLen
            elif len(x_in.shape) == 3:
                (factors, wordLen, batch_size) = x_in.shape
                n_sents += batch_size
                n_words += wordLen * batch_size

            if config.mode == 'joint' and flag == DataType.DATA_FOR_CLF:
                # aux loss is only valid in joint mode
                loss = aux_updater.update(sess, x_in, x_mask_in, y_in, y_mask_in, write_summary_for_this_batch, flag)
                aux_loss += loss
            else:
                loss = main_updater.update(sess, x_in, x_mask_in, y_in, y_mask_in, write_summary_for_this_batch, flag)
                main_loss += loss

            progress.uidx += 1

            if config.disp_freq and progress.uidx % config.disp_freq == 0:
                duration = time.time() - last_time
                disp_time = datetime.now().strftime('[%Y-%m-%d %H:%M:%S]')

                msg = '{0} Epoch: {1} Update: {2} ' \
                    'MT-Loss/word: {3:.5f} CLF-Loss: {4:.5f} '\
                    'Words/sec: {5:.5f} Sents/sec: {6:.5f}'
                
                if config.mode == 'clf':
                    aux_loss = main_loss
                    main_loss = numpy.nan
                elif config.mode in ('nmt' , 'baseline-nmt'):
                    aux_loss = numpy.nan

                logging.info(msg.format(disp_time, progress.eidx, progress.uidx, 
                                        main_loss/n_words, aux_loss/n_words,
                                        n_words/duration, n_sents/duration))

                last_time = time.time()
                main_loss, aux_loss = 0., 0.
                n_sents, n_words = 0, 0

            if config.valid_freq and progress.uidx % config.valid_freq == 0:
                valid_score = validate(sess, replicas[0], config, valid_text_iterator)
                valid_ce = valid_score.ce
                if (len(progress.history_errs) == 0 or
                    valid_ce < min(progress.history_errs)):
                    progress.history_errs.append(valid_ce)
                    progress.bad_counter = 0
                    save_non_checkpoint(sess, saver, config.saveto)
                    progress_path = '{0}.progress.json'.format(config.saveto)
                    progress.save_to_json(progress_path)
                else:
                    progress.history_errs.append(valid_ce)
                    progress.bad_counter += 1
                    if progress.bad_counter > config.patience:
                        logging.info('Early Stop!')
                        progress.estop = True
                        break

                if config.mode == 'clf':
                    valid_acc = valid_score.acc
                    # decide wether save as the best-model or not
                    need_to_save = ((len(progress.acc_scores) == 0 or 
                            valid_acc > max(progress.acc_scores)))
                    progress.acc_scores.append(valid_acc)

                elif config.mode != 'clf' and config.valid_script is not None:
                    bleu_score = validate_with_script(sess, replicas[0], config)
                    need_to_save = (bleu_score is not None and
                        (len(progress.bleu_scores) == 0 or
                         bleu_score > max(progress.bleu_scores)))
                    if bleu_score is None:
                        bleu_score = 0.0  # ensure a valid value is written
                    progress.bleu_scores.append(bleu_score)

                if need_to_save:
                    save_path = config.saveto + ".best-valid"
                    save_non_checkpoint(sess, saver, save_path)
                    write_config_to_json_file(config, save_path)

                    progress_path = '{}.progress.json'.format(save_path)
                    progress.save_to_json(progress_path)

            if config.save_freq and progress.uidx % config.save_freq == 0:
                saver.save(sess, save_path=config.saveto, global_step=progress.uidx)
                write_config_to_json_file(config, "%s-%s" % (config.saveto, progress.uidx))

                progress_path = '{0}-{1}.progress.json'.format(config.saveto, progress.uidx)
                progress.save_to_json(progress_path)

            if config.finish_after and progress.uidx % config.finish_after == 0:
                logging.info("Maximum number of updates reached")
                saver.save(sess, save_path=config.saveto, global_step=progress.uidx)
                write_config_to_json_file(config, "%s-%s" % (config.saveto, progress.uidx))

                progress.estop=True
                progress_path = '{0}-{1}.progress.json'.format(config.saveto, progress.uidx)
                progress.save_to_json(progress_path)
                break
        if progress.estop:
            break


def validate(session, model, config, text_iterator):
    valid_score = collections.namedtuple('valid_score', ['ce', 'acc'])
    if config.mode == 'clf':
        ce, acc = _validate_clf(session, model, config, text_iterator)
        return valid_score(ce, acc)
    else:
        ce = _validate_nmt(session, model, config, text_iterator)
        return valid_score(ce, None)


def _validate_clf(session, model, config, text_iterator):
    ce_vals, y_pred, y_true = [], [], []

    for (xx, yy), flag in text_iterator:
        x, x_mask, y, y_mask = util.data_prepare_func(flag, xx, yy, config.factors, maxlen=None)

        # Run the minibatch through the model to get the sentence-level cross entropy values.
        feeds = {model.inputs.x: x,
                 model.inputs.x_mask: x_mask,
                 model.inputs.y: y,
                 model.inputs.y_mask: y_mask,
                 model.inputs.training: False}
        batch_ce_vals, batch_preds = session.run([model.loss_per_sec, model.preds], feed_dict=feeds)
        y_true.extend(y.ravel().tolist())
        y_pred.extend(batch_preds.tolist())
        ce_vals.extend(batch_ce_vals.tolist())
        logging.info("Seen {}".format(len(ce_vals)))

    assert len(y_true) == len(y_pred)
    acc_score = (numpy.array(y_true) == numpy.array(y_pred)).sum() / len(y_true)

    num_secs = len(ce_vals)
    sum_ce = sum(ce_vals)
    avg_ce = sum_ce / num_secs

    logging.info('Validation external score (Accuracy) : {0:.5f} '.format(acc_score))
    logging.info('Validation cross entropy (AVG/SUM): {0:.5f} {1:.5f}'.format(avg_ce, sum_ce))

    return avg_ce, acc_score


def _validate_nmt(session, model, config, text_iterator):
    ce_vals, token_counts = [], []
    for (xx, yy), flag in text_iterator:
        x, x_mask, y, y_mask = util.data_prepare_func(flag, xx, yy, config.factors, maxlen=None)

        feeds = {model.inputs.x: x,
                 model.inputs.x_mask: x_mask,
                 model.inputs.y: y,
                 model.inputs.y_mask: y_mask,
                 model.inputs.training: False}
        batch_ce_vals = session.run(model.loss_per_sentence, feed_dict=feeds)
        word_len = y_mask.shape[1]
        batch_token_counts = [numpy.count_nonzero(s) for s in 
                                            numpy.reshape(
                                                numpy.transpose(y_mask, (0, 2, 1)),
                                                (-1, word_len))]
        ce_vals += list(batch_ce_vals)
        token_counts += batch_token_counts
        logging.info("Seen {}".format(len(ce_vals)))

    assert len(ce_vals) == len(token_counts)
    num_sents = len(ce_vals)
    num_tokens = sum(token_counts)
    sum_ce = sum(ce_vals)
    avg_ce = sum_ce / num_sents
    logging.info('Validation cross entropy (AVG/SUM/N_SENTS/N_TOKENS): {0:.5f} ' \
                 '{1:.5f} {2:.5f} {3:.5f}'.format(avg_ce, sum_ce, num_sents, num_tokens))
    return avg_ce


def save_non_checkpoint(session, saver, save_path):
    """Saves the model to a temporary directory then moves it to save_path.

    Rationale: we use TensorFlow's standard tf.train.Saver mechanism for saving
    training checkpoints and also for saving the current best model according
    to validation metrics. Since these are all stored in the same directory,
    their paths would normally all get written to the same 'checkpoint' file,
    with the file containing whichever one was last saved. That creates a
    problem if training is interrupted after a best-so-far model is saved but
    before a regular checkpoint is saved, since Nematus will try to load the
    best-so-far model instead of the last checkpoint when it is restarted. To
    avoid this, we save the best-so-far models to a temporary directory, then
    move them to their desired location. The 'checkpoint' file that is written
    to the temporary directory can safely be deleted along with the directory.

    Args:
        session: a TensorFlow session.
        saver: a tf.train.Saver
        save_path: string containing the path to save the model to.

    Returns:
        None.
    """
    head, tail = os.path.split(save_path)
    assert tail != ""
    base_dir = "." if head == "" else head
    with tempfile.TemporaryDirectory(dir=base_dir) as tmp_dir:
        tmp_save_path = os.path.join(tmp_dir, tail)
        saver.save(session, save_path=tmp_save_path)
        for filename in os.listdir(tmp_dir):
            if filename == 'checkpoint':
                continue
            new = os.path.join(tmp_dir, filename)
            old = os.path.join(base_dir, filename)
            os.replace(src=new, dst=old)


def validate_with_script(session, model, config):
    if config.valid_script == None:
        return None
    logging.info('Starting external validation.')
    out = tempfile.NamedTemporaryFile(mode='w')
    translate.translate_file(input_file=open(config.valid_datasets[0]),
                             output_file=out,
                             session=session,
                             model=model,
                             config=config,
                             beam_size=config.beam_size,
                             normalization_alpha=config.normalization_alpha)
    out.flush()
    args = [config.valid_script, out.name]
    proc = subprocess.Popen(args, stdin=None, stdout=subprocess.PIPE,
                            stderr=subprocess.PIPE)
    stdout_bytes, stderr_bytes = proc.communicate()
    encoding = locale.getpreferredencoding()
    stdout = stdout_bytes.decode(encoding=encoding)
    stderr = stderr_bytes.decode(encoding=encoding)
    if len(stderr) > 0:
        logging.info("Validation script wrote the following to standard "
                     "error:\n" + stderr)
    if proc.returncode != 0:
        logging.warning("Validation script failed (returned exit status of "
                        "{}).".format(proc.returncode))
        return None
    try:
        score = float(stdout.split()[0])
    except:
        logging.warning("Validation script output does not look like a score: "
                        "{}".format(stdout))
        return None
    logging.info("Validation script score: {}".format(score))
    return score

if __name__ == "__main__":
    # Parse command-line arguments.
    config = read_config_from_cmdline()
    logging.info(config)

    # Create the TensorFlow session.
    tf_config = tf.ConfigProto()
    tf_config.allow_soft_placement = True

    # Train.
    with tf.Session(config=tf_config) as sess:
        train(config, sess)
