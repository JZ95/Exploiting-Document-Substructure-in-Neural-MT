import tensorflow as tf
import layers
import model_inputs
from sampling_utils import SamplingUtils

"""Builds a GRU-based attentional encoder-decoder model.

This class is responsible for constructing a TensorFlow graph that takes
a minibatch of sentence pairs as input and calculates a loss value via
teacher forcing (specifically, the loss is the mean sentence-level
cross-entropy).

For inference (sampling and beam search), see rnn_inference.py.

For optimization, see model_updater.py.
"""

class WordLevelEncoder(object):
    def __init__(self, config, batch_size, dropout_source, dropout_embedding,
                 dropout_hidden):
        self.dropout_source = dropout_source

        with tf.variable_scope("embedding"):
            self.emb_layer = layers.EmbeddingLayer(config.source_vocab_sizes,
                                                   config.dim_per_factor)


        bias_type = layers.LegacyBiasType.NEMATUS_COMPAT_FALSE

        with tf.variable_scope("forward-stack"):
            self.forward_encoder = layers.GRUStack(
                input_size=config.embedding_size,
                state_size=config.word_rnn_state_size,
                batch_size=batch_size,
                use_layer_norm=config.rnn_layer_normalization,
                legacy_bias_type=bias_type,
                dropout_input=dropout_embedding,
                dropout_state=dropout_hidden,
                stack_depth=config.word_rnn_enc_depth,
                transition_depth=config.word_rnn_enc_transition_depth,
                alternating=True,
                residual_connections=True,
                first_residual_output=1)

        with tf.variable_scope("backward-stack"):
            self.backward_encoder = layers.GRUStack(
                input_size=config.embedding_size,
                state_size=config.word_rnn_state_size,
                batch_size=batch_size,
                use_layer_norm=config.rnn_layer_normalization,
                legacy_bias_type=bias_type,
                dropout_input=dropout_embedding,
                dropout_state=dropout_hidden,
                stack_depth=config.word_rnn_enc_depth,
                transition_depth=config.word_rnn_enc_transition_depth,
                alternating=True,
                reverse_alternation=True,
                residual_connections=True,
                first_residual_output=1)

    def get_context(self, x, x_mask):
        """ 
        args:
            x: shape - [factors, seqLen, batch_size], type - numpy.array
            x_mask: shape - [seqLen, batch_size], type - numpy.array
        returns:
            embs: shape - [seqLen, batch_size, embed_size]
            concat_states: shape - [time_step, batch_size, state_size * 2]
        """
        with tf.variable_scope("embedding"):
            embs = self.emb_layer.forward(x)
            if self.dropout_source != None:
                embs = self.dropout_source(embs)

        with tf.variable_scope("forward-stack"):
            fwd_states = self.forward_encoder.forward(embs, x_mask)

        with tf.variable_scope("backward-stack"):
            bwd_states = self.backward_encoder.forward(embs, x_mask)

        # Concatenate the left-to-right and the right-to-left states, in that
        # order. This is for compatibility with models that were trained with
        # the Theano version.
        stack_depth = len(self.forward_encoder.grus)
        if stack_depth % 2 == 0:
            concat_states = tf.concat([bwd_states, fwd_states], axis=2)
        else:
            concat_states = tf.concat([fwd_states, bwd_states], axis=2)
        return concat_states, embs


class SentLevelEncoder(object):
    """ High level Encoder based on the Nematus Encoder, which models the information
    on Sentence, Paragraph level.
    The only difference is that this class doesn't have to build an embedding layer.
    """
    def __init__(self, config, batch_size, dropout_input, dropout_hidden):

        bias_type = layers.LegacyBiasType.NEMATUS_COMPAT_FALSE

        with tf.variable_scope("forward-stack"):
            self.forward_encoder = layers.GRUStack(
                input_size=2*config.word_rnn_state_size,
                state_size=config.sent_rnn_state_size,
                batch_size=batch_size,
                use_layer_norm=config.rnn_layer_normalization,
                legacy_bias_type=bias_type,
                dropout_input=dropout_input,
                dropout_state=dropout_hidden,
                stack_depth=config.sent_rnn_enc_depth,
                transition_depth=config.sent_rnn_enc_transition_depth,
                alternating=True,
                residual_connections=True,
                first_residual_output=1)

        with tf.variable_scope("backward-stack"):
            self.backward_encoder = layers.GRUStack(
                input_size=2*config.word_rnn_state_size,
                state_size=config.sent_rnn_state_size,
                batch_size=batch_size,
                use_layer_norm=config.rnn_layer_normalization,
                legacy_bias_type=bias_type,
                dropout_input=dropout_input,
                dropout_state=dropout_hidden,
                stack_depth=config.sent_rnn_enc_depth,
                transition_depth=config.sent_rnn_enc_transition_depth,
                alternating=True,
                reverse_alternation=True,
                residual_connections=True,
                first_residual_output=1)

    def get_context(self, x, x_mask):
        """ 
        args:
            x: shape - [seqLen, batch_size, state_size], type - numpy.array
            x_mask: shape - [seqLen, batch_size], type - numpy.array
        """
        with tf.variable_scope("forward-stack"):
            fwd_states = self.forward_encoder.forward(x, x_mask)

        with tf.variable_scope("backward-stack"):
            bwd_states = self.backward_encoder.forward(x, x_mask)

        # Concatenate the left-to-right and the right-to-left states, in that
        # order. This is for compatibility with models that were trained with
        # the Theano version.
        stack_depth = len(self.forward_encoder.grus)
        if stack_depth % 2 == 0:
            concat_states = tf.concat([bwd_states, fwd_states], axis=2)
        else:
            concat_states = tf.concat([fwd_states, bwd_states], axis=2)
        return concat_states


class Predictor:
    def __init__(self, config, batch_size, dropout_embedding, dropout_hidden, hidden_to_logits_W=None):
        self.config = config

        with tf.variable_scope("prev_emb_to_hidden"):
            self.prev_emb_to_hidden = layers.FeedForwardLayer(
                in_size=config.target_embedding_size,
                out_size=config.target_embedding_size,
                batch_size=batch_size,
                non_linearity=lambda y: y,
                use_layer_norm=config.rnn_layer_normalization,
                dropout_input=dropout_embedding)

        with tf.variable_scope("state_to_hidden"):
            self.state_to_hidden = layers.FeedForwardLayer(
                in_size=config.word_rnn_state_size,
                out_size=config.target_embedding_size,
                batch_size=batch_size,
                non_linearity=lambda y: y,
                use_layer_norm=config.rnn_layer_normalization,
                dropout_input=dropout_hidden)

        with tf.variable_scope("attended_context_to_hidden"):
            self.att_ctx_to_hidden = layers.FeedForwardLayer(
                in_size=2 * config.word_rnn_state_size,
                out_size=config.target_embedding_size,
                batch_size=batch_size,
                non_linearity=lambda y: y,
                use_layer_norm=config.rnn_layer_normalization,
                dropout_input=dropout_hidden)

        if config.output_hidden_activation == 'prelu':
            with tf.variable_scope("hidden_prelu"):
                self.hidden_prelu = layers.PReLU(in_size=config.target_embedding_size)

        with tf.variable_scope("hidden_to_logits"):
            self.hidden_to_logits = layers.FeedForwardLayer(
                in_size=config.target_embedding_size,
                out_size=config.target_vocab_size,
                batch_size=batch_size,
                non_linearity=lambda y: y,
                W=hidden_to_logits_W,
                dropout_input=dropout_embedding)

        if config.softmax_mixture_size > 1:
            with tf.variable_scope("hidden_to_pi_logits"):
                self.hidden_to_pi_logits = layers.FeedForwardLayer(
                    in_size=config.target_embedding_size,
                    out_size=config.softmax_mixture_size,
                    batch_size=batch_size,
                    non_linearity=lambda y: y,
                    dropout_input=dropout_embedding)

            self.hidden_to_mos_hidden = []
            for k in range(config.softmax_mixture_size):
                with tf.variable_scope("hidden_to_mos_hidden_{}".format(k)):
                    layer = layers.FeedForwardLayer(
                        in_size=config.target_embedding_size,
                        out_size=config.target_embedding_size,
                        batch_size=batch_size,
                        use_layer_norm=config.rnn_layer_normalization,
                        dropout_input=dropout_embedding)
                    self.hidden_to_mos_hidden.append(layer)

        if config.rnn_lexical_model:
            with tf.variable_scope("lexical_to_logits"):
                self.lexical_to_logits = layers.FeedForwardLayer(
                                in_size=config.target_embedding_size,
                                out_size=config.target_vocab_size,
                                batch_size=batch_size,
                                non_linearity=lambda y: y,
                                dropout_input=dropout_embedding)

        with tf.variable_scope("clf_state_to_hidden"):
            self.clf_state_to_hidden = layers.FeedForwardLayer(
                in_size=config.sec_repr_size,
                out_size=config.target_embedding_size,
                batch_size=batch_size,
                non_linearity=lambda y: y,
                use_layer_norm=config.rnn_layer_normalization,
                dropout_input=dropout_hidden)
    
    def get_logits(self, y_embs, states, attended_states, clf_states, lexical_states, multi_step=True):
        """ add additional text classifier representation for each section into the computation for logits
        """
        with tf.variable_scope("prev_emb_to_hidden"):
            hidden_emb = self.prev_emb_to_hidden.forward(y_embs, input_is_3d=multi_step)
            
        with tf.variable_scope("state_to_hidden"):
            hidden_state = self.state_to_hidden.forward(states, input_is_3d=multi_step)

        with tf.variable_scope("attended_context_to_hidden"):
            hidden_att_ctx = self.att_ctx_to_hidden.forward(attended_states,input_is_3d=multi_step)

        with tf.variable_scope("clf_state_to_hidden"):
            hidden_clf_state = self.clf_state_to_hidden.forward(clf_states, input_is_3d=False)
        
        hidden = hidden_emb + hidden_state + hidden_att_ctx + hidden_clf_state
        if self.config.output_hidden_activation == 'tanh':
            hidden = tf.tanh(hidden)
        elif self.config.output_hidden_activation == 'relu':
            hidden = tf.nn.relu(hidden)
        elif self.config.output_hidden_activation == 'prelu':
            hidden = self.hidden_prelu.forward(hidden)
        elif self.config.output_hidden_activation == 'linear':
            pass
        else:
            assert False, 'Unknown output activation function "%s"' % self.config.output_hidden_activation

        if self.config.softmax_mixture_size == 1:
            with tf.variable_scope("hidden_to_logits"):
                logits = self.hidden_to_logits.forward(hidden, input_is_3d=multi_step)

            if self.config.rnn_lexical_model:
                with tf.variable_scope("lexical_to_logits"):
                    logits += self.lexical_to_logits.forward(lexical_states, input_is_3d=multi_step)
        else:
            assert self.config.softmax_mixture_size > 1
            pi_logits = self.hidden_to_pi_logits.forward(hidden,
                                                         input_is_3d=multi_step)
            pi = tf.nn.softmax(pi_logits)
            probs = None
            for k in range(self.config.softmax_mixture_size):
                hidden_k = self.hidden_to_mos_hidden[k].forward(hidden,
                    input_is_3d=multi_step)
                logits_k = self.hidden_to_logits.forward(hidden_k,
                    input_is_3d=multi_step)
                probs_k = tf.nn.softmax(logits_k)
                weight = pi[..., k:k+1]
                if k == 0:
                    probs = probs_k * weight
                else:
                    probs += probs_k * weight
            logits = tf.log(probs)

        return logits


class Decoder:
    def __init__(self, config, context, x_embs, x_mask,
                 dropout_target, dropout_embedding, dropout_hidden,
                 encoder_embedding_layer=None):
        self.dropout_target = dropout_target
        batch_size = tf.shape(x_mask)[1]

        with tf.variable_scope("initial_state_constructor"):
            context_sum = tf.reduce_sum(
                            context * tf.expand_dims(x_mask, axis=2),
                            axis=0)
            context_mean = tf.div_no_nan(
                                context_sum, 
                                tf.expand_dims(tf.reduce_sum(x_mask, axis=0), 
                                axis=1))
            self.init_state_layer = layers.FeedForwardLayer(
                in_size=config.word_rnn_state_size * 2,
                out_size=config.word_rnn_state_size,
                batch_size=batch_size,
                use_layer_norm=config.rnn_layer_normalization,
                dropout_input=dropout_hidden)
            self.init_state = self.init_state_layer.forward(context_mean)
            self.x_embs = x_embs

            self.translation_maxlen = config.translation_maxlen
            self.embedding_size = config.target_embedding_size
            self.state_size = config.word_rnn_state_size
            self.target_vocab_size = config.target_vocab_size

        with tf.variable_scope("embedding"):
            if encoder_embedding_layer == None:
                self.y_emb_layer = layers.EmbeddingLayer(
                    vocabulary_sizes=[config.target_vocab_size],
                    dim_per_factor=[config.target_embedding_size])
            else:
                self.y_emb_layer = encoder_embedding_layer

        with tf.variable_scope("base"):
            with tf.variable_scope("gru0"):
                bias_type = layers.LegacyBiasType.NEMATUS_COMPAT_FALSE
                self.grustep1 = layers.GRUStep(
                    input_size=config.target_embedding_size,
                    state_size=config.word_rnn_state_size,
                    batch_size=batch_size,
                    use_layer_norm=config.rnn_layer_normalization,
                    legacy_bias_type=bias_type,
                    dropout_input=dropout_embedding,
                    dropout_state=dropout_hidden)
            with tf.variable_scope("attention"):
                self.attstep = layers.AttentionStep(
                    context=context,
                    context_state_size=2*config.word_rnn_state_size,
                    context_mask=x_mask,
                    state_size=config.word_rnn_state_size,
                    hidden_size=2*config.word_rnn_state_size,
                    use_layer_norm=config.rnn_layer_normalization,
                    dropout_context=dropout_hidden,
                    dropout_state=dropout_hidden)
            bias_type = layers.LegacyBiasType.NEMATUS_COMPAT_TRUE
            self.grustep2 = layers.DeepTransitionGRUStep(
                input_size=2*config.word_rnn_state_size,
                state_size=config.word_rnn_state_size,
                batch_size=batch_size,
                use_layer_norm=config.rnn_layer_normalization,
                legacy_bias_type=bias_type,
                dropout_input=dropout_hidden,
                dropout_state=dropout_hidden,
                transition_depth=config.rnn_dec_base_transition_depth-1,
                var_scope_fn=lambda i: "gru{0}".format(i+1))

        with tf.variable_scope("high"):
            if config.rnn_dec_depth == 1:
                self.high_gru_stack = None
            else:
                bias_type = layers.LegacyBiasType.NEMATUS_COMPAT_TRUE
                self.high_gru_stack = layers.GRUStack(
                    input_size=config.word_rnn_state_size,
                    state_size=config.word_rnn_state_size,
                    batch_size=batch_size,
                    use_layer_norm=config.rnn_layer_normalization,
                    legacy_bias_type=bias_type,
                    dropout_input=dropout_hidden,
                    dropout_state=dropout_hidden,
                    stack_depth=config.rnn_dec_depth-1,
                    transition_depth=config.rnn_dec_high_transition_depth,
                    context_state_size=(2*config.word_rnn_state_size if config.rnn_dec_deep_context else 0),
                    residual_connections=True,
                    first_residual_output=0)

        if config.rnn_lexical_model:
            with tf.variable_scope("lexical"):
                self.lexical_layer = layers.LexicalModel(
                    in_size=config.embedding_size,
                    out_size=config.embedding_size,
                    batch_size=batch_size,
                    use_layer_norm=config.rnn_layer_normalization,
                    dropout_embedding=dropout_embedding,
                    dropout_hidden=dropout_hidden)
        else:
            self.lexical_layer = None

        with tf.variable_scope("next_word_predictor"):
            W = None
            if config.tie_decoder_embeddings:
                W = self.y_emb_layer.get_embeddings(factor=0)
                W = tf.transpose(W)
            self.predictor = Predictor(config, batch_size, dropout_embedding,
                                         dropout_hidden, hidden_to_logits_W=W)

    def score(self, y, sec_repr):
        with tf.variable_scope("y_embeddings_layer"):
            y_but_last = tf.slice(y, [0,0], [tf.shape(y)[0]-1, -1])
            y_embs = self.y_emb_layer.forward(y_but_last, factor=0)
            if self.dropout_target != None:
                y_embs = self.dropout_target(y_embs)
            y_embs = tf.pad(y_embs,
                            mode='CONSTANT',
                            paddings=[[1,0],[0,0],[0,0]]) # prepend zeros

        init_attended_context = tf.zeros([tf.shape(self.init_state)[0], self.state_size*2])
        init_att_alphas = tf.zeros([tf.shape(self.x_embs)[0], tf.shape(self.x_embs)[1]])
        init_state_att_ctx = (self.init_state, init_attended_context, init_att_alphas)
        gates_x, proposal_x = self.grustep1.precompute_from_x(y_embs)
        
        # FIXME: here throws a lot of NAN error 
        def step_fn(prev, x):
            prev_state = prev[0]
            prev_att_ctx = prev[1]
            prev_lexical_state = prev[2]
            gates_x2d = x[0]
            proposal_x2d = x[1]
            state = self.grustep1.forward(
                        prev_state,
                        gates_x=gates_x2d,
                        proposal_x=proposal_x2d)
            att_ctx, att_alphas = self.attstep.forward(state)
            state = self.grustep2.forward(state, att_ctx)
            return (state, att_ctx, att_alphas)

        layer = layers.RecurrentLayer(initial_state=init_state_att_ctx,
                                      step_fn=step_fn)
        states, attended_states, attention_weights = layer.forward((gates_x, proposal_x))

        if self.high_gru_stack != None:
            states = self.high_gru_stack.forward(
                states,
                context_layer=(attended_states if self.high_gru_stack.context_state_size > 0 else None),
                init_state=self.init_state)

        if self.lexical_layer is not None:
            lexical_states = self.lexical_layer.forward(self.x_embs, attention_weights, multi_step=True)
        else:
            lexical_states = None

        logits = self.predictor.get_logits(y_embs, states, attended_states, sec_repr, lexical_states, multi_step=True)
        return logits


class HANModel(object):
    def __init__(self, config):
        self.inputs = model_inputs.CLFModelInputs(config)
        
        dropout_source = None
        if config.rnn_use_dropout and config.rnn_dropout_source > 0.0:
            def dropout_source(x):
                return tf.layers.dropout(
                    x, noise_shape=(tf.shape(x)[0], tf.shape(x)[1], 1),
                    rate=config.rnn_dropout_source,
                    training=self.inputs.training)

        dropout_embedding, dropout_hidden = None, None
        if config.rnn_use_dropout and config.rnn_dropout_embedding > 0.0:
            def dropout_embedding(e):
                return tf.layers.dropout(e, noise_shape=tf.shape(e),
                                         rate=config.rnn_dropout_embedding,
                                         training=self.inputs.training)

        if config.rnn_use_dropout and config.rnn_dropout_hidden > 0.0:
            def dropout_hidden(h):
                return tf.layers.dropout(h, noise_shape=tf.shape(h),
                                         rate=config.rnn_dropout_hidden,
                                         training=self.inputs.training)

        # batch_size is the alias of the number of sections
        _, sent_len, word_len, batch_size = tf.unstack(tf.shape(self.inputs.x))
        
        # reshape layer for word encoder
        self.word_input = tf.reshape(
            tf.transpose(self.inputs.x, (0, 2, 1, 3)),   # n_factor * sent_len * word_len * n_sec -> n_factor * word_len * sent_len * n_sec
            (config.factors, word_len, sent_len * batch_size))
        self.word_input_mask = tf.reshape(
            tf.transpose(self.inputs.x_mask, (1, 0, 2)),   # sent_len * word_len * n_sec ->  word_len * sent_len * n_sec
            (word_len, sent_len * batch_size))

        # init word-level encoder and attention
        self._init_word_encoder(config, sent_len * batch_size, dropout_source, dropout_embedding, dropout_hidden)
        self._init_word_attn(config, sent_len * batch_size, dropout_hidden)

        # reshape layer for sent encoder
        # reshape context vector, always batch channel last
        self.sent_input = tf.reshape(self.word_attn_context, 
            (sent_len, batch_size, self.word_attn_context.shape[-1]))
        self.sent_input_mask = tf.sign(
            tf.reduce_sum(self.inputs.x_mask, axis=1))
        
        # init sentence-level encoder and attention
        self._init_sent_encoder(config, batch_size, dropout_hidden)
        self._init_sent_attn(config, batch_size, dropout_hidden)
        
        with tf.variable_scope("fully-connect-1"):
            self.fc_layer_1 = layers.FeedForwardLayer(in_size=2 * config.sent_rnn_state_size, out_size=config.sec_repr_size, batch_size=batch_size)
            self.sec_repr = self.fc_layer_1.forward(self.sent_attn_context)

        with tf.variable_scope("fully-connect-2"):
            self.fc_layer_2 = layers.FeedForwardLayer(in_size=config.sec_repr_size, out_size=9, batch_size=batch_size)
            # logits: [1, batch_size, class_num]
            self.logits = tf.expand_dims(self.fc_layer_2.forward(self.sec_repr), axis=0, name='logits')
            self._preds = tf.argmax(tf.squeeze(self.logits, axis=0), axis=1, name='preds')

        with tf.variable_scope("loss"):
            # we can view the output label as a one-token sentence
            self.loss_layer = layers.Masked_cross_entropy_loss(
                self.inputs.y, self.inputs.y_mask, config.label_smoothing,
                training=self.inputs.training)
            self._loss_per_sec = self.loss_layer.forward(self.logits)
            self._loss = tf.reduce_mean(self._loss_per_sec, keepdims=False)

    def _init_word_encoder(self, config, total_sent_num, dropout_source, dropout_embedding, dropout_hidden):
        # word-level encoder which is shared with NMT
        with tf.variable_scope("encoder"):
            # the input of Encoder put batch_size as its last dimension
            self.word_encoder = WordLevelEncoder(config, total_sent_num, dropout_source, 
                                                 dropout_embedding, dropout_hidden)
            self.word_level_ctx, self.word_embeds = self.word_encoder.get_context(self.word_input, self.word_input_mask)
            # word_level_ctx - a simple concate of bi-direction GRU states
            # [time_steps, batch_size, 2 * state_size]
            self.word_level_query = tf.get_variable(name='word-level-query', 
                                                    shape=[2 * config.word_rnn_state_size], 
                                                    dtype=tf.float32, 
                                                    initializer=tf.random_normal_initializer,
                                                    trainable=True)

    def _init_word_attn(self, config, total_sent_num, dropout_hidden):
        with tf.variable_scope("attention"):
            self.word_attn_layer = layers.AttentionStep(
                                        context=self.word_level_ctx,
                                        context_state_size=2 * config.word_rnn_state_size,          # the size of context
                                        context_mask=self.word_input_mask,
                                        state_size=2 * config.word_rnn_state_size,                  # the size of query vector
                                        hidden_size=2 * config.word_rnn_state_size,                
                                        use_layer_norm=config.rnn_layer_normalization,   
                                        dropout_context=dropout_hidden,
                                        dropout_state=dropout_hidden)
            # word_attn [batch_size * sent_len, state_size * 2]
            self.word_attn_context, self.scores = self.word_attn_layer.forward(
                        tf.broadcast_to(self.word_level_query, [total_sent_num, 2 * config.word_rnn_state_size])
                        )

    def _init_sent_encoder(self, config, total_sec_num, dropout_hidden):
        # sent-level encoder
        with tf.variable_scope("sent-encoder"):
            self.sent_encoder = SentLevelEncoder(config, total_sec_num, dropout_hidden, dropout_hidden)
            self.sent_level_ctx = self.sent_encoder.get_context(self.sent_input, self.sent_input_mask)

            self.sent_level_query = tf.get_variable(name='sent-level-query',
                                                shape=[2 * config.sent_rnn_state_size],
                                                dtype=tf.float32,
                                                initializer=tf.random_normal_initializer,
                                                trainable=True)

    def _init_sent_attn(self, config, total_sec_num, dropout_hidden):
        with tf.variable_scope("sent-attention"):
            # sent_attn [batch_size, state_size * 2]
            self.sent_attn_layer = layers.AttentionStep(
                context=self.sent_level_ctx, 
                context_state_size=2 * config.sent_rnn_state_size,
                context_mask=self.sent_input_mask,
                state_size=2 * config.sent_rnn_state_size,
                hidden_size=2 * config.sent_rnn_state_size,
                use_layer_norm=config.rnn_layer_normalization,
                dropout_context=dropout_hidden,
                dropout_state=dropout_hidden)
            
            # now batch channel first
            self.sent_attn_context, _ = self.sent_attn_layer.forward(
                tf.broadcast_to(self.sent_level_query, (total_sec_num, 2 * config.sent_rnn_state_size))
                )

    @property
    def loss_per_sec(self):
        return self._loss_per_sec
    
    @property
    def preds(self):
        return self._preds

    @property
    def loss(self):
        return self._loss
    

class ContextAwareNMT(HANModel):
    def __init__(self, config):
        self.inputs = model_inputs.NMTModelInputs(config)

        dropout_source, dropout_target = None, None
        if config.rnn_use_dropout and config.rnn_dropout_source > 0.0:
            def dropout_source(x):
                return tf.layers.dropout(
                    x, noise_shape=(tf.shape(x)[0], tf.shape(x)[1], 1),
                    rate=config.rnn_dropout_source,
                    training=self.inputs.training)
        if config.rnn_use_dropout and config.rnn_dropout_target > 0.0:
            def dropout_target(y):
                return tf.layers.dropout(
                    y, noise_shape=(tf.shape(y)[0], tf.shape(y)[1], 1),
                    rate=config.rnn_dropout_target,
                    training=self.inputs.training)

        dropout_embedding, dropout_hidden = None, None
        if config.rnn_use_dropout and config.rnn_dropout_embedding > 0.0:
            def dropout_embedding(e):
                return tf.layers.dropout(e, noise_shape=tf.shape(e),
                                         rate=config.rnn_dropout_embedding,
                                         training=self.inputs.training)
        if config.rnn_use_dropout and config.rnn_dropout_hidden > 0.0:
            def dropout_hidden(h):
                return tf.layers.dropout(h, noise_shape=tf.shape(h),
                                         rate=config.rnn_dropout_hidden,
                                         training=self.inputs.training)


        _, sent_len, word_len_x, batch_size = tf.unstack(tf.shape(self.inputs.x))
        _, word_len_y, _ = tf.unstack(tf.shape(self.inputs.y))

        self.word_input = tf.reshape(
            tf.transpose(self.inputs.x, (0, 2, 1, 3)),
            (config.factors, word_len_x, sent_len * batch_size))
        self.word_input_mask = tf.reshape(
            tf.transpose(self.inputs.x_mask, (1, 0, 2)),
            (word_len_x, sent_len * batch_size))

        self.input_y_2d = tf.reshape(
            tf.transpose(self.inputs.y, (1, 0, 2)),
            (word_len_y, sent_len * batch_size))
        self.input_y_mask_2d = tf.reshape(
            tf.transpose(self.inputs.y_mask, (1, 0, 2)),
            (word_len_y, sent_len * batch_size))


        self._init_word_encoder(config, sent_len * batch_size, dropout_source, dropout_embedding, dropout_hidden)
        self._init_word_attn(config, sent_len * batch_size, dropout_hidden)

        self.sent_input = tf.reshape(self.word_attn_context, (sent_len, batch_size, self.word_attn_context.shape[-1]))
        self.sent_input_mask = tf.sign(tf.reduce_sum(self.inputs.x_mask, axis=1))
        
        self._init_sent_encoder(config, batch_size, dropout_hidden)
        self._init_sent_attn(config, batch_size, dropout_hidden)
        
        # the second FC layer for text-Clf is abandoned
        with tf.variable_scope("fully-connect-1"):
            self.fc_layer_1 = layers.FeedForwardLayer(in_size=2 * config.sent_rnn_state_size, out_size=config.sec_repr_size, batch_size=batch_size)
            self.sec_repr = self.fc_layer_1.forward(self.sent_attn_context)
        
        with tf.variable_scope("decoder"):
            if config.tie_encoder_decoder_embeddings:
                tied_embeddings = self.encoder.emb_layer
            else:
                tied_embeddings = None
            self.decoder = Decoder(config, self.word_level_ctx, self.word_embeds, self.word_input_mask,
                                               dropout_target, dropout_embedding,
                                               dropout_hidden, tied_embeddings)

            # sec_repr is produced by HAN, which is not a part of decoder
            self.sec_repr_broadcast = tf.reshape(
                tf.broadcast_to(self.sec_repr, (sent_len, batch_size, config.sec_repr_size)),
                (sent_len * batch_size, config.sec_repr_size))
            self.logits = self.decoder.score(self.input_y_2d, self.sec_repr_broadcast)

        with tf.variable_scope("loss"):
            self.loss_layer = layers.Masked_cross_entropy_loss(
                self.input_y_2d, self.input_y_mask_2d, config.label_smoothing,
                training=self.inputs.training)
            self._loss_per_sentence = self.loss_layer.forward(self.logits)
            self._loss = tf.reduce_mean(self._loss_per_sentence, keepdims=False)

        self.sampling_utils = SamplingUtils(config)

    @property
    def loss_per_sentence(self):
        return self._loss_per_sentence