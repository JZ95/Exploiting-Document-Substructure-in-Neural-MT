"""
Re-define part of the configuraion
"""
from nematus.config import ConfigSpecification, ParameterSpecification
from nematus.config import _check_config_consistency, _construct_argument_parser
from nematus.config import _derive_dim_per_factor, _derive_model_version, _derive_rnn_dropout_embedding, _derive_rnn_dropout_hidden, \
                           _derive_source_dataset, _derive_source_vocab_sizes, _derive_target_vocab_size, _derive_valid_source_dataset, \
                           _derive_target_embedding_size
import collections
import logging
import sys
import argparse


class MyConfigSpecification(ConfigSpecification):
    def __init__(self):
        """Builds the collection of ParameterSpecifications."""

        # Define the parameter groups and their descriptions.
        description_pairs = [
            ('',                      None),
            ('data',                  'data sets; model loading and saving'),
            ('network',               'general network parameters'),
            ('network_rnn',           'network parameters (rnn-specific)'),
            ('training',              'training parameters'),
            ('validation',            'validation parameters'),
            ('display',               'display parameters'),
            ('translate',             'translate parameters'),
            ('sampling',              'sampling parameters'),
        ]
        self._group_descriptions = collections.OrderedDict(description_pairs)

        # Add all the ParameterSpecification objects.
        self._param_specs = self._define_param_specs()

        # Check that there are no duplicated names.
        self._check_self()

        # Build a dictionary for looking up ParameterSpecifications by name.
        self._name_to_spec = self._build_name_to_spec()

    def _define_param_specs(self):
        """Adds all ParameterSpecification objects."""
        param_specs = {}

        # Add an empty list for each parameter group.
        for group in self.group_names:
            param_specs[group] = []

        # Add non-command-line parameters.
        group = param_specs['']

        group.append(ParameterSpecification(
            name='model_version', default=None,
            derivation_func=_derive_model_version))

        group.append(ParameterSpecification(
            name='theano_compat', default=None,
            derivation_func=lambda _, meta_config: meta_config.from_theano))

        group.append(ParameterSpecification(
            name='source_dicts', default=None,
            derivation_func=lambda config, _: config.dictionaries[:] if config.mode == 'clf' else config.dictionaries[:-1]))

        group.append(ParameterSpecification(
            name='target_dict', default=None,
            derivation_func=lambda config, _: None if config.mode == 'clf' else config.dictionaries[-1]))

        group.append(ParameterSpecification(
            name='target_embedding_size', default=None,
            derivation_func=_derive_target_embedding_size))

        # All remaining parameters are command-line parameters.

        # Add command-line parameters for the 'data' group.

        group = param_specs['data']

        group.append(ParameterSpecification(
            name='dataset', default=None,
            visible_arg_names=['--dataset'],
            type=str, metavar='PATH',
            help='training corpus (require XML data)'))

        group.append(ParameterSpecification(
            name='dictionaries', default=None,
            visible_arg_names=['--dictionaries'], hidden_arg_names=[],
            type=str, required=True, metavar='PATH', nargs='+',
            help='network vocabularies (one per source factor, plus target '
                 'vocabulary if choosing nmt mode)'))

        group.append(ParameterSpecification(
            name='save_freq', default=30000,
            legacy_names=['saveFreq'],
            visible_arg_names=['--save_freq'], hidden_arg_names=['--saveFreq'],
            type=int, metavar='INT',
            help='save frequency (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='saveto', default='model',
            visible_arg_names=['--model'], hidden_arg_names=['--saveto'],
            type=str, metavar='PATH',
            help='model file name (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='reload', default=None,
            visible_arg_names=['--reload'],
            type=str, metavar='PATH',
            help='load existing model from this path. Set to '
                 '"latest_checkpoint" to reload the latest checkpoint in the '
                 'same directory of --model'))

        group.append(ParameterSpecification(
            name='reload_training_progress', default=True,
            visible_arg_names=['--no_reload_training_progress'],
            action='store_false',
            help='don\'t reload training progress (only used if --reload '
                 'is enabled)'))

        group.append(ParameterSpecification(
            name='summary_dir', default=None,
            visible_arg_names=['--summary_dir'],
            type=str, metavar='PATH',
            help='directory for saving summaries (default: same directory '
                 'as the --model file)'))

        group.append(ParameterSpecification(
            name='summary_freq', default=0,
            legacy_names=['summaryFreq'],
            visible_arg_names=['--summary_freq'],
            hidden_arg_names=['--summaryFreq'],
            type=int, metavar='INT',
            help='Save summaries after INT updates, if 0 do not save '
                 'summaries (default: %(default)s)'))

        # Add command-line parameters for 'network' group.

        group = param_specs['network']

        group.append(ParameterSpecification(
            name='mode', default='clf',
            visible_arg_names=['--mode'],
            type=str, choices=['clf', 'nmt'],
            help='training a text-classifier or seq2seq context-aware neural MT(default: %(default)s)'))

        group.append(ParameterSpecification(
            name='model_type', default='rnn',
            visible_arg_names=['--model_type'],
            type=str, choices=['rnn'],
            help='model type (default: %(default)s), Transformer is not supported here.'))

        group.append(ParameterSpecification(
            name='embedding_size', default=512,
            legacy_names=['dim_word'],
            visible_arg_names=['--embedding_size'],
            hidden_arg_names=['--dim_word'],
            type=int, metavar='INT',
            help='embedding layer size (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='source_vocab_sizes', default=None,
            visible_arg_names=['--source_vocab_sizes'],
            hidden_arg_names=['--n_words_src'],
            derivation_func=_derive_source_vocab_sizes,
            type=int, metavar='INT', nargs='+',
            help='source vocabulary sizes (one per input factor) (default: '
                 '%(default)s)'))

        group.append(ParameterSpecification(
            name='target_vocab_size', default=-1,
            legacy_names=['n_words'],
            visible_arg_names=['--target_vocab_size'],
            hidden_arg_names=['--n_words'],
            derivation_func=_derive_target_vocab_size,
            type=int, metavar='INT',
            help='target vocabulary size (default: %(default)s) only useful in nmt mode'))

        group.append(ParameterSpecification(
            name='factors', default=1,
            visible_arg_names=['--factors'],
            type=int, metavar='INT',
            help='number of input factors (default: %(default)s) - CURRENTLY '
                 'ONLY WORKS FOR \'rnn\' MODEL'))

        group.append(ParameterSpecification(
            name='dim_per_factor', default=None,
            visible_arg_names=['--dim_per_factor'],
            derivation_func=_derive_dim_per_factor,
            type=int, metavar='INT', nargs='+',
            help='list of word vector dimensionalities (one per factor): '
                 '\'--dim_per_factor 250 200 50\' for total dimensionality '
                 'of 500 (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='tie_encoder_decoder_embeddings', default=False,
            visible_arg_names=['--tie_encoder_decoder_embeddings'],
            action='store_true',
            help='tie the input embeddings of the encoder and the decoder '
                 '(first factor only). Source and target vocabulary size '
                 'must be the same'))

        group.append(ParameterSpecification(
            name='tie_decoder_embeddings', default=False,
            visible_arg_names=['--tie_decoder_embeddings'],
            action='store_true',
            help='tie the input embeddings of the decoder with the softmax '
                 'output embeddings'))

        group.append(ParameterSpecification(
            name='output_hidden_activation', default='tanh',
            visible_arg_names=['--output_hidden_activation'],
            type=str, choices=['tanh', 'relu', 'prelu', 'linear'],
            help='activation function in hidden layer of the output '
                 'network (default: %(default)s) - CURRENTLY ONLY WORKS '
                 'FOR \'rnn\' MODEL'))

        group.append(ParameterSpecification(
            name='softmax_mixture_size', default=1,
            visible_arg_names=['--softmax_mixture_size'],
            type=int, metavar='INT',
            help='number of softmax components to use (default: '
                 '%(default)s) - CURRENTLY ONLY WORKS FOR \'rnn\' MODEL'))

        # Add command-line parameters for 'network_rnn' group.

        group = param_specs['network_rnn']

        # NOTE: parameter names in this group must use the rnn_ prefix.
        #       read_config_from_cmdline() uses this to check that only
        #       model type specific options are only used with the appropriate
        #       model type.
        
        # SETTING FOR WORD RNN
        group.append(ParameterSpecification(
            name='word_rnn_enc_depth', default=1,
            visible_arg_names=['--word_rnn_enc_depth'],
            type=int, metavar='INT',
            help='number of word-level-encoder layers (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='word_rnn_enc_transition_depth', default=1,
            visible_arg_names=['--word_rnn_enc_transition_depth'],
            type=int, metavar='INT',
            help='number of GRU transition operations applied in the '
                 'word-level-encoder. Minimum is 1. (Only applies to gru). (default: '
                 '%(default)s)'))

        group.append(ParameterSpecification(
            name='word_rnn_state_size', default=1000,
            visible_arg_names=['--word_rnn_state_size'], 
            type=int, metavar='INT',
            help='word level rnn hidden state size (default: %(default)s)'))
        
        # SETTING FOR SENT RNN
        group.append(ParameterSpecification(
            name='sent_rnn_enc_depth', default=1,
            visible_arg_names=['--sent_rnn_enc_depth'],
            type=int, metavar='INT',
            help='number of sent-level-encoder layers (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='sent_rnn_enc_transition_depth', default=1,
            visible_arg_names=['--sent_rnn_enc_transition_depth'],
            type=int, metavar='INT',
            help='number of GRU transition operations applied in the '
                 'sent-level-encoder. Minimum is 1. (Only applies to gru). (default: '
                 '%(default)s)'))

        group.append(ParameterSpecification(
            name='sent_rnn_state_size', default=500,
            visible_arg_names=['--sent_rnn_state_size'],
            type=int, metavar='INT',
            help='sent level rnn hidden state size (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='sec_repr_size', default=500,
            visible_arg_names=['--sec_repr_size'],
            type=int, metavar='INT',
            help='representation size for section (default: %(default)s)'))
        
        group.append(ParameterSpecification(
            name='rnn_dec_depth', default=1,
            legacy_names=['dec_depth'],
            visible_arg_names=['--rnn_dec_depth'],
            hidden_arg_names=['--dec_depth'],
            type=int, metavar='INT',
            help='number of decoder layers (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='rnn_dec_base_transition_depth', default=2,
            legacy_names=['dec_base_recurrence_transition_depth'],
            visible_arg_names=['--rnn_dec_base_transition_depth'],
            hidden_arg_names=['--dec_base_recurrence_transition_depth'],
            type=int, metavar='INT',
            help='number of GRU transition operations applied in the first '
                 'layer of the decoder. Minimum is 2.  (Only applies to '
                 'gru_cond). (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='rnn_dec_high_transition_depth', default=1,
            legacy_names=['dec_high_recurrence_transition_depth'],
            visible_arg_names=['--rnn_dec_high_transition_depth'],
            hidden_arg_names=['--dec_high_recurrence_transition_depth'],
            type=int, metavar='INT',
            help='number of GRU transition operations applied in the higher '
                 'layers of the decoder. Minimum is 1. (Only applies to '
                 'gru). (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='rnn_dec_deep_context', default=False,
            legacy_names=['dec_deep_context'],
            visible_arg_names=['--rnn_dec_deep_context'],
            hidden_arg_names=['--dec_deep_context'],
            action='store_true',
            help='pass context vector (from first layer) to deep decoder '
                 'layers'))

        # GENERAL SETTING FOR RNN
        group.append(ParameterSpecification(
            name='rnn_use_dropout', default=False,
            legacy_names=['use_dropout'],
            visible_arg_names=['--rnn_use_dropout'],
            hidden_arg_names=['--use_dropout'],
            action='store_true',
            help='use dropout layer (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='rnn_dropout_embedding', default=None,
            legacy_names=['dropout_embedding'],
            visible_arg_names=['--rnn_dropout_embedding'],
            hidden_arg_names=['--dropout_embedding'],
            derivation_func=_derive_rnn_dropout_embedding,
            type=float, metavar='FLOAT',
            # FIXME rnn_dropout_embedding effectively has two defaults,
            #       depending on whether we're reading from the command-
            #       line or from a JSON config - does this make sense?
            #       We hardcode the former here.
            help='dropout for input embeddings (0: no dropout) (default: '
                 '0.2)'))

        group.append(ParameterSpecification(
            name='rnn_dropout_hidden', default=None,
            legacy_names=['dropout_hidden'],
            visible_arg_names=['--rnn_dropout_hidden'],
            hidden_arg_names=['--dropout_hidden'],
            derivation_func=_derive_rnn_dropout_hidden,
            type=float, metavar='FLOAT',
            # FIXME rnn_dropout_hidden effectively has two defaults,
            #       depending on whether we're reading from the command-
            #       line or from a JSON config - does this make sense?
            #       We hardcode the former here.
            help='dropout for hidden layer (0: no dropout) (default: 0.2)'))

        group.append(ParameterSpecification(
            name='rnn_dropout_source', default=0.0,
            legacy_names=['dropout_source'],
            visible_arg_names=['--rnn_dropout_source'],
            hidden_arg_names=['--dropout_source'],
            type=float, metavar='FLOAT',
            help='dropout source words (0: no dropout) (default: '
                 '%(default)s)'))

        group.append(ParameterSpecification(
            name='rnn_layer_normalization', default=False,
            legacy_names=['use_layer_norm', 'layer_normalisation'],
            visible_arg_names=['--rnn_layer_normalisation'],
            hidden_arg_names=['--use_layer_norm', '--layer_normalisation'],
            action='store_true',
            help='Set to use layer normalization in encoder and decoder'))

        group.append(ParameterSpecification(
            name='rnn_lexical_model', default=False,
            legacy_names=['lexical_model'],
            visible_arg_names=['--rnn_lexical_model'],
            hidden_arg_names=['--lexical_model'],
            action='store_true',
            help='Enable feedforward lexical model (Nguyen and Chiang, 2018)'))

        # Add command-line parameters for 'training' group.

        group = param_specs['training']

        group.append(ParameterSpecification(
            name='loss_function', default='cross-entropy',
            visible_arg_names=['--loss_function'],
            type=str, choices=['cross-entropy'],
            help='loss function (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='decay_c', default=0.0,
            visible_arg_names=['--decay_c'],
            type=float, metavar='FLOAT',
            help='L2 regularization penalty (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='map_decay_c', default=0.0,
            visible_arg_names=['--map_decay_c'],
            type=float, metavar='FLOAT',
            help='MAP-L2 regularization penalty towards original weights '
                 '(default: %(default)s)'))

        group.append(ParameterSpecification(
            name='prior_model', default=None,
            visible_arg_names=['--prior_model'],
            type=str, metavar='PATH',
            help='Prior model for MAP-L2 regularization. Unless using '
                 '\"--reload\", this will also be used for initialization.'))

        group.append(ParameterSpecification(
            name='clip_c', default=1.0,
            visible_arg_names=['--clip_c'],
            type=float, metavar='FLOAT',
            help='gradient clipping threshold (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='label_smoothing', default=0.0,
            visible_arg_names=['--label_smoothing'],
            type=float, metavar='FLOAT',
            help='label smoothing (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='optimizer', default='adam',
            visible_arg_names=['--optimizer'],
            type=str, choices=['adam'],
            help='optimizer (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='adam_beta1', default=0.9,
            visible_arg_names=['--adam_beta1'],
            type=float, metavar='FLOAT',
            help='exponential decay rate for the first moment estimates '
                 '(default: %(default)s)'))

        group.append(ParameterSpecification(
            name='adam_beta2', default=0.999,
            visible_arg_names=['--adam_beta2'],
            type=float, metavar='FLOAT',
            help='exponential decay rate for the second moment estimates '
                 '(default: %(default)s)'))

        group.append(ParameterSpecification(
            name='adam_epsilon', default=1e-08,
            visible_arg_names=['--adam_epsilon'],
            type=float, metavar='FLOAT',
            help='constant for numerical stability (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='learning_schedule', default='constant',
            visible_arg_names=['--learning_schedule'],
            type=str, choices=['constant'],
            help='learning schedule (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='learning_rate', default=0.0001,
            visible_arg_names=['--learning_rate'],
            hidden_arg_names=['--lrate'],
            legacy_names=['lrate'],
            type=float, metavar='FLOAT',
            help='learning rate (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='warmup_steps', default=8000,
            visible_arg_names=['--warmup_steps'],
            type=int, metavar='INT',
            help='number of initial updates during which the learning rate is '
                 'increased linearly during learning rate scheduling '
                 '(default: %(default)s)'))

        group.append(ParameterSpecification(
            name='maxlen_word', default=100,
            visible_arg_names=['--maxlen-word'],
            type=int, metavar='INT',
            help='maximum sentence length (counted by words) for training and validation '
                 '(default: %(default)s)'))
        
        group.append(ParameterSpecification(
            name='maxlen_sent', default=100,
            visible_arg_names=['--maxlen-sent'],
            type=int, metavar='INT',
            help='maximum section length (counted by sentences) for training and validation '
                 '(default: %(default)s)'))

        group.append(ParameterSpecification(
            name='batch_size', default=80,
            visible_arg_names=['--batch_size'],
            type=int, metavar='INT',
            help='minibatch size (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='token_batch_size', default=0,
            visible_arg_names=['--token_batch_size'],
            type=int, metavar='INT',
            help='minibatch size (expressed in number of source or target '
                 'tokens). Sentence-level minibatch size will be dynamic. If '
                 'this is enabled, batch_size only affects sorting by '
                 'length. (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='max_sentences_per_device', default=0,
            visible_arg_names=['--max_sentences_per_device'],
            type=int, metavar='INT',
            help='maximum size of minibatch subset to run on a single device, '
                 'in number of sentences (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='max_tokens_per_device', default=0,
            visible_arg_names=['--max_tokens_per_device'],
            type=int, metavar='INT',
            help='maximum size of minibatch subset to run on a single device, '
                 'in number of tokens (either source or target - whichever is '
                 'highest) (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='gradient_aggregation_steps', default=1,
            visible_arg_names=['--gradient_aggregation_steps'],
            type=int, metavar='INT',
            help='number of times to accumulate gradients before aggregating '
                 'and applying; the minibatch is split between steps, so '
                 'adding more steps allows larger minibatches to be used '
                 '(default: %(default)s)'))

        group.append(ParameterSpecification(
            name='maxibatch_size', default=20,
            visible_arg_names=['--maxibatch_size'],
            type=int, metavar='INT',
            help='size of maxibatch (number of minibatches that are sorted '
                 'by length) (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='sort_by_length', default=True,
            visible_arg_names=['--no_sort_by_length'],
            action='store_false',
            help='do not sort sentences in maxibatch by length'))

        group.append(ParameterSpecification(
            name='shuffle_each_epoch', default=True,
            visible_arg_names=['--no_shuffle'],
            action='store_false',
            help='disable shuffling of training data (for each epoch)'))

        group.append(ParameterSpecification(
            name='keep_train_set_in_memory', default=False,
            visible_arg_names=['--keep_train_set_in_memory'],
            action='store_true',
            help='Keep training dataset lines stores in RAM during training'))

        group.append(ParameterSpecification(
            name='max_epochs', default=5000,
            visible_arg_names=['--max_epochs'],
            type=int, metavar='INT',
            help='maximum number of epochs (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='finish_after', default=10000000,
            visible_arg_names=['--finish_after'],
            type=int, metavar='INT',
            help='maximum number of updates (minibatches) (default: '
                 '%(default)s)'))

        # Add command-line parameters for 'validation' group.

        group = param_specs['validation']

        group.append(ParameterSpecification(
            name='valid_dataset', default=None,
            visible_arg_names=['--valid_dataset'],
            type=str, metavar='PATH',
            help='validation corpus (default: %(default)s), requre XML data'))

        group.append(ParameterSpecification(
            name='valid_batch_size', default=80,
            visible_arg_names=['--valid_batch_size'],
            type=int, metavar='INT',
            help='validation minibatch size (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='valid_token_batch_size', default=0,
            visible_arg_names=['--valid_token_batch_size'],
            type=int, metavar='INT',
            help='validation minibatch size (expressed in number of source '
                 'or target tokens). Sentence-level minibatch size will be '
                 'dynamic. If this is enabled, valid_batch_size only affects '
                 'sorting by length. (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='valid_freq', default=10000,
            legacy_names=['validFreq'],
            visible_arg_names=['--valid_freq'],
            hidden_arg_names=['--validFreq'],
            type=int, metavar='INT',
            help='validation frequency (default: %(default)s)'))

        group.append(ParameterSpecification(
            name='patience', default=10,
            visible_arg_names=['--patience'],
            type=int, metavar='INT',
            help='early stopping patience (default: %(default)s)'))

        # Add command-line parameters for 'display' group.

        group = param_specs['display']

        group.append(ParameterSpecification(
            name='disp_freq', default=1000,
            legacy_names=['dispFreq'],
            visible_arg_names=['--disp_freq'], hidden_arg_names=['--dispFreq'],
            type=int, metavar='INT',
            help='display loss after INT updates (default: %(default)s)'))
        group.append(ParameterSpecification(
            name='sample_freq', default=10000,
            legacy_names=['sampleFreq'],
            visible_arg_names=['--sample_freq'],
            hidden_arg_names=['--sampleFreq'],
            type=int, metavar='INT',
            help='display some samples after INT updates (default: '
                 '%(default)s)'))

        group.append(ParameterSpecification(
            name='beam_freq', default=10000,
            legacy_names=['beamFreq'],
            visible_arg_names=['--beam_freq'], hidden_arg_names=['--beamFreq'],
            type=int, metavar='INT',
            help='display some beam_search samples after INT updates '
                 '(default: %(default)s)'))

        group.append(ParameterSpecification(
            name='beam_size', default=12,
            visible_arg_names=['--beam_size'],
            type=int, metavar='INT',
            help='size of the beam (default: %(default)s)'))

        # Add command-line parameters for 'translate' group.

        group = param_specs['translate']

        group.append(ParameterSpecification(
            name='normalization_alpha', type=float, default=0.0, nargs="?",
            const=1.0, metavar="ALPHA",
            visible_arg_names=['--normalization_alpha'],
            help='normalize scores by sentence length (with argument, " \
                 "exponentiate lengths by ALPHA)'))

        group.append(ParameterSpecification(
            name='n_best', default=False,
            visible_arg_names=['--n_best'],
            action='store_true', dest='n_best',
            help='Print full beam'))

        group.append(ParameterSpecification(
            name='translation_maxlen', default=200,
            visible_arg_names=['--translation_maxlen'],
            type=int, metavar='INT',
            help='Maximum length of translation output sentence (default: '
                 '%(default)s)'))

        group.append(ParameterSpecification(
            name='translation_strategy', default='beam_search',
            visible_arg_names=['--translation_strategy'],
            type=str, choices=['beam_search', 'sampling'],
            help='translation_strategy, either beam_search or sampling (default: %(default)s)'))

        # Add command-line parameters for 'sampling' group.

        group = param_specs['sampling']

        group.append(ParameterSpecification(
            name='sampling_temperature', type=float, default=1.0,
            metavar="FLOAT",
            visible_arg_names=['--sampling_temperature'],
            help='softmax temperature used for sampling (default %(default)s)'))

        return param_specs


def my_read_config_from_cmdline():
    """Reads a config from the command-line.

    Logs an error and exits if the parameter values are not mutually
    consistent.

    Returns:
        An argparse.Namespace object representing the config.
    """

    spec = MyConfigSpecification()

    # Construct an argparse.ArgumentParser and parse command-line args.
    parser = _construct_argument_parser(spec)
    config = parser.parse_args()

    # Construct a second ArgumentParser but using default=argparse.SUPPRESS
    # in every argparse.add_argument() call. This allows us to determine
    # which parameters were actually set by the user.
    # Solution is from https://stackoverflow.com/a/45803037
    aux_parser = _construct_argument_parser(spec, suppress_missing=True)
    aux_config = aux_parser.parse_args()
    set_by_user = set(vars(aux_config).keys())
    # Perform consistency checks.
    error_messages = _check_config_consistency(spec, config, set_by_user)
    if len(error_messages) > 0:
        for msg in error_messages:
            logging.error(msg)
        sys.exit(1)

    # Set meta parameters.
    meta_config = argparse.Namespace()
    meta_config.from_cmdline = True
    meta_config.from_theano = False

    # Run derivation functions.
    for group in spec.group_names:
        for param in spec.params_by_group(group):
            if param.derivation_func is not None:
                setattr(config, param.name,
                        param.derivation_func(config, meta_config))

    return config


def _check_config_consistency(spec, config, set_by_user):
    """Performs consistency checks on a config read from the command-line.

    Args:
        spec: a ConfigSpecification object.
        config: an argparse.Namespace object.
        set_by_user: a set of strings representing parameter names.

    Returns:
        A list of error messages, one for each check that failed. An empty
        list indicates that all checks passed.
    """

    def arg_names_string(param):
        arg_names = param.visible_arg_names + param.hidden_arg_names
        return ' / '.join(arg_names)

    error_messages = []

    # Check parameters are appropriate for the model type.
    assert config.model_type is not None
    for group in spec.group_names:
        for param in spec.params_by_group(group):
            if param.name not in set_by_user:
                continue

    # Check user-supplied learning schedule options are consistent.
    if config.learning_schedule == 'constant':
        param = spec.lookup('warmup_steps')
        assert param is not None
        if param.name in set_by_user:
            msg = '{} cannot be used with \'constant\' learning ' \
                   'schedule'.format(arg_names_string(param),
                                     config.model_type)
            error_messages.append(msg)

    # TODO Other similar checks? e.g. check user hasn't set adam parameters
    #       if optimizer != 'adam' (not currently possible but probably will
    #       be in in the future)...

    # Check if user is trying to use the Transformer with features that
    # aren't supported yet.
    if not config.dataset:
        msg = '--dataset is required'
        error_messages.append(msg)

    if (config.source_vocab_sizes is not None and
            len(config.source_vocab_sizes) > config.factors):
        msg = 'too many values supplied to \'--source_vocab_sizes\' option ' \
              '(expected one per factor = {})'.format(config.factors)
        error_messages.append(msg)

    if config.dim_per_factor is None and config.factors != 1:
        msg = 'if using factored input, you must specify \'dim_per_factor\''
        error_messages.append(msg)

    if config.dim_per_factor is not None:
        if len(config.dim_per_factor) != config.factors:
            msg = 'mismatch between \'--factors\' ({0}) and ' \
                  '\'--dim_per_factor\' ({1} entries)'.format(
                      config.factors, len(config.dim_per_factor))
            error_messages.append(msg)
        elif sum(config.dim_per_factor) != config.embedding_size:
            msg = 'mismatch between \'--embedding_size\' ({0}) and ' \
                  '\'--dim_per_factor\' (sums to {1})\''.format(
                      config.embedding_size, sum(config.dim_per_factor))
            error_messages.append(msg)

    if config.mode == 'clf':
        if len(config.dictionaries) != config.factors:
            msg = '\'--dictionaries\' must specify one dictionary per source factor'
            error_messages.append(msg)
    else:
        if len(config.dictionaries) != config.factors + 1:
            msg = '\'--dictionaries\' must specify one dictionary per source factor plus one target dictionary'
            error_messages.append(msg)

    max_sents_param = spec.lookup('max_sentences_per_device')
    max_tokens_param = spec.lookup('max_tokens_per_device')

    # TODO Extend ParameterSpecification to support mutually exclusive
    #      command-line args.
    if (max_sents_param.name in set_by_user
        and max_tokens_param.name in set_by_user):
        msg = '{} is mutually exclusive with {}'.format(
            arg_names_string(max_sents_param),
            arg_names_string(max_tokens_param))
        error_messages.append(msg)

    aggregation_param = spec.lookup('gradient_aggregation_steps')

    if (aggregation_param.name in set_by_user
        and (max_sents_param.name in set_by_user
             or max_tokens_param.name in set_by_user)):
        msg = '{} is mutually exclusive with {} / {}'.format(
            arg_names_string(aggregation_param),
            arg_names_string(max_sents_param),
            arg_names_string(max_tokens_param))
        error_messages.append(msg)

    # softmax_mixture_size and lexical_model are currently mutually exclusive:
    if config.softmax_mixture_size > 1 and config.rnn_lexical_model:
       error_messages.append('behavior of --rnn_lexical_model is undefined if softmax_mixture_size > 1')


    return error_messages