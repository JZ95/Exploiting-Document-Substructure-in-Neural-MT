import numpy
import math
from nematus.model_updater import ModelUpdater, _ModelUpdateGraph


class MyModelUpdater(ModelUpdater):
    """ redefine several member functions
    """
    def __init__(self, config, num_gpus, replicas, optimizer, global_step,
                 summary_writer=None):
        super(MyModelUpdater, self).__init__(config, num_gpus, replicas, optimizer, global_step, summary_writer)
    

    def _split_minibatch_into_n(self, x_mask, y_mask, n):
        """Determines how to split a minibatch into n equal-sized sub-batches.

        The sub-batch size is (approximately) the minibatch size divided by n,
        where size is defined as the number of source + target tokens.

        Args:
            x_mask: Numpy array with shape (sent_len, word_len, batch_size)
            y_mask: Numpy array with shape (1, batch_size)
            n: int

        Returns:
            A list of indices representing the starting points of each
            sub-batch.
        """

        word_lens = numpy.sum(x_mask, axis=1)
        sent_lens = (word_lens > 0).sum(axis=0)
        assert sent_lens.shape[-1] == word_lens.shape[-1]
        num_secs = x_mask.shape[-1]

        mem_total = word_lens.max() * sent_lens.max() * num_secs
        mem_per_sample = numpy.max(word_lens, axis=0) * numpy.max(sent_lens, axis=0)
        soft_limit = math.ceil(mem_total / n)

        start_points = [0]
        while True:
            i = start_points[-1]
            mem_largest = mem_per_sample[i]
            next_start_point = None
            for j in range(i+1, num_secs):
                mem_largest = max(mem_largest, mem_per_sample[j])
                mem = mem_largest * (j-i+1)
                if mem > soft_limit:
                    # Allow the sub-batch to be over-filled, but only by one
                    # sentence worth of tokens.
                    next_start_point = j + 1
                    break
            if next_start_point is None or next_start_point >= num_secs:
                break
            start_points.append(next_start_point)

        assert len(start_points) <= n
        return start_points

    def _split_minibatch_for_device_size(self, x_mask, y_mask,
                                         max_sents_per_device=0,
                                         max_tokens_per_device=0):
        """Determines how to split a minibatch into device-sized sub-batches.

        Either max_sents_per_device or max_tokens_per_device must be given.

        Args:
            x_mask: Numpy array with shape (seq_len, batch_size)
            y_mask: Numpy array with shape (seq_len, batch_size)
            max_sents_per_device: int
            max_tokens_per_device: int

        Returns:
            A list of indices representing the starting points of each
            sub-batch.
        """
        raise NotImplementedError('device-sized sub-batches feature not supported yet')

        assert max_sents_per_device == 0 or max_tokens_per_device == 0
        assert not (max_sents_per_device == 0 and max_tokens_per_device == 0)

        # Determine where to split the minibatch to produce sub-batches that
        # fit the device capacity.
        if max_sents_per_device != 0:
            start_points = range(0, num_sents, max_sents_per_device)
        else:
            source_lengths = numpy.sum(x_mask, axis=0)
            target_lengths = numpy.sum(y_mask, axis=0)
            assert len(source_lengths) == len(target_lengths)
            num_sents = len(source_lengths)

            start_points = [0]
            while True:
                i = start_points[-1]
                s_longest = source_lengths[i]
                t_longest = target_lengths[i]
                next_start_point = None
                for j in range(i+1, num_sents):
                    s_longest = max(s_longest, source_lengths[j])
                    t_longest = max(t_longest, target_lengths[j])
                    s_tokens = s_longest * (j-i+1)
                    t_tokens = t_longest * (j-i+1)
                    if (s_tokens > max_tokens_per_device
                        or t_tokens > max_tokens_per_device):
                        next_start_point = j
                        break
                if next_start_point is None:
                    break
                start_points.append(next_start_point)

        return start_points

    def _split_and_pad_minibatch(self, x, x_mask, y, y_mask, start_points):
        """Splits a minibatch according to a list of split points.

        Args:
            x: Numpy array with shape (factors, sent_len, word_len, batch_size)
            x_mask: Numpy array with shape (sent_len, word_len, batch_size)
            y: Numpy array with shape (1, batch_size)
            y_mask: Numpy array with shape (1, batch_size)
            start_points: list of zero-based indices

        Returns:
            Five lists: for each of x, x_mask, y, and y_mask, respectively,
            a list is returned containing the split version. The fifth list
            contains the (unnormalized) weights of the sub-batches.
        """

        # Split the individual arrays.

        def split_array(a, start_points):
            batch_size = a.shape[-1]
            next_points = start_points[1:] + [batch_size]
            return [a[..., p:q] for p, q in zip(start_points, next_points)]

        split_x = split_array(x, start_points)
        split_x_mask = split_array(x_mask, start_points)
        split_y = split_array(y, start_points)
        split_y_mask = split_array(y_mask, start_points)

        # Trim arrays so that the seq_len dimension is equal to the longest
        # source / target sentence in the sub-batch (rather than the whole
        # minibatch).

        def trim_arrays(arrays, new_seq_lens):
            return [a[..., 0:l, :] for a, l in zip(arrays, new_seq_lens)]

        max_lens = [int(numpy.max(numpy.sum(m, axis=0))) for m in split_x_mask]
        split_x = trim_arrays(split_x, max_lens)
        split_x_mask = trim_arrays(split_x_mask, max_lens)

        max_lens = [int(numpy.max(numpy.sum(m, axis=0))) for m in split_y_mask]
        split_y = trim_arrays(split_y, max_lens)
        split_y_mask = trim_arrays(split_y_mask, max_lens)

        # Compute the weight of each sub-batch by summing the number of
        # source and target tokens. Note that this counts actual tokens
        # (up to and including the <EOS> tokens) not the capacity of the
        # sub-batch.
        weights = [numpy.sum(s) + numpy.sum(t)
                      for s, t in zip(split_x_mask, split_y_mask)]

        # Pad the split lists with dummy arrays so that the total number of
        # sub-batches is a multiple of the number of replicas.

        remainder = len(start_points) % len(self._replicas)
        padding_size = 0 if remainder == 0 else len(self._replicas) - remainder

        def pad(split_a, padding_size):
            assert len(split_a) > 0
            dummy_array = split_a[0][..., -1:]
            for i in range(padding_size):
                split_a.append(dummy_array)

        pad(split_x, padding_size)
        pad(split_x_mask, padding_size)
        pad(split_y, padding_size)
        pad(split_y_mask, padding_size)

        for i in range(padding_size):
            weights.append(0.0)
        
        return split_x, split_x_mask, split_y, split_y_mask, weights