import numpy
import math
from nematus.model_updater import ModelUpdater, _ModelUpdateGraph


class MyModelUpdater(ModelUpdater):
    """ redefine several member functions
    """
    def __init__(self, config, num_gpus, replicas, optimizer, global_step,
                 summary_writer=None):
        super(MyModelUpdater, self).__init__(config, num_gpus, replicas, optimizer, global_step, summary_writer)
    
    def _split_minibatch_into_n(self, x_mask, y_mask, n):
        if self._config.mode == 'clf':
            return __split_minibatch_into_n_clf(x_mask, y_mask, n)
        else:
            return __split_minibatch_into_n_nmt(x_mask, y_mask, n)

    def _split_minibatch_for_device_size(self, x_mask, y_mask,
                                         max_sents_per_device=0,
                                         max_tokens_per_device=0):
        """Determines how to split a minibatch into device-sized sub-batches.

        Either max_sents_per_device or max_tokens_per_device must be given.

        Args:
            x_mask: Numpy array with shape (seq_len, batch_size)
            y_mask: Numpy array with shape (seq_len, batch_size)
            max_sents_per_device: int
            max_tokens_per_device: int

        Returns:
            A list of indices representing the starting points of each
            sub-batch.
        """
        raise NotImplementedError('device-sized sub-batches feature not supported yet')
    
    def _split_and_pad_minibatch(self, x, x_mask, y, y_mask, start_points):
        if self._config.mode == 'clf':
            return __split_n_pad_minibatch_clf(x, x_mask, y, y_mask, start_points, len(self._replicas))
        else:
            return __split_n_pad_minibatch_nmt(x, x_mask, y, y_mask, start_points, len(self._replicas))


def __split_minibatch_into_n_clf(x_mask, y_mask, n):
    """
    Args:
        x_mask: (sent_len, word_len, batch_size)
        y_mask: (1, batch_size)
        n: int

    Returns:
        A list of indices representing the starting points of each
        sub-batch.
    """
    word_lens = numpy.sum(x_mask, axis=1)
    sent_lens = (word_lens > 0).sum(axis=0)
    assert sent_lens.shape[-1] == word_lens.shape[-1]
    num_secs = x_mask.shape[-1]

    mem_total = word_lens.max() * sent_lens.max() * num_secs
    mem_per_sample = numpy.max(word_lens, axis=0) * numpy.max(sent_lens, axis=0)
    soft_limit = math.ceil(mem_total / n)

    start_points = [0]
    while True:
        i = start_points[-1]
        mem_largest = mem_per_sample[i]
        next_start_point = None
        for j in range(i+1, num_secs):
            mem_largest = max(mem_largest, mem_per_sample[j])
            mem = mem_largest * (j-i+1)
            if mem > soft_limit:
                # Allow the sub-batch to be over-filled, but only by one
                # sentence worth of tokens.
                next_start_point = j + 1
                break
        if next_start_point is None or next_start_point >= num_secs:
            break
        start_points.append(next_start_point)

    assert len(start_points) <= n
    return start_points


def __split_minibatch_into_n_nmt(x_mask, y_mask, n):
    """
    Args:
        x_mask: (sent_len, word_len, batch_size)
        y_mask: (sent_len, word_len, batch_size)
        n: int

    Returns:
        A list of indices representing the starting points of each
        sub-batch.
    """
    word_lens_x = numpy.sum(x_mask, axis=1)
    word_lens_y = numpy.sum(y_mask, axis=1)
    
    sent_lens_x = (word_lens_x > 0).sum(axis=0)
    sent_lens_y = (word_lens_y > 0).sum(axis=0)
    sent_lens = sent_lens_x
    num_secs = x_mask.shape[-1]

    assert (sent_lens_x != sent_lens_y).sum() == 0
    assert sent_lens.shape[-1] == word_lens_x.shape[-1]

    mem_x_total = word_lens_x.max() * sent_lens.max() * num_secs
    mem_y_total = word_lens_y.max() * sent_lens.max() * num_secs
    mem_per_sample_x = numpy.max(word_lens_x, axis=0) * numpy.max(sent_lens, axis=0)
    mem_per_sample_y = numpy.max(word_lens_y, axis=0) * numpy.max(sent_lens, axis=0)
    
    soft_limit = math.ceil((mem_x_total + mem_y_total) / n)

    start_points = [0]
    while True:
        i = start_points[-1]
        mem_largest_x = mem_per_sample_x[i]
        mem_largest_y = mem_per_sample_y[i]
        next_start_point = None
        for j in range(i+1, num_secs):
            mem_largest_x = max(mem_largest_x, mem_per_sample_x[j])
            mem_largest_y = max(mem_largest_y, mem_per_sample_y[j])
            mem_x = mem_largest_x * (j-i+1)
            mem_y = mem_largest_y * (j-i+1)
            if mem_x + mem_y > soft_limit:
                # Allow the sub-batch to be over-filled, but only by one
                # sentence worth of tokens.
                next_start_point = j + 1
                break
        if next_start_point is None or next_start_point >= num_secs:
            break
        start_points.append(next_start_point)

    assert len(start_points) <= n
    return start_points


def __split_n_pad_minibatch_clf(x, x_mask, y, y_mask, start_points, n_replicas):
    """Splits a minibatch according to a list of split points.

    Args:
        x: Numpy array with shape (factors, sent_len, word_len, batch_size)
        x_mask: Numpy array with shape (sent_len, word_len, batch_size)
        y: Numpy array with shape (1, batch_size)
        y_mask: Numpy array with shape (1, batch_size)
        start_points: list of zero-based indices

    Returns:
        Five lists: for each of x, x_mask, y, and y_mask, respectively,
        a list is returned containing the split version. The fifth list
        contains the (unnormalized) weights of the sub-batches.
    """

    # Split the individual arrays.
    split_x = split_array(x, start_points)
    split_x_mask = split_array(x_mask, start_points)
    split_y = split_array(y, start_points)
    split_y_mask = split_array(y_mask, start_points)

    # Trim arrays so that the seq_len dimension is equal to the longest
    # source / target sentence in the sub-batch (rather than the whole
    # minibatch).

    max_word_lens = [int(m.sum(axis=1).max()) for m in split_x_mask]
    max_sent_lens = [int((m.sum(axis=1) > 0).sum(axis=0).max()) for m in split_x_mask]

    # only x-array should be trimmed
    split_x = trim_arrays(split_x, max_sent_lens, max_word_lens)
    split_x_mask = trim_arrays(split_x_mask, max_sent_lens, max_word_lens)

    # weights are useless in text classifier, thus would set as 1
    weights = [1 for _ in range(len(split_x_mask))]

    # Pad the split lists with dummy arrays so that the total number of
    # sub-batches is a multiple of the number of replicas.

    remainder = len(start_points) % n_replicas
    padding_size = 0 if remainder == 0 else n_replicas - remainder

    pad(split_x, padding_size)
    pad(split_x_mask, padding_size)
    pad(split_y, padding_size)
    pad(split_y_mask, padding_size)

    for i in range(padding_size):
        weights.append(0.0)

    return split_x, split_x_mask, split_y, split_y_mask, weights


def __split_n_pad_minibatch_nmt(x, x_mask, y, y_mask, start_points, n_replicas):
    """Splits a minibatch according to a list of split points.

    Args:
        x: Numpy array with shape (factors, sent_len, word_len, batch_size)
        x_mask: Numpy array with shape (sent_len, word_len, batch_size)
        y: Numpy array with shape (sent_len, word_len, batch_size)
        y_mask: Numpy array with shape (sent_len, word_len, batch_size)
        start_points: list of zero-based indices

    Returns:
        Five lists: for each of x, x_mask, y, and y_mask, respectively,
        a list is returned containing the split version. The fifth list
        contains the (unnormalized) weights of the sub-batches.
    """

    split_x = split_array(x, start_points)
    split_x_mask = split_array(x_mask, start_points)
    split_y = split_array(y, start_points)
    split_y_mask = split_array(y_mask, start_points)

    max_word_lens_x = [int(m.sum(axis=1).max()) for m in split_x_mask]
    max_word_lens_y = [int(m.sum(axis=1).max()) for m in split_y_mask]
    max_sent_lens = [int((m.sum(axis=1) > 0).sum(axis=0).max()) for m in split_x_mask]

    split_x = trim_arrays(split_x, max_sent_lens, max_word_lens_x)
    split_x_mask = trim_arrays(split_x_mask, max_sent_lens, max_word_lens_x)

    split_y = trim_arrays(split_y, max_sent_lens, max_word_lens_y)
    split_y_mask = trim_arrays(split_y_mask, max_sent_lens, max_word_lens_y)

    # weights are useless in text classifier, thus would set as 1
    weights = [1 for _ in range(len(split_x_mask))]

    # Pad the split lists with dummy arrays so that the total number of
    # sub-batches is a multiple of the number of replicas.

    remainder = len(start_points) % n_replicas
    padding_size = 0 if remainder == 0 else n_replicas - remainder

    pad(split_x, padding_size)
    pad(split_x_mask, padding_size)
    pad(split_y, padding_size)
    pad(split_y_mask, padding_size)

    for i in range(padding_size):
        weights.append(0.0)

    return split_x, split_x_mask, split_y, split_y_mask, weights


def split_array(a, start_points):
    batch_size = a.shape[-1]
    next_points = start_points[1:] + [batch_size]
    return [a[..., p:q] for p, q in zip(start_points, next_points)]


def trim_arrays(arrays, new_sent_lens, new_word_lens):
    return [ a[..., 0:sent_l, 0:word_l, :]
                for a, sent_l, word_l in 
                    zip(arrays, new_sent_lens, new_word_lens)]


def pad(split_a, padding_size):
    assert len(split_a) > 0
    # whatever the dummy array is,
    # it would be handled the weights
    dummy_array = split_a[0][..., -1:]
    for i in range(padding_size):
        split_a.append(dummy_array)