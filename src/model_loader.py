import json
import logging
import os
import re
import sys
import numpy
import tensorflow as tf
from tensorflow.contrib.framework.python.framework import checkpoint_utils
import tensorflow.contrib.slim as slim #tensorflow.contrib.framework ???
import training_progress


def init_or_restore_variables(config, sess, ensemble_scope=None, train=False):
    # Construct a mapping between saved variable names and names in the current
    # scope. There are two reasons why names might be different:
    #
    #   1. This model is part of an ensemble, in which case a model-specific
    #       name scope will be active.
    #
    #   2. The saved model is from an old version of Nematus (before deep model
    #        support was added) and uses a different variable naming scheme
    #        for the GRUs.

    accum_regex = re.compile('^accum\d+$')

    def is_excluded_variable(name):
        # Exclude gradient accumulation variables.
        if accum_regex.match(name):
            return True
        if name == 'accumulated_loss':
            return True
        return False

    variables = slim.get_variables_to_restore()
    
    var_map = {}
    for v in variables:
        name = v.name.split(':')[0]
        if ensemble_scope == None:
            saved_name = name
        elif v.name.startswith(ensemble_scope.name + "/"):
            saved_name = name[len(ensemble_scope.name)+1:]
            # The ensemble scope is repeated for Adam variables. See
            # https://github.com/tensorflow/tensorflow/issues/8120
            if saved_name.startswith(ensemble_scope.name + "/"):
                saved_name = saved_name[len(ensemble_scope.name)+1:]
        else: # v belongs to a different model in the ensemble.
            continue
        if is_excluded_variable(saved_name):
            continue

        var_map[saved_name] = v

    saver = tf.train.Saver(var_map, max_to_keep=None)

    # compute reload model filename
    reload_filename = None
    if config.reload == 'latest_checkpoint':
        checkpoint_dir = os.path.dirname(config.saveto)
        reload_filename = tf.train.latest_checkpoint(checkpoint_dir)
        if reload_filename != None:
            if (os.path.basename(reload_filename).rsplit('-', 1)[0] !=
                os.path.basename(config.saveto)):
                logging.error("Mismatching model filename found in the same directory while reloading from the latest checkpoint")
                sys.exit(1)
            logging.info('Latest checkpoint found in directory ' + os.path.abspath(checkpoint_dir))
    elif config.reload != None:
        reload_filename = config.reload

    # initialize or reload training progress
    if train:
        progress = training_progress.TrainingProgress()
        progress.bad_counter = 0
        progress.uidx = 0
        progress.eidx = 0
        progress.estop = False
        if config.mode == 'clf':
            progress.clf_errs = []
            progress.acc_scores = []
        elif config.mode in ('nmt', 'baseline-nmt'):
            progress.nmt_errs = []
            progress.bleu_scores = []
        elif config.mode == 'joint':
            progress.nmt_errs = []
            progress.bleu_scores = []
            progress.clf_errs = []
            progress.acc_scores = []

        if reload_filename and config.reload_training_progress:
            path = reload_filename + '.progress.json'
            if os.path.exists(path):
                logging.info('Reloading training progress')
                progress.load_from_json(path)
                if (progress.estop == True or
                    progress.eidx > config.max_epochs or
                    progress.uidx >= config.finish_after):
                    logging.warning('Training is already complete. Disable reloading of training progress (--no_reload_training_progress) or remove or modify progress file (%s) to train anyway.' % path)
                    sys.exit(0)

    init_op = tf.global_variables_initializer()

    # initialize or restore model
    if reload_filename:
        logging.info('Loading model parameters from file ' + os.path.abspath(reload_filename))
        
        shared_var_map = {}
        var_list = checkpoint_utils.list_variables(os.path.abspath(reload_filename))
        for var_name, _ in var_list:
            if var_name in var_map:
                shared_var_map[var_name] = var_map[var_name]

        if len(shared_var_map) < len(var_map):
            logging.info('Loading shared parameters:')
            for var_name in shared_var_map:
                logging.info('Loading ' + var_name)
        
        for var_name in var_map:
            if var_name not in shared_var_map:
                logging.info('Initializing %s from scratch' % var_name)

        aux_saver = tf.train.Saver(shared_var_map)

        # Initialize all variables even though most will be overwritten by
        # the subsequent saver.restore() call. This is to allow for variables
        # that are not saved to the checkpoint. Currently that is just the
        # gradient accumulation variables, which are unusual in that they
        # persist across multiple sessions during training (and therefore need
        # to be variables) but are regularly reset to zero.
        sess.run(init_op)
        aux_saver.restore(sess, os.path.abspath(reload_filename))

    elif config.load_params_from:
        sess.run(init_op)
        loaded_vars = set()
        for ckpt, var_scopes in config.load_params_from.items():
            tmp_vars = load_params(sess, ckpt, var_scopes, var_map)
            loaded_vars = loaded_vars.union(tmp_vars)

        for var_name in var_map:
            if var_name not in loaded_vars:
                logging.info('Initializing %s from scratch' % var_name)

    else:
        logging.info('Initializing all model parameters from scratch...')
        sess.run(init_op)
    logging.info('Done')

    if train:
        return saver, progress
    else:
        return saver


def load_params(sess, ckpt_path, var_scope_names, graph_vars):
    """
    Parameters:
        sess: tf.Session object
        ckpt_path: filename for the checkpoint to load the parameters from
        var_names: the name for the variable scope whose parameters need to be loaded
        graph_vars: all variables in the graph
    Returns:
        a set for the names of all the variables in the graph that are loaded from ckpt_path
    """
    logging.info('Loading model parameters from file ' + os.path.abspath(ckpt_path))
    tmp = {}
    for scope_name in var_scope_names:
        for var_name in graph_vars:
            if re.match(scope_name, var_name):
                tmp[var_name] = graph_vars[var_name]
                logging.info('Loading %s ' % var_name)
    
    if tmp:
        tmp_saver = tf.train.Saver(tmp)
        tmp_saver.restore(sess, os.path.abspath(ckpt_path))
    else:
        logging.warning('Do not load any parameters from ' + os.path.abspath(ckpt_path))
    return set(tmp.keys())
