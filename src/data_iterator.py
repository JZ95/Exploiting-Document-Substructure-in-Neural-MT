import numpy
from util import load_dict
from exception import Error
import shuffle
import gzip


def fopen(filename, mode='r'):
    if filename.endswith('.gz'):
        return gzip.open(filename, mode)
    return open(filename, mode)


def _determine_unk_val(d):
    if '<UNK>' in d and d['<UNK>'] == 2:
        return 2
    return 1


def _lookup_token(t, d, unk_val):
    return d[t] if t in d else unk_val


def _preprocessing(lines):
    """ remove segment symbol <p>, </p> and 
    include all the sentences between into one list
    """
    ret = []
    sec = []
    for line in lines:
        if line.strip() == '<p>':
            pass
        elif line.strip() == '</p>':
            ret.append(sec)
            sec = []
        else:
            sec.append(line.strip())
    return ret


class FileWrapper(object):
    def __init__(self, fname):
        self.pos = 0
        self.lines = fopen(fname).readlines()
        self.lines = numpy.array(_preprocessing(self.lines), dtype=numpy.object)  # use numpy array to enable shuffle

    def __iter__(self):
        return self

    def __next__(self):
        if self.pos >= len(self.lines):
            raise StopIteration
        l = self.lines[self.pos]
        self.pos += 1
        return l

    def reset(self):
        self.pos = 0

    def seek(self, pos):
        assert pos == 0
        self.pos = 0

    def readline(self):
        return next(self)

    def shuffle_lines(self, perm):
        self.lines = self.lines[perm]
        self.pos = 0

    def __len__(self):
        return len(self.lines)


class CLFTextIterator:
    """ Iterator for Section
    Only used in Training Text Classifier
    """
    def __init__(self, source_dataset, target_dataset,
                 source_dicts, target_dict=None,
                 batch_size=128,
                 maxlen_sent=100,
                 maxlen_word=100,
                 source_vocab_sizes=None,
                 skip_empty=False,
                 shuffle_each_epoch=False,
                 sort_by_length=True,
                 use_factor=False,
                 maxibatch_size=20,
                 token_batch_size=0,
                 keep_data_in_memory=False):

        if keep_data_in_memory:
            self.source, self.target = FileWrapper(source_dataset), FileWrapper(target_dataset)
            if shuffle_each_epoch:
                r = numpy.random.permutation(len(self.source))
                self.source.shuffle_lines(r)
                self.target.shuffle_lines(r)
        elif shuffle_each_epoch:
            raise NotImplementedError
            # self.source_orig = source_dataset
            # self.target_orig = target_dataset
            # self.source, self.target = shuffle.main([self.source_orig, self.target_orig], temporary=True)
        else:
            raise NotImplementedError
            # self.source = fopen(source_dataset, 'r')
            # self.target = fopen(target_dataset, 'r')
        
        # allow multi-factor input
        self.source_dicts = []
        for source_dict in source_dicts:
            self.source_dicts.append(load_dict(source_dict))
        
        self.target_dict = load_dict(target_dict)

        # Determine the UNK value for each dictionary (the value depends on
        # which version of build_dictionary.py was used).

        self.source_unk_vals = [_determine_unk_val(d)
                                for d in self.source_dicts]

        self.keep_data_in_memory = keep_data_in_memory
        self.batch_size = batch_size
        self.maxlen_sent = maxlen_sent
        self.maxlen_word = maxlen_word
        self.skip_empty = skip_empty
        self.use_factor = use_factor

        self.source_vocab_sizes = source_vocab_sizes
        self.token_batch_size = token_batch_size

        if self.source_vocab_sizes != None:
            assert len(self.source_vocab_sizes) == len(self.source_dicts)
            for d, vocab_size in zip(self.source_dicts, self.source_vocab_sizes):
                if vocab_size != None and vocab_size > 0:
                    for key, idx in list(d.items()):
                        if idx >= vocab_size:
                            del d[key]

        self.shuffle = shuffle_each_epoch
        self.sort_by_length = sort_by_length

        self.buffer = []
        # k is the buffer size, which controls how many senteces would be sorted
        self.k = batch_size * maxibatch_size

        self.end_of_data = False

    def __iter__(self):
        return self

    def reset(self):
        if self.shuffle:
            if self.keep_data_in_memory:
                r = numpy.random.permutation(len(self.source))
                self.source.shuffle_lines(r)
                self.target.shuffle_lines(r)
            else:
                self.source, self.target = shuffle.main([self.source_orig, self.target_orig], temporary=True)
        else:
            self.source.seek(0)
            self.target.seek(0)

    def __next__(self):
        if self.end_of_data:
            self.end_of_data = False
            self.reset()
            raise StopIteration
 
        # fill the buffer if empty
        if len(self.buffer) == 0:
            for text, label in zip(self.source, self.target):
                sec_text = []
                for sent in text:
                    # truncate sentence
                    sent = sent.split()[:self.maxlen_word]

                    if self.skip_empty and len(sent) == 0:
                        continue

                    sec_text.append(sent)

                sec_text = sec_text[:self.maxlen_sent]
                if self.skip_empty and len(sec_text) == 0:
                    continue
                label = label[0]
                self.buffer.append((sec_text, label))

                if len(self.buffer) == self.k:
                    break

            if len(self.buffer) == 0:
                self.end_of_data = False
                self.reset()
                raise StopIteration

            # sort by source/target buffer length
            if self.sort_by_length:
                raise NotImplementedError
                # tlen = numpy.array([max(len(s), len(t)) for (
                #     s, t) in zip(self.source_buffer, self.target_buffer)])
                # tidx = tlen.argsort()

                # _sbuf = [self.source_buffer[i] for i in tidx]
                # _tbuf = [self.target_buffer[i] for i in tidx]

                # self.source_buffer = _sbuf
                # self.target_buffer = _tbuf
            else:
                self.buffer.reverse()

        try:
            # actual work here
            batch_text_indices = []
            batch_label_indices = []
            while True:
                # read from source file and map to word index
                try:
                    sec_text, label = self.buffer.pop()
                except IndexError:
                    break
                
                sec_tmp = []
                for sent in sec_text:
                    sent_tmp = []
                    for w in sent:
                        if self.use_factor:
                            w = [_lookup_token(f, self.source_dicts[i],
                                            self.source_unk_vals[i])
                                for (i, f) in enumerate(w.split('|'))]
                        else:
                            w = [_lookup_token(w, self.source_dicts[0],
                                            self.source_unk_vals[0])]
                        sent_tmp.append(w)
                    sec_tmp.append(sent_tmp[:])
                
                sec_indices = sec_tmp
                label_index = self.target_dict[label]

                batch_text_indices.append(sec_indices[:])
                batch_label_indices.append(label_index)
                # longest_source = max(longest_source, len(ss_indices))
                # longest_target = max(longest_target, len(tt_indices))

                if self.token_batch_size:
                    raise NotImplementedError
                    # if len(source) * longest_source > self.token_batch_size or \
                    #         len(target) * longest_target > self.token_batch_size:
                    #     # remove last sentence pair (that made batch over-long)
                    #     source.pop()
                    #     target.pop()
                    #     self.source_buffer.append(ss)
                    #     self.target_buffer.append(tt)

                    #     break

                else:
                    if len(batch_text_indices) >= self.batch_size:
                        break
        except IOError:
            self.end_of_data = True

        assert len(batch_text_indices) == len(batch_label_indices)
        return batch_text_indices, batch_label_indices


class NMTTextIterator(CLFTextIterator):
    """ Iterator for Section
    Only used in Training Context Aware NMT
    """
    def __init__(self, source_dataset, target_dataset,
                 source_dicts, target_dict,
                 batch_size=128,
                 maxlen_sent=100,
                 maxlen_word=100,
                 source_vocab_sizes=None,
                 target_vocab_size=None,
                 skip_empty=False,
                 shuffle_each_epoch=False,
                 sort_by_length=True,
                 use_factor=False,
                 maxibatch_size=20,
                 token_batch_size=0,
                 keep_data_in_memory=False):
        
        super(NMTTextIterator, self).__init__(source_dataset, target_dataset,
                                            source_dicts, target_dict,
                                            batch_size,
                                            maxlen_sent,
                                            maxlen_word,
                                            source_vocab_sizes,
                                            skip_empty,
                                            shuffle_each_epoch,
                                            sort_by_length,
                                            use_factor,
                                            maxibatch_size,
                                            token_batch_size,
                                            keep_data_in_memory)
        
        self.target_vocab_size = target_vocab_size
        self.target_unk_val = _determine_unk_val(self.target_dict)

        if self.target_vocab_size != None and self.target_vocab_size > 0:
            for key, idx in list(self.target_dict.items()):
                if idx >= self.target_vocab_size:
                    del self.target_dict[key]

    def __next__(self):
        if self.end_of_data:
            self.end_of_data = False
            self.reset()
            raise StopIteration
 
        # fill the buffer if empty
        if len(self.buffer) == 0:
            for src_text, tgt_text in zip(self.source, self.target):
                src_sec, tgt_sec = [], []
                for ss, tt in zip(src_text, tgt_text):
                    src_sent = ss.split()[:self.maxlen_word]
                    tgt_sent = tt.split()[:self.maxlen_word]

                    if self.skip_empty and (len(src_sent) == 0 or len(tgt_sent) == 0):
                        continue

                    src_sec.append(src_sent[:self.maxlen_sent])
                    tgt_sec.append(tgt_sent[:self.maxlen_sent])

                if self.skip_empty and (len(src_sec) == 0 or len(tgt_sec) == 0):
                    continue

                self.buffer.append((src_sec, tgt_sec))

                if len(self.buffer) == self.k:
                    break

            if len(self.buffer) == 0:
                self.end_of_data = False
                self.reset()
                raise StopIteration

            # sort by source/target buffer length
            if self.sort_by_length:
                raise NotImplementedError
                # tlen = numpy.array([max(len(s), len(t)) for (
                #     s, t) in zip(self.source_buffer, self.target_buffer)])
                # tidx = tlen.argsort()

                # _sbuf = [self.source_buffer[i] for i in tidx]
                # _tbuf = [self.target_buffer[i] for i in tidx]

                # self.source_buffer = _sbuf
                # self.target_buffer = _tbuf
            else:
                self.buffer.reverse()

        try:
            # actual work here
            batch_src_indices = []
            batch_tgt_indices = []
            while True:
                # read from source file and map to word index
                try:
                    src_sec, tgt_sec = self.buffer.pop()
                except IndexError:
                    break

                sec_tmp = []
                for sent in src_sec:
                    sent_tmp = []
                    for w in sent:
                        if self.use_factor:
                            w = [_lookup_token(f, self.source_dicts[i],
                                            self.source_unk_vals[i])
                                for (i, f) in enumerate(w.split('|'))]
                        else:
                            w = [_lookup_token(w, self.source_dicts[0],
                                            self.source_unk_vals[0])]
                        sent_tmp.append(w)
                    sec_tmp.append(sent_tmp[:])
                
                src_sec_indices = sec_tmp

                sec_tmp = []
                for sent in tgt_sec:
                    sec_tmp.append([_lookup_token(w, self.target_dict, self.target_unk_val) for w in sent])

                tgt_sec_indices = sec_tmp

                batch_src_indices.append(src_sec_indices[:])
                batch_tgt_indices.append(tgt_sec_indices[:])

                if self.token_batch_size:
                    raise NotImplementedError
                    # if len(source) * longest_source > self.token_batch_size or \
                    #         len(target) * longest_target > self.token_batch_size:
                    #     # remove last sentence pair (that made batch over-long)
                    #     source.pop()
                    #     target.pop()
                    #     self.source_buffer.append(ss)
                    #     self.target_buffer.append(tt)

                    #     break

                else:
                    if len(batch_src_indices) >= self.batch_size:
                        break
        except IOError:
            self.end_of_data = True

        assert len(batch_src_indices) == len(batch_tgt_indices)
        return batch_src_indices, batch_tgt_indices


class SimpleFileWrapper(object):
    """ A simple section wrapper only for translation mode
    and only loop once
    """
    def __init__(self, file_obj):
        self.pos = 0
        self.lines = _preprocessing(file_obj.readlines())
    
    def __iter__(self):
        return self

    def __next__(self):
        if self.pos >= len(self.lines):
            raise StopIteration
        l = self.lines[self.pos]
        self.pos += 1
        return l


if __name__ == "__main__":
    iter_ = NMTTextIterator(
                 source_dataset='/Users/j.zhou/Exploit-Document-Substructure-in-Neural-MT/tests/data/nmt-data.zh',
                 target_dataset='/Users/j.zhou/Exploit-Document-Substructure-in-Neural-MT/tests/data/nmt-data.en',
                 source_dicts=['/Users/j.zhou/Exploit-Document-Substructure-in-Neural-MT/tests/data/vocab.zh.json'],
                 target_dict='/Users/j.zhou/Exploit-Document-Substructure-in-Neural-MT/tests/data/vocab.en.json',
                 batch_size=128,
                 maxlen_sent=100,
                 maxlen_word=100,
                 source_vocab_sizes=None,
                 skip_empty=False,
                 shuffle_each_epoch=False,
                 sort_by_length=False,
                 use_factor=False,
                 maxibatch_size=20,
                 token_batch_size=0,
                 keep_data_in_memory=True)
    
    i = 0
    for x, y in iter_:
        if i > 1: 
            break
        print(y)
        i += 1