import numpy
import tempfile
import random
import os
from util import load_dict
from constants import DataType
from abc import ABC, abstractmethod
from exception import Error

def _determine_unk_val(d):
    if '<UNK>' in d and d['<UNK>'] == 2:
        return 2
    return 1


def _lookup_token(t, d, unk_val):
    return d[t] if t in d else unk_val


def _shuffle(files):
    fds = [open(ff, encoding="UTF-8") for ff in files]

    lines = []
    for l in fds[0]:
        line = [l.strip()] + [ff.readline().strip() for ff in fds[1:]]
        lines.append(line)

    for ff in fds:
        ff.close()

    random.shuffle(lines)

    fds = []
    for ff in files:
        path, filename = os.path.split(os.path.realpath(ff))
        fd = tempfile.TemporaryFile(prefix=filename+'.shuf',
                                    dir=path,
                                    mode='w+',
                                    encoding="UTF-8")
        fds.append(fd)

    for l in lines:
        for ii, fd in enumerate(fds):
            print(l[ii], file=fd)

    for ff in fds:
        ff.seek(0)

    return fds


def _preprocessing(lines):
    """ remove segment symbol <p>, </p> and 
    include all the sentences between into one list
    """
    ret = []
    sec = []
    for line in lines:
        if line.strip() == '<p>':
            pass
        elif line.strip() == '</p>':
            ret.append(sec)
            sec = []
        else:
            sec.append(line.strip())
    return ret


class IterWrapper(object):
    """ Wrapper for a simple Iterator (behaves like a list of object)
    """
    def __init__(self, lines):
        self.pos = 0
        self.lines = numpy.array(lines, dtype=numpy.object)  # use numpy array to enable shuffle

    def __iter__(self):
        return self

    def __next__(self):
        if self.pos >= len(self.lines):
            raise StopIteration
        l = self.lines[self.pos]
        self.pos += 1
        return l

    def reset(self):
        self.pos = 0

    def seek(self, pos):
        assert pos == 0
        self.pos = 0

    def readline(self):
        return next(self)

    def shuffle_lines(self, perm):
        self.lines = self.lines[perm]
        self.pos = 0

    def __len__(self):
        return len(self.lines)


class SentWrapper(object):
    """ A simple section wrapper only for translation mode
    and only loop once
    """
    def __init__(self, file_obj, n_sent):
        self.pos = 0
        self.lines = file_obj.readlines()
        self.batch_size = n_sent
    
    def __iter__(self):
        return self

    def __next__(self):
        if self.pos >= len(self.lines):
            raise StopIteration
        l = self.lines[self.pos: self.pos+self.batch_size]
        self.pos += self.batch_size
        return l


class PassageWrapper(object):
    """ A simple section wrapper only for translation mode
    and only loop once
    """
    def __init__(self, file_obj):
        self.pos = 0
        self.lines = _preprocessing(file_obj.readlines())
    
    def __iter__(self):
        return self

    def __next__(self):
        if self.pos >= len(self.lines):
            raise StopIteration
        l = self.lines[self.pos]
        self.pos += 1
        return l


class TextIterator(ABC):
    """ abstract class for TextIterator
    """
    def __init__(self, source_dataset, target_dataset,
                 source_dicts, target_dict,
                 source_vocab_sizes=None,
                 target_vocab_size=None,
                 batch_size=128,
                 sort_by_length=True,
                 use_factor=False,
                 seg_file=False,
                 keep_data_in_memory=True,
                 maxibatch_size=20):

        self._data_in_mem = keep_data_in_memory
        if self._data_in_mem:
            src_lines = open(source_dataset, 'r').readlines()
            tgt_lines = open(target_dataset, 'r').readlines()
            if seg_file:
                src_lines = _preprocessing(src_lines)
                tgt_lines = _preprocessing(tgt_lines)

            self.source, self.target = IterWrapper(src_lines), IterWrapper(tgt_lines)
            r = numpy.random.permutation(len(self.source))
            self.source.shuffle_lines(r)
            self.target.shuffle_lines(r)
        else:
            if seg_file:
                raise NotImplementedError('option seg_file cannot be enabled when keeping data in disk')
            self.source_origin, self.target_origin = source_dataset, target_dataset
            self.source, self.target = _shuffle([self.source_origin, self.target_origin])

        
        # handle dicts
        self.source_dicts = []
        for source_dict in source_dicts:
            self.source_dicts.append(load_dict(source_dict))
        self.target_dict = load_dict(target_dict)

        self.source_unk_vals = [_determine_unk_val(d)
                                for d in self.source_dicts]
        self.target_unk_val = _determine_unk_val(self.target_dict)


        self.source_vocab_sizes = source_vocab_sizes
        self.target_vocab_size = target_vocab_size

        if self.source_vocab_sizes != None:
            assert len(self.source_vocab_sizes) == len(self.source_dicts)
            for d, vocab_size in zip(self.source_dicts, self.source_vocab_sizes):
                if vocab_size != None and vocab_size > 0:
                    for key, idx in list(d.items()):
                        if idx >= vocab_size:
                            del d[key]

        if self.target_vocab_size != None and self.target_vocab_size > 0:
            for key, idx in list(self.target_dict.items()):
                if idx >= self.target_vocab_size:
                    del self.target_dict[key]

        self.batch_size = batch_size
        self.use_factor = use_factor
        self.sort_by_length = sort_by_length
        self.buffer = []
        # k is the buffer size, which controls how many senteces would be sorted
        self.k = batch_size * maxibatch_size
        self.end_of_data = False
    
    def __iter__(self):
        return self
    
    def reset(self):
        if self._data_in_mem:
            r = numpy.random.permutation(len(self.source))
            self.source.shuffle_lines(r)
            self.target.shuffle_lines(r)
        else:
            self.source, self.target = _shuffle([self.source_origin, self.target_origin])
    
    @abstractmethod
    def __next__(self):
        raise NotImplementedError


class CLFTextIterator(TextIterator):
    """ Iterator for Section
    Only used in Training Text Classifier
    """
    def __init__(self, 
                 source_dataset, target_dataset,
                 source_dicts, target_dict=None,
                 source_vocab_sizes=None,
                 batch_size=128,
                 maxlen_sent=100,
                 maxlen_word=100,
                 sort_by_length=True,
                 use_factor=False,
                 keep_data_in_memory=True,
                 maxibatch_size=20):
        super(CLFTextIterator, self).__init__(source_dataset, target_dataset,
                                              source_dicts, target_dict,
                                              source_vocab_sizes, None,  # target vocab set to None
                                              batch_size,
                                              sort_by_length, 
                                              use_factor, 
                                              True, # need to segment file by <p></p> tag
                                              keep_data_in_memory,
                                              maxibatch_size)
        self.maxlen_sent = maxlen_sent
        self.maxlen_word = maxlen_word


    def __next__(self):
        """ Return 
        batch_text_indices: [[[[int...]...]...]...], 
            each int is a factor, 
            the innermost list is a word, 
            the second innermost list is a sent, 
            the third innermost list is a passage
        batch_label_indices: [int...], each int is a class in CLF
        """
        if self.end_of_data:
            self.end_of_data = False
            self.reset()
            raise StopIteration
 
        # fill the buffer if empty
        if len(self.buffer) == 0:
            for text, label in zip(self.source, self.target):
                sec_text = []
                for sent in text:
                    # truncate sentence
                    sent = sent.split()[:self.maxlen_word]

                    if len(sent) == 0:
                        continue
                    sec_text.append(sent)

                if len(sec_text) == 0:
                    continue

                label = label[0]
                self.buffer.append((sec_text[:self.maxlen_sent], label))

                if len(self.buffer) == self.k:
                    break

            if len(self.buffer) == 0:
                self.end_of_data = False
                self.reset()
                raise StopIteration

            if self.sort_by_length:
                # sort by sentence length
                tlen = numpy.array([len(s) for (s,t) in self.buffer])
                tidx = tlen.argsort()
                _buf = [self.buffer[i] for i in tidx]

                self.buffer = _buf
            else:
                self.buffer.reverse()

        try:
            # actual work here
            batch_text_indices = []
            batch_label_indices = []
            while True:
                # read from source file and map to word index
                try:
                    sec_text, label = self.buffer.pop()
                except IndexError:
                    break
                
                sec_tmp = []
                for sent in sec_text:
                    sent_tmp = []
                    for w in sent:
                        if self.use_factor:
                            w = [_lookup_token(f, self.source_dicts[i],
                                            self.source_unk_vals[i])
                                for (i, f) in enumerate(w.split('|'))]
                        else:
                            w = [_lookup_token(w, self.source_dicts[0],
                                            self.source_unk_vals[0])]
                        sent_tmp.append(w)
                    sec_tmp.append(sent_tmp[:])
                
                sec_indices = sec_tmp
                label_index = self.target_dict[label]

                batch_text_indices.append(sec_indices[:])
                batch_label_indices.append(label_index)

                if len(batch_text_indices) >= self.batch_size:
                    break
        except IOError:
            self.end_of_data = True

        assert len(batch_text_indices) == len(batch_label_indices)
        return (batch_text_indices, batch_label_indices), DataType.DATA_FOR_CLF


class VanillaNMTTextIterator(TextIterator):
    """ Iterator for Data in Vanilla sentence-level NMT
    """
    def __init__(self, 
                 source_dataset, target_dataset,
                 source_dicts, target_dict,
                 source_vocab_sizes=None, target_vocab_size=None,
                 batch_size=128,
                 maxlen_word=100,
                 sort_by_length=True,
                 use_factor=False,
                 keep_data_in_memory=True,
                 maxibatch_size=20):
        
        super(VanillaNMTTextIterator, self).__init__(source_dataset, target_dataset,
                                                     source_dicts, target_dict,
                                                     source_vocab_sizes, target_vocab_size,
                                                     batch_size,
                                                     sort_by_length,
                                                     use_factor,
                                                     False,
                                                     keep_data_in_memory,
                                                     maxibatch_size)
        self.maxlen_word = maxlen_word

    def __next__(self):
        """ Return 
        batch_src_indices: [[[int...]...]...], each int is a factor, the innermost list is a word, the second innermost list is a sent, 
        batch_tgt_indices: [[int...]...], each int is a word, the innermost list is a sent
        """
        if self.end_of_data:
            self.end_of_data = False
            self.reset()
            raise StopIteration
 
        # fill the buffer if empty
        if len(self.buffer) == 0:
            for ss, tt in zip(self.source, self.target):    # ss:str, tt: str
                ss = ss.split()
                tt = tt.split()

                # skip empty sentences or too long sentences
                if (len(ss) == 0 or len(tt) == 0) or (self.maxlen_word is not None and (len(ss) > self.maxlen_word or len(tt) > self.maxlen_word)):
                    continue

                self.buffer.append((ss, tt))

                if len(self.buffer) == self.k:
                    break

            if len(self.buffer) == 0:
                self.end_of_data = False
                self.reset()
                raise StopIteration

            # sort by source/target buffer length
            if self.sort_by_length:
                tlen = numpy.array([max(len(s),len(t)) for (s,t) in self.buffer])
                tidx = tlen.argsort()
                _buf = [self.buffer[i] for i in tidx]

                self.buffer = _buf
            else:
                self.buffer.reverse()

        try:
            # actual work here
            batch_src_indices = []
            batch_tgt_indices = []
            while True:
                # read from source file and map to word index
                try:
                    src_sent, tgt_sent = self.buffer.pop()
                except IndexError:
                    break

                src_indices = []
                for w in src_sent:
                    if self.use_factor:
                        w = [_lookup_token(f, self.source_dicts[i],
                                        self.source_unk_vals[i])
                            for (i, f) in enumerate(w.split('|'))]
                    else:
                        w = [_lookup_token(w, self.source_dicts[0],
                                        self.source_unk_vals[0])]
                    src_indices.append(w)

                tgt_indices = [_lookup_token(w, self.target_dict, self.target_unk_val) for w in tgt_sent]

                batch_src_indices.append(src_indices)
                batch_tgt_indices.append(tgt_indices)

                if len(batch_src_indices) >= self.batch_size:
                    break
        except IOError:
            self.end_of_data = True

        assert len(batch_src_indices) == len(batch_tgt_indices)
        return (batch_src_indices, batch_tgt_indices), DataType.DATA_FOR_VANILLA_NMT


class StructureAwareNMTIterator(TextIterator):
    def __init__(self, 
                 source_dataset, target_dataset,
                 source_dicts, target_dict,
                 source_vocab_sizes=None,
                 target_vocab_size=None,
                 batch_size=128,
                 maxlen_sent=100,
                 maxlen_word=100,
                 sort_by_length=True,
                 use_factor=False,
                 keep_data_in_memory=True,
                 maxibatch_size=20):
        super(StructureAwareNMTIterator, self).__init__(source_dataset, target_dataset,
                                                        source_dicts, target_dict,
                                                        source_vocab_sizes, target_vocab_size,
                                                        batch_size,
                                                        sort_by_length, 
                                                        use_factor,
                                                        True, # need to segment file by <p></p> tag
                                                        keep_data_in_memory,
                                                        maxibatch_size)
        self.maxlen_sent = maxlen_sent
        self.maxlen_word = maxlen_word


    def __next__(self):
        """ Return 
        batch_src_indices: [[[[int...]...]...]...], 
            each int is a factor, 
            the innermost list is a word, 
            the second innermost list is a sent, 
            the third innermost list is a passage
        batch_tgt_indices: [[[int...]...]...], 
            each int is a word, 
            the innermost list is a sent, 
            the second innermost list is a passage
        """
        if self.end_of_data:
            self.end_of_data = False
            self.reset()
            raise StopIteration
 
        # fill the buffer if empty
        if len(self.buffer) == 0:
            for src_text, tgt_text in zip(self.source, self.target):
                ss, tt = [], []
                for src_sent, tgt_sent in zip(src_text, tgt_text):
                    src_sent = src_sent.split()
                    tgt_sent = tgt_sent.split()

                    if len(src_sent) == 0 or len(tgt_sent) == 0:
                        continue
                    ss.append(src_sent)
                    tt.append(tgt_sent)

                if (len(ss) == 0 or len(tt) == 0) or \
                   (self.maxlen_sent is not None and (len(ss) > self.maxlen_sent or len(tt) > self.maxlen_sent)) or \
                   (self.maxlen_word is not None and (max(list(map(len, ss))) > self.maxlen_word or max(list(map(len, tt))) > self.maxlen_word)):
                   continue
                
                self.buffer.append((ss, tt))

                if len(self.buffer) == self.k:
                    break

            if len(self.buffer) == 0:
                self.end_of_data = False
                self.reset()
                raise StopIteration

            if self.sort_by_length:
                # sort by number of sentence of each passage
                tlen = numpy.array([len(s) for (s,t) in self.buffer])
                tidx = tlen.argsort()
                _buf = [self.buffer[i] for i in tidx]

                self.buffer = _buf
            else:
                self.buffer.reverse()

        try:
            # actual work here
            batch_src_indices = []
            batch_tgt_indices = []
            while True:
                # read from source file and map to word index
                try:
                    ss, tt = self.buffer.pop()
                except IndexError:
                    break
                
                ss_tmp, tt_tmp = [], []
                for src_sent, tgt_sent in zip(ss, tt):
                    ss_tmp_sent = []
                    for w in src_sent:
                        if self.use_factor:
                            w = [_lookup_token(f, self.source_dicts[i],
                                            self.source_unk_vals[i])
                                for (i, f) in enumerate(w.split('|'))]
                        else:
                            w = [_lookup_token(w, self.source_dicts[0],
                                            self.source_unk_vals[0])]
                        ss_tmp_sent.append(w)
                    
                    tt_tmp_sent = [_lookup_token(w, self.target_dict, self.target_unk_val) for w in tgt_sent]

                    ss_tmp.append(ss_tmp_sent[:])
                    tt_tmp.append(tt_tmp_sent[:])
                
                batch_src_indices.append(ss_tmp[:])
                batch_tgt_indices.append(tt_tmp[:])

                if len(batch_src_indices) >= self.batch_size:
                    break
        except IOError:
            self.end_of_data = True

        assert len(batch_src_indices) == len(batch_tgt_indices)
        return (batch_src_indices, batch_tgt_indices), DataType.DATA_FOR_NMT


class JointLearningWrapper(object):
    def __init__(self, nmt_iter, clf_iter, k):
        """
        k - get data from clf_iter every k steps
        """
        assert isinstance(k, int)
        assert k > 0

        self.cnt = 0
        self.k = k
        self.clf_iter = clf_iter
        self.nmt_iter = nmt_iter

    
    def __next__(self):
        self.cnt += 1
        if self.cnt % self.k:
            try:
                return next(self.nmt_iter)
            except StopIteration:
                # running out of nmt's data is the end of one epoch
                raise StopIteration
        else:
            try:
                return next(self.clf_iter)
            except StopIteration:
                # CLF ITER would reset point automatically
                # just call next again
                return next(self.clf_iter)
    
    def __iter__(self):
        return self


def iter_builder(config, is_valid_data=False):
    if is_valid_data:
        source_path, target_path = config.valid_datasets
        keep_data_in_memory = False
        maxlen_sent = None
        maxlen_word = None
    else:
        source_path, target_path = config.datasets
        keep_data_in_memory = config.keep_train_set_in_memory
        maxlen_sent = config.maxlen_sent
        maxlen_word = config.maxlen_word

    if config.mode == 'baseline-nmt':
        return VanillaNMTTextIterator(
                 source_dataset=source_path, target_dataset=target_path,
                 source_dicts=config.source_dicts, target_dict=config.target_dict,
                 source_vocab_sizes=config.source_vocab_sizes, target_vocab_size=config.target_vocab_size,
                 batch_size=config.batch_size,
                 maxlen_word=maxlen_word,
                 sort_by_length=True,
                 use_factor=False,
                 keep_data_in_memory=keep_data_in_memory,
                 maxibatch_size=config.maxibatch_size)

    elif config.mode == 'nmt':        
        return StructureAwareNMTIterator(
                    source_dataset=source_path, target_dataset=target_path,
                    source_dicts=config.source_dicts, target_dict=config.target_dict,
                    source_vocab_sizes=config.source_vocab_sizes, target_vocab_size=config.target_vocab_size,
                    batch_size=config.batch_size,
                    maxlen_sent=maxlen_sent,
                    maxlen_word=maxlen_word,
                    sort_by_length=True,
                    use_factor=False,
                    keep_data_in_memory=keep_data_in_memory,
                    maxibatch_size=config.maxibatch_size)

    elif config.mode == 'clf':        
        return CLFTextIterator(
                    source_dataset=source_path, target_dataset=target_path,
                    source_dicts=config.source_dicts, target_dict=config.target_dict,
                    source_vocab_sizes=config.source_vocab_sizes,
                    batch_size=config.batch_size,
                    maxlen_sent=maxlen_sent,
                    maxlen_word=maxlen_word,
                    sort_by_length=True,
                    use_factor=False,
                    keep_data_in_memory=keep_data_in_memory,
                    maxibatch_size=config.maxibatch_size)

    else:
        if is_valid_data:
            clf_source_path, clf_target_path = config.valid_aux_datasets
            nmt_source_path, nmt_target_path = config.valid_datasets
            max_len_word = None
            max_len_sent = None
        else:
            clf_source_path, clf_target_path = config.aux_datasets
            nmt_source_path, nmt_target_path = config.datasets
            maxlen_sent = config.maxlen_sent
            maxlen_word = config.maxlen_word

        clf_iter =  CLFTextIterator(
                        source_dataset=clf_source_path, target_dataset=clf_target_path,
                        source_dicts=config.source_dicts, target_dict=config.aux_dictionary,
                        source_vocab_sizes=config.source_vocab_sizes,
                        batch_size=config.batch_size,
                        maxlen_sent=max_len_sent,
                        maxlen_word=max_len_word,
                        sort_by_length=True,
                        use_factor=False,
                        keep_data_in_memory=keep_data_in_memory,
                        maxibatch_size=config.maxibatch_size)

        nmt_iter =  StructureAwareNMTIterator(
                        source_dataset=nmt_source_path, target_dataset=nmt_target_path,
                        source_dicts=config.source_dicts, target_dict=config.target_dict,
                        source_vocab_sizes=config.source_vocab_sizes, target_vocab_size=config.target_vocab_size,
                        batch_size=config.batch_size,
                        maxlen_sent=max_len_sent,
                        maxlen_word=max_len_word,
                        sort_by_length=True,
                        use_factor=False,
                        keep_data_in_memory=keep_data_in_memory,
                        maxibatch_size=config.maxibatch_size)

        return JointLearningWrapper(nmt_iter, clf_iter, config.multi_task_update_ratio)
