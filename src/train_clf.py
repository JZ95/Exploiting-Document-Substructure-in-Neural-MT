#!/usr/bin/env python3
'''
Build a neural machine translation model with soft attention
'''
import collections
from datetime import datetime
import json
import os
import locale
import logging
import subprocess
import sys
import tempfile
import time

# Start logging.
level = logging.INFO
logging.basicConfig(level=level, format='%(levelname)s: %(message)s')

import numpy
import tensorflow as tf
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
from nematus.config import write_config_to_json_file
from nematus.learning_schedule import ConstantSchedule
from nematus import model_loader, util
from my_config import my_read_config_from_cmdline
from my_data_loader import SectionIterator
from my_rnn_model import RNNModelClf
from my_model_updater import MyModelUpdater
import my_utils


def load_data(config):
    logging.info('Reading data...')
    text_iterator = SectionIterator(
                        xml_data=config.source_dataset,
                        source_dicts=config.source_dicts,
                        model_type=config.model_type,
                        batch_size=config.batch_size,
                        maxlen_sent=config.maxlen_sent,
                        maxlen_word=config.maxlen_word,
                        source_vocab_sizes=config.source_vocab_sizes,
                        skip_empty=True,
                        shuffle_each_epoch=config.shuffle_each_epoch,
                        sort_by_length=False,
                        use_factor=(config.factors > 1),
                        maxibatch_size=config.maxibatch_size,
                        token_batch_size=config.token_batch_size,
                        keep_data_in_memory=True)

    if config.valid_freq and config.valid_source_dataset:
        valid_text_iterator = SectionIterator(
                                xml_data=config.valid_source_dataset,
                                source_dicts=config.source_dicts,
                                model_type=config.model_type,
                                batch_size=config.batch_size,
                                maxlen_sent=config.maxlen_sent,
                                maxlen_word=config.maxlen_word,
                                source_vocab_sizes=config.source_vocab_sizes,
                                skip_empty=True,
                                shuffle_each_epoch=config.shuffle_each_epoch,
                                sort_by_length=False,
                                use_factor=(config.factors > 1),
                                maxibatch_size=config.maxibatch_size,
                                token_batch_size=config.token_batch_size,
                                keep_data_in_memory=True)
    else:
        logging.info('no validation set loaded')
        valid_text_iterator = None
    logging.info('Done')
    return text_iterator, valid_text_iterator


def train(config, sess):
    assert (config.prior_model != None and (tf.train.checkpoint_exists(os.path.abspath(config.prior_model))) or (config.map_decay_c==0.0)), \
    "MAP training requires a prior model file: Use command-line option --prior_model"

    # Construct the graph, with one model replica per GPU
    num_gpus = len(util.get_available_gpus())
    num_replicas = max(1, num_gpus)

    logging.info('Building model...')
    replicas = []
    for i in range(num_replicas):
        device_type = "GPU" if num_gpus > 0 else "CPU"
        device_spec = tf.DeviceSpec(device_type=device_type, device_index=i)
        with tf.device(device_spec):
            with tf.variable_scope(tf.get_variable_scope(), reuse=(i>0)):
                if config.model_type == "transformer":
                    raise NotImplementedError('do not support Transformer architecutre yet')
                else:
                    model = RNNModelClf(config)
                replicas.append(model)

    init = tf.zeros_initializer(dtype=tf.int32)
    global_step = tf.get_variable('time', [], initializer=init, trainable=False)

    if config.learning_schedule == "constant":
        schedule = ConstantSchedule(config.learning_rate)
    elif config.learning_schedule == "transformer":
        raise NotImplementedError('do not support Transformer Scheduler yet')
    else:
        logging.error('Learning schedule type is not valid: {}'.format(
            config.learning_schedule))
        sys.exit(1)

    if config.optimizer == 'adam':
        optimizer = tf.train.AdamOptimizer(learning_rate=schedule.learning_rate,
                                           beta1=config.adam_beta1,
                                           beta2=config.adam_beta2,
                                           epsilon=config.adam_epsilon)
    else:
        logging.error('No valid optimizer defined: {}'.format(config.optimizer))
        sys.exit(1)

    if config.summary_freq:
        summary_dir = (config.summary_dir if config.summary_dir is not None
                       else os.path.abspath(os.path.dirname(config.saveto)))
        writer = tf.summary.FileWriter(summary_dir, sess.graph)
    else:
        writer = None

    updater = MyModelUpdater(config, num_gpus, replicas, optimizer, global_step,
                           writer)

    saver, progress = model_loader.init_or_restore_variables(
        config, sess, train=True)

    global_step.load(progress.uidx, sess)

    #save model options
    write_config_to_json_file(config, config.saveto)

    text_iterator, valid_text_iterator = load_data(config)
    # x = numpy.random.randint(low=0, high=128, size=(10, 100, 100, 1)).tolist()
    # y = numpy.random.randint(low=0, high=10, size=(10,)).tolist()
    # test_iterator = zip([x] * 5, [y] * 5)
    total_loss = 0.
    n_docs, n_sents, n_words = 0, 0, 0
    last_time = time.time()
    logging.info("Initial uidx={}".format(progress.uidx))
    for progress.eidx in range(progress.eidx, config.max_epochs):
        logging.info('Starting epoch {0}'.format(progress.eidx))
        for sections, labels in text_iterator:
        # for sections, labels in test_iterator:
            if len(sections[0][0][0]) != config.factors:
                logging.error('Mismatch between number of factors in settings ({0}), and number in training corpus ({1})\n'.format(config.factors, len(sections[0][0][0])))
                sys.exit(1)
            x_in, x_mask_in, y_in, y_mask_in = my_utils.prepare_cls_data(sections, labels, config.factors, maxlen=None)
            if x_in is None:
                logging.info('Minibatch with zero sample under length {0}'.format(config.maxlen))
                continue
            write_summary_for_this_batch = config.summary_freq and ((progress.uidx % config.summary_freq == 0) or (config.finish_after and progress.uidx % config.finish_after == 0))
            (factors, sentLen, wordLen, batch_size) = x_in.shape

            loss = updater.update(sess, x_in, x_mask_in, y_in, y_mask_in,
                                  write_summary_for_this_batch)
            total_loss += loss
            n_sents += sentLen
            n_words += sentLen * wordLen
            n_docs += batch_size
            progress.uidx += 1

            if config.disp_freq and progress.uidx % config.disp_freq == 0:
                duration = time.time() - last_time
                disp_time = datetime.now().strftime('[%Y-%m-%d %H:%M:%S]')
                logging.info('{0} Epoch: {1} Update: {2} Loss/Doc: {3:.5f} Words/sec: {4:.5f} Sents/sec: {5:.5f}'.format(disp_time, progress.eidx, progress.uidx, total_loss/n_docs, n_words/duration, n_sents/duration))
                last_time = time.time()
                total_loss = 0.
                n_docs, n_sents, n_words = 0, 0, 0

            if config.valid_freq and progress.uidx % config.valid_freq == 0:
                valid_ce, valid_f1_score = validate(sess, replicas[0], config, valid_text_iterator)

                if (len(progress.history_errs) == 0 or
                    valid_ce < min(progress.history_errs)):
                    progress.history_errs.append(valid_ce)
                    progress.bad_counter = 0
                    save_non_checkpoint(sess, saver, config.saveto)
                    progress_path = '{0}.progress.json'.format(config.saveto)
                    progress.save_to_json(progress_path)
                    # decide wether save as the best-model or not
                    need_to_save = ((len(progress.valid_script_scores) == 0 or 
                            valid_f1_score > max(progress.valid_script_scores)))
                    progress.valid_script_scores.append(valid_f1_score)
                    if need_to_save:
                        progress.bad_counter = 0
                        save_path = config.saveto + ".best-f1-valid"
                        save_non_checkpoint(sess, saver, save_path)
                        write_config_to_json_file(config, save_path)

                        progress_path = '{}.progress.json'.format(save_path)
                        progress.save_to_json(progress_path)
                else:
                    progress.history_errs.append(valid_ce)
                    progress.bad_counter += 1
                    if progress.bad_counter > config.patience:
                        logging.info('Early Stop!')
                        progress.estop = True
                        break

            if config.save_freq and progress.uidx % config.save_freq == 0:
                saver.save(sess, save_path=config.saveto, global_step=progress.uidx)
                write_config_to_json_file(config, "%s-%s" % (config.saveto, progress.uidx))

                progress_path = '{0}-{1}.progress.json'.format(config.saveto, progress.uidx)
                progress.save_to_json(progress_path)

            if config.finish_after and progress.uidx % config.finish_after == 0:
                logging.info("Maximum number of updates reached")
                saver.save(sess, save_path=config.saveto, global_step=progress.uidx)
                write_config_to_json_file(config, "%s-%s" % (config.saveto, progress.uidx))

                progress.estop=True
                progress_path = '{0}-{1}.progress.json'.format(config.saveto, progress.uidx)
                progress.save_to_json(progress_path)
                break
        if progress.estop:
            break


def save_non_checkpoint(session, saver, save_path):
    """Saves the model to a temporary directory then moves it to save_path.

    Rationale: we use TensorFlow's standard tf.train.Saver mechanism for saving
    training checkpoints and also for saving the current best model according
    to validation metrics. Since these are all stored in the same directory,
    their paths would normally all get written to the same 'checkpoint' file,
    with the file containing whichever one was last saved. That creates a
    problem if training is interrupted after a best-so-far model is saved but
    before a regular checkpoint is saved, since Nematus will try to load the
    best-so-far model instead of the last checkpoint when it is restarted. To
    avoid this, we save the best-so-far models to a temporary directory, then
    move them to their desired location. The 'checkpoint' file that is written
    to the temporary directory can safely be deleted along with the directory.

    Args:
        session: a TensorFlow session.
        saver: a tf.train.Saver
        save_path: string containing the path to save the model to.

    Returns:
        None.
    """
    head, tail = os.path.split(save_path)
    assert tail != ""
    base_dir = "." if head == "" else head
    with tempfile.TemporaryDirectory(dir=base_dir) as tmp_dir:
        tmp_save_path = os.path.join(tmp_dir, tail)
        saver.save(session, save_path=tmp_save_path)
        for filename in os.listdir(tmp_dir):
            if filename == 'checkpoint':
                continue
            new = os.path.join(tmp_dir, filename)
            old = os.path.join(base_dir, filename)
            os.replace(src=new, dst=old)


def validate(session, model, config, text_iterator):
    ce_vals, y_pred, y_true = [], [], []
    for xx, yy in text_iterator:
        if len(xx[0][0][0]) != config.factors:
            logging.error('Mismatch between number of factors in settings ' \
                          '({0}) and number present in data ({1})'.format(
                          config.factors, len(xx[0][0][0])))
            sys.exit(1)
        x, x_mask, y, y_mask = my_utils.prepare_cls_data(xx, yy, config.factors, maxlen=None)

        # Run the minibatch through the model to get the sentence-level cross
        # entropy values.
        feeds = {model.inputs.x: x,
                 model.inputs.x_mask: x_mask,
                 model.inputs.y: y,
                 model.inputs.y_mask: y_mask,
                 model.inputs.training: False}
        batch_ce_vals, batch_preds = session.run([model.loss_per_sec, model.preds], feed_dict=feeds)
        y_true.extend(y.ravel().tolist())
        y_pred.extend(batch_preds.tolist())
        ce_vals.extend(batch_ce_vals.tolist())
        logging.info("Seen {}".format(len(ce_vals)))
    
    mirco_precision = precision_score(y_true, y_pred, average='micro')
    mirco_recall = recall_score(y_true, y_pred, average='micro')
    mirco_f1_score = f1_score(y_true, y_pred, average='micro')
    acc_score = accuracy_score(y_true, y_pred)

    num_secs = len(ce_vals)
    sum_ce = sum(ce_vals)
    avg_ce = sum_ce / num_secs

    logging.info('Validation external score (Accuracy/Precision/Recall/F1-Score) : {0:.5f} ' \
                 '{1:.5f} {2:.5f} {3:.5f}'.format(acc_score, mirco_precision, mirco_recall, mirco_f1_score))
    logging.info('Validation cross entropy (AVG/SUM): {0:.5f} {1:.5f}'.format(avg_ce, sum_ce))

    return avg_ce, mirco_f1_score


def main():
    # Parse command-line arguments.
    config = my_read_config_from_cmdline()
    logging.info(config)

    # Create the TensorFlow session.
    tf_config = tf.ConfigProto()
    tf_config.allow_soft_placement = True

    # Train.
    with tf.Session(config=tf_config) as sess:
        train(config, sess)

if __name__ == "__main__":
    main()