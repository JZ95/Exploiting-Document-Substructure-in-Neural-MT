""" My Utility function
"""
import numpy


def prepare_clf_data(seqs_x, seq_y, n_factors, maxlen=None):
    """
    seqsx: a nested list, where
            len(seqs_x) = batch_size, 
            len(seqs_x[0]) = num_sents in the first section
    """
    lengths_sent_x = [len(sec) for sec in seqs_x]
    lengths_word_x = [[len(sent) for sent in sec] for sec in seqs_x]

    if maxlen is not None:
        raise NotImplementedError

    n_samples = len(seqs_x)
    maxlen_sent_x = numpy.max(lengths_sent_x)      # sentence has not <EOS> tok
    maxlen_word_x = numpy.max(list(map(max, lengths_word_x))) + 1  # plus one for the <EOS> tok

    # each word the source sent is a list of its factors
    x = numpy.zeros((n_factors, maxlen_sent_x, maxlen_word_x, n_samples)).astype('int64')
    y = numpy.zeros((1, n_samples)).astype('int64')
    x_mask = numpy.zeros((maxlen_sent_x, maxlen_word_x, n_samples)).astype('float32')
    y_mask = numpy.ones((1, n_samples)).astype('int64')
    for i, [s_x, s_y] in enumerate(zip(seqs_x, seq_y)):
        for j, word_len in enumerate(lengths_word_x[i]):
            x[:, j, :word_len, i] = list(zip(*s_x[j]))
            x_mask[j, :word_len+1, i] = 1.
    y[0, :] = seq_y

    # x shape: [factors, seqNum, seqLen, batch_size]
    # y shape: [batch_size], y has no padiing actually, but mask is still provided for the compatiablity with nematus
    # with x_mask you can compute the number of sents in each section or number of toks in each sent
    return x, x_mask, y, y_mask


def prepare_nmt_data(seqs_x, seqs_y, n_factors, maxlen=None):
    """
    seqs_x: a nested list, where
            len(seqs_x) = batch_size, 
            len(seqs_x[0]) = num_sents in the first section
    seqs_x and seqs_y have the same data struct
    """
    lengths_sent_x = [len(sec) for sec in seqs_x]
    lengths_word_x = [[len(sent) for sent in sec] for sec in seqs_x]
    lengths_word_y = [[len(sent) for sent in sec] for sec in seqs_y]

    if maxlen is not None:
        raise NotImplementedError

    n_samples = len(seqs_x)
    maxlen_sent = numpy.max(lengths_sent_x)      # sentence has not <EOS> tok

    maxlen_word_x = numpy.max(list(map(max, lengths_word_x))) + 1  # plus one for the <EOS> tok
    maxlen_word_y = numpy.max(list(map(max, lengths_word_y))) + 1  # plus one for the <EOS> tok

    # each word the source sent is a list of its factors
    x = numpy.zeros((n_factors, maxlen_sent, maxlen_word_x, n_samples)).astype('int64')
    y = numpy.zeros((maxlen_sent, maxlen_word_y, n_samples)).astype('int64')
    x_mask = numpy.zeros((maxlen_sent, maxlen_word_x, n_samples)).astype('float32')
    y_mask = numpy.zeros((maxlen_sent, maxlen_word_y, n_samples)).astype('float32')

    for i, [s_x, s_y] in enumerate(zip(seqs_x, seqs_y)):
        for j, word_len in enumerate(lengths_word_x[i]):
            x[:, j, :word_len, i] = list(zip(*s_x[j]))
            x_mask[j, :word_len+1, i] = 1.
        for j, word_len in enumerate(lengths_word_y[i]):
            y[j, :word_len, i] = s_y[j]
            y_mask[j, :word_len+1, i] = 1.
    
    return x, x_mask, y, y_mask