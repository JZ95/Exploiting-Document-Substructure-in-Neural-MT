import tensorflow as tf
from nematus.rnn_model import Encoder, RNNModel
from nematus import layers, model_inputs


class HighLevelEncoder:
    """ High level Encoder based on the Nematus Encoder, which models the information
    on Sentence, Paragraph level.
    The only difference is that this class doesn't have to build an embedding layer.
    """
    def __init__(self, config, batch_size, dropout_input, dropout_hidden):
        if config.theano_compat:
            bias_type = layers.LegacyBiasType.THEANO_A
        else:
            bias_type = layers.LegacyBiasType.NEMATUS_COMPAT_FALSE

        with tf.variable_scope("forward-stack"):
            self.forward_encoder = layers.GRUStack(
                input_size=2*config.state_size,
                state_size=config.state_size,
                batch_size=batch_size,
                use_layer_norm=config.rnn_layer_normalization,
                legacy_bias_type=bias_type,
                dropout_input=dropout_input,
                dropout_state=dropout_hidden,
                stack_depth=config.rnn_enc_depth,
                transition_depth=config.rnn_enc_transition_depth,
                alternating=True,
                residual_connections=True,
                first_residual_output=1)

        with tf.variable_scope("backward-stack"):
            self.backward_encoder = layers.GRUStack(
                input_size=2*config.state_size,
                state_size=config.state_size,
                batch_size=batch_size,
                use_layer_norm=config.rnn_layer_normalization,
                legacy_bias_type=bias_type,
                dropout_input=dropout_input,
                dropout_state=dropout_hidden,
                stack_depth=config.rnn_enc_depth,   # TODO change a CLF specific enc depth
                transition_depth=config.rnn_enc_transition_depth,
                alternating=True,
                reverse_alternation=True,
                residual_connections=True,
                first_residual_output=1)

    def get_context(self, x, x_mask):
        """ 
        args:
            x: shape - [seqLen, batch_size, state_size], type - numpy.array
            x_mask: shape - [seqLen, batch_size], type - numpy.array
        """
        with tf.variable_scope("forward-stack"):
            fwd_states = self.forward_encoder.forward(x, x_mask)

        with tf.variable_scope("backward-stack"):
            bwd_states = self.backward_encoder.forward(x, x_mask)

        # Concatenate the left-to-right and the right-to-left states, in that
        # order. This is for compatibility with models that were trained with
        # the Theano version.
        stack_depth = len(self.forward_encoder.grus)
        if stack_depth % 2 == 0:
            concat_states = tf.concat([bwd_states, fwd_states], axis=2)
        else:
            concat_states = tf.concat([fwd_states, bwd_states], axis=2)
        return concat_states


class ClfModelInputs(object):
    def __init__(self, config):
        # variable dimensions
        sent_len, word_len, batch_size = None, None, None

        self.x = tf.placeholder(
            name='x',
            shape=(config.factors, sent_len, word_len, batch_size),
            dtype=tf.int32)

        self.x_mask = tf.placeholder(
            name='x_mask',
            shape=(sent_len, word_len, batch_size),
            dtype=tf.float32)

        self.y = tf.placeholder(
            name='y',
            shape=(batch_size),
            dtype=tf.int32)

        self.y_mask = tf.placeholder(
            name='y_mask',
            shape=(batch_size),
            dtype=tf.int32)

        self.training = tf.placeholder_with_default(
            False,
            name='training',
            shape=())


class RNNModelClf(RNNModel):
    def __init__(self, config):
        self.inputs = ClfModelInputs(config)
        # dynamic value
        # batch_size is the alias of the number of sections
        _, sent_len, word_len, batch_size = tf.unstack(tf.shape(self.inputs.x))
        # break section boundary
        word_input = tf.reshape(self.inputs.x, (config.factors, word_len, sent_len * batch_size))
        word_input_mask = tf.reshape(self.inputs.x_mask, (word_len, sent_len * batch_size))
        # Dropout functions for words.
        # These probabilistically zero-out all embedding values for individual
        # words.
        dropout_source = None
        if config.rnn_use_dropout and config.rnn_dropout_source > 0.0:
            def dropout_source(x):
                return tf.layers.dropout(
                    x, noise_shape=(tf.shape(x)[0], tf.shape(x)[1], 1),
                    rate=config.rnn_dropout_source,
                    training=self.inputs.training)

        # Dropout functions for use within FF, GRU, and attention layers.
        # We use Gal and Ghahramani (2016)-style dropout, so these functions
        # will be used to create 2D dropout masks that are reused at every
        # timestep.
        dropout_embedding, dropout_hidden = None, None
        if config.rnn_use_dropout and config.rnn_dropout_embedding > 0.0:
            def dropout_embedding(e):
                return tf.layers.dropout(e, noise_shape=tf.shape(e),
                                         rate=config.rnn_dropout_embedding,
                                         training=self.inputs.training)
        if config.rnn_use_dropout and config.rnn_dropout_hidden > 0.0:
            def dropout_hidden(h):
                return tf.layers.dropout(h, noise_shape=tf.shape(h),
                                         rate=config.rnn_dropout_hidden,
                                         training=self.inputs.training)

        # word-level encoder which is shared with NMT
        with tf.variable_scope("encoder"):
            # the input of Encoder put batch_size as its last dimension
            self.word_encoder = Encoder(config, sent_len * batch_size, dropout_source,
                                   dropout_embedding, dropout_hidden)
            word_level_ctx, _ = self.word_encoder.get_context(word_input, word_input_mask)
            # word_level_ctx - a simple concate of bi-direction GRU states
            # [time_steps, batch_size, 2 * state_size]
            word_level_query = tf.reduce_sum(word_level_ctx, axis=0)

        with tf.variable_scope("attention"):
            # reduce the dim of word
            # word_attn [batch_size, state_size * 2]
            self.word_attn_layer = layers.AttentionStep(
                                        context=word_level_ctx,
                                        context_state_size=2*config.state_size,
                                        context_mask=word_input_mask,
                                        state_size=2*config.state_size,
                                        hidden_size=2*config.state_size,
                                        use_layer_norm=config.rnn_layer_normalization,
                                        dropout_context=dropout_hidden,
                                        dropout_state=dropout_hidden)
            
            word_attn_context, _ = self.word_attn_layer.forward(word_level_query)
        # reshape context vector, always batch channel last
        sent_input = tf.reshape(word_attn_context, (sent_len, batch_size, word_attn_context.shape[-1]))
        sent_input_mask = tf.sign(tf.reduce_sum(self.inputs.x_mask, axis=1))
        
        # sent-level encoder
        with tf.variable_scope("sent-encoder"):
            self.sent_encoder = HighLevelEncoder(config, batch_size, dropout_hidden, dropout_hidden)
            sent_level_ctx = self.sent_encoder.get_context(sent_input, sent_input_mask)
            sent_level_query = tf.reduce_sum(sent_level_ctx, axis=0)

        with tf.variable_scope("sent-attention"):
            # sent_attn [batch_size, state_size * 2]
            self.sent_attn_layer = layers.AttentionStep(
                context=sent_level_ctx, 
                context_state_size=2*config.state_size,
                context_mask=sent_input_mask,
                state_size=2*config.state_size,
                hidden_size=2*config.state_size,
                use_layer_norm=config.rnn_layer_normalization,
                dropout_context=dropout_hidden,
                dropout_state=dropout_hidden)
            
            # now batch channel first
            sent_attn_context, _ = self.sent_attn_layer.forward(sent_level_query)
        
        with tf.variable_scope("clf"):
            self.class_layer = layers.FeedForwardLayer(in_size=2*config.state_size, out_size=9, batch_size=batch_size)
            self.logits = tf.expand_dims(self.class_layer.forward(sent_attn_context), axis=0, name='logits')


        with tf.variable_scope("loss"):
            # we can view the output label as a one-token sentence
            self.loss_layer = layers.Masked_cross_entropy_loss(
                self.inputs.y, self.inputs.y_mask, config.label_smoothing,
                training=self.inputs.training)
            self._loss_per_sec = self.loss_layer.forward(self.logits)
            self._loss = tf.reduce_mean(self._loss_per_sec, keepdims=False)

class MemNet(object):
    pass