import tensorflow as tf
from nematus.rnn_model import Encoder, RNNModel
from nematus import layers, model_inputs

class MyAttentionStep(layers.AttentionStep):
    """ A custimized class of AttentionStep for the text classification
    only a minor change on the forward method
    """
    def __init__(self,
                 context,
                 context_state_size,
                 context_mask,
                 state_size,
                 hidden_size,
                 use_layer_norm=False,
                 dropout_context=None,
                 dropout_state=None):
        super(MyAttentionStep, self).__init__(context,
                                              context_state_size,
                                              context_mask,
                                              state_size,
                                              hidden_size,
                                              use_layer_norm,
                                              dropout_context,
                                              dropout_state)

    def forward(self, prev_state):
        # prev_state [batch, state_size]
        prev_state = layers.apply_dropout_mask(prev_state, self.dropout_mask_state_to_hidden)
        hidden_from_state = tf.matmul(prev_state, self.state_to_hidden)
        if self.use_layer_norm:
            hidden_from_state = \
                self.hidden_state_norm.forward(hidden_from_state)
        hidden = self.hidden_from_context + hidden_from_state
        hidden = tf.nn.tanh(hidden)
        # context has shape seqLen x batch x context_state_size
        # mask has shape seqLen x batch

        scores = layers.matmul3d(hidden, self.hidden_to_score) # seqLen x batch x 1
        scores = tf.squeeze(scores, axis=2)
        scores = scores - tf.reduce_max(scores, axis=0, keepdims=True)
        scores = tf.exp(scores)
        scores *= self.context_mask
        # CHANGE IS HERE: avoid the nan problem caused by zero-division
        scores = tf.div_no_nan(scores, tf.reduce_sum(scores, axis=0, keepdims=True))

        attention_context = self.context * tf.expand_dims(scores, axis=2)
        attention_context = tf.reduce_sum(attention_context, axis=0, keepdims=False)
        # attention_context: [batch, 2 * state_size]
        return attention_context, scores


class WordLevelEncoder(Encoder):
    def __init__(self, config, batch_size, dropout_source, dropout_embedding,
                 dropout_hidden):
        self.dropout_source = dropout_source

        with tf.variable_scope("embedding"):
            self.emb_layer = layers.EmbeddingLayer(config.source_vocab_sizes,
                                                   config.dim_per_factor)

        if config.theano_compat:
            bias_type = layers.LegacyBiasType.THEANO_A
        else:
            bias_type = layers.LegacyBiasType.NEMATUS_COMPAT_FALSE

        with tf.variable_scope("forward-stack"):
            self.forward_encoder = layers.GRUStack(
                input_size=config.embedding_size,
                state_size=config.word_rnn_state_size,
                batch_size=batch_size,
                use_layer_norm=config.rnn_layer_normalization,
                legacy_bias_type=bias_type,
                dropout_input=dropout_embedding,
                dropout_state=dropout_hidden,
                stack_depth=config.word_rnn_enc_depth,
                transition_depth=config.word_rnn_enc_transition_depth,
                alternating=True,
                residual_connections=True,
                first_residual_output=1)

        with tf.variable_scope("backward-stack"):
            self.backward_encoder = layers.GRUStack(
                input_size=config.embedding_size,
                state_size=config.word_rnn_state_size,
                batch_size=batch_size,
                use_layer_norm=config.rnn_layer_normalization,
                legacy_bias_type=bias_type,
                dropout_input=dropout_embedding,
                dropout_state=dropout_hidden,
                stack_depth=config.word_rnn_enc_depth,
                transition_depth=config.word_rnn_enc_transition_depth,
                alternating=True,
                reverse_alternation=True,
                residual_connections=True,
                first_residual_output=1)


class SentLevelEncoder:
    """ High level Encoder based on the Nematus Encoder, which models the information
    on Sentence, Paragraph level.
    The only difference is that this class doesn't have to build an embedding layer.
    """
    def __init__(self, config, batch_size, dropout_input, dropout_hidden):
        if config.theano_compat:
            bias_type = layers.LegacyBiasType.THEANO_A
        else:
            bias_type = layers.LegacyBiasType.NEMATUS_COMPAT_FALSE

        with tf.variable_scope("forward-stack"):
            self.forward_encoder = layers.GRUStack(
                input_size=2*config.word_rnn_state_size,
                state_size=config.sent_rnn_state_size,
                batch_size=batch_size,
                use_layer_norm=config.rnn_layer_normalization,
                legacy_bias_type=bias_type,
                dropout_input=dropout_input,
                dropout_state=dropout_hidden,
                stack_depth=config.sent_rnn_enc_depth,
                transition_depth=config.sent_rnn_enc_transition_depth,
                alternating=True,
                residual_connections=True,
                first_residual_output=1)

        with tf.variable_scope("backward-stack"):
            self.backward_encoder = layers.GRUStack(
                input_size=2*config.word_rnn_state_size,
                state_size=config.sent_rnn_state_size,
                batch_size=batch_size,
                use_layer_norm=config.rnn_layer_normalization,
                legacy_bias_type=bias_type,
                dropout_input=dropout_input,
                dropout_state=dropout_hidden,
                stack_depth=config.sent_rnn_enc_depth,
                transition_depth=config.sent_rnn_enc_transition_depth,
                alternating=True,
                reverse_alternation=True,
                residual_connections=True,
                first_residual_output=1)

    def get_context(self, x, x_mask):
        """ 
        args:
            x: shape - [seqLen, batch_size, state_size], type - numpy.array
            x_mask: shape - [seqLen, batch_size], type - numpy.array
        """
        with tf.variable_scope("forward-stack"):
            fwd_states = self.forward_encoder.forward(x, x_mask)

        with tf.variable_scope("backward-stack"):
            bwd_states = self.backward_encoder.forward(x, x_mask)

        # Concatenate the left-to-right and the right-to-left states, in that
        # order. This is for compatibility with models that were trained with
        # the Theano version.
        stack_depth = len(self.forward_encoder.grus)
        if stack_depth % 2 == 0:
            concat_states = tf.concat([bwd_states, fwd_states], axis=2)
        else:
            concat_states = tf.concat([fwd_states, bwd_states], axis=2)
        return concat_states


class ClfModelInputs(object):
    def __init__(self, config):
        # variable dimensions
        sent_len, word_len, batch_size = None, None, None

        self.x = tf.placeholder(
            name='x',
            shape=(config.factors, sent_len, word_len, batch_size),
            dtype=tf.int32)

        self.x_mask = tf.placeholder(
            name='x_mask',
            shape=(sent_len, word_len, batch_size),
            dtype=tf.float32)

        self.y = tf.placeholder(
            name='y',
            shape=(batch_size),
            dtype=tf.int32)

        self.y_mask = tf.placeholder(
            name='y_mask',
            shape=(batch_size),
            dtype=tf.int32)

        self.training = tf.placeholder_with_default(
            False,
            name='training',
            shape=())


class RNNModelClf(RNNModel):
    def __init__(self, config):
        self.inputs = ClfModelInputs(config)
        # dynamic value
        # batch_size is the alias of the number of sections
        _, sent_len, word_len, batch_size = tf.unstack(tf.shape(self.inputs.x))
        # break section boundary
        word_input = tf.reshape(
            tf.transpose(self.inputs.x, (0, 2, 1, 3)),   # n_factor * sent_len * word_len * n_sec -> n_factor * word_len * sent_len * n_sec
            (config.factors, word_len, sent_len * batch_size))
        word_input_mask = tf.reshape(
            tf.transpose(self.inputs.x_mask, (1, 0, 2)),   # sent_len * word_len * n_sec ->  word_len * sent_len * n_sec
            (word_len, sent_len * batch_size))

        dropout_source = None
        if config.rnn_use_dropout and config.rnn_dropout_source > 0.0:
            def dropout_source(x):
                return tf.layers.dropout(
                    x, noise_shape=(tf.shape(x)[0], tf.shape(x)[1], 1),
                    rate=config.rnn_dropout_source,
                    training=self.inputs.training)

        # Dropout functions for use within FF, GRU, and attention layers.
        # We use Gal and Ghahramani (2016)-style dropout, so these functions
        # will be used to create 2D dropout masks that are reused at every
        # timestep.
        dropout_embedding, dropout_hidden = None, None
        if config.rnn_use_dropout and config.rnn_dropout_embedding > 0.0:
            def dropout_embedding(e):
                return tf.layers.dropout(e, noise_shape=tf.shape(e),
                                         rate=config.rnn_dropout_embedding,
                                         training=self.inputs.training)
        if config.rnn_use_dropout and config.rnn_dropout_hidden > 0.0:
            def dropout_hidden(h):
                return tf.layers.dropout(h, noise_shape=tf.shape(h),
                                         rate=config.rnn_dropout_hidden,
                                         training=self.inputs.training)

        # word-level encoder which is shared with NMT
        with tf.variable_scope("encoder"):
            # the input of Encoder put batch_size as its last dimension
            self.word_encoder = WordLevelEncoder(config, sent_len * batch_size, dropout_source, 
                                                 dropout_embedding, dropout_hidden)
            self.word_level_ctx, _ = self.word_encoder.get_context(word_input, word_input_mask)
            # word_level_ctx - a simple concate of bi-direction GRU states
            # [time_steps, batch_size, 2 * state_size]

            word_encoder_stack_depth = len(self.word_encoder.forward_encoder.grus)
            if word_encoder_stack_depth % 2 == 0:
                # [batch_size, 2 * state_size]
                word_level_query = tf.concat(
                    [self.word_level_ctx[0, :, :config.word_rnn_state_size],   # the last state of bwd
                     self.word_level_ctx[-1, :, config.word_rnn_state_size:]], # the last state of fwd
                     axis=1)
            else:
                word_level_query = tf.concat(
                    [self.word_level_ctx[-1, :, :config.word_rnn_state_size],  # the last state of fwd 
                     self.word_level_ctx[0, :, config.word_rnn_state_size:]],  # the last state of bwd
                     axis=1)

        with tf.variable_scope("attention"):
            self.word_attn_layer = MyAttentionStep(
                                        context=self.word_level_ctx,
                                        context_state_size=2*config.word_rnn_state_size,          # the size of context
                                        context_mask=word_input_mask,
                                        state_size=2*config.word_rnn_state_size,                  # the size of query vector
                                        hidden_size=2*config.word_rnn_state_size,                
                                        use_layer_norm=config.rnn_layer_normalization,   
                                        dropout_context=dropout_hidden,
                                        dropout_state=dropout_hidden)
            # word_attn [batch_size * sent_len, state_size * 2]
            # self.word_attn_context, self.scores = self.word_attn_layer.forward(word_level_query)
            self.word_attn_context, self.scores = self.word_attn_layer.forward(word_level_query)
        # reshape context vector, always batch channel last
        # sent_input = tf.reshape(word_level_query, (sent_len, batch_size, word_level_query.shape[-1]))
        sent_input = tf.reshape(self.word_attn_context, (sent_len, batch_size, self.word_attn_context.shape[-1]))
        sent_input_mask = tf.sign(tf.reduce_sum(self.inputs.x_mask, axis=1))
        
        # sent-level encoder
        with tf.variable_scope("sent-encoder"):
            self.sent_encoder = SentLevelEncoder(config, batch_size, dropout_hidden, dropout_hidden)
            self.sent_level_ctx = self.sent_encoder.get_context(sent_input, sent_input_mask)

            sent_encoder_stack_depth = len(self.sent_encoder.forward_encoder.grus)
            if sent_encoder_stack_depth % 2 == 0:
                sent_level_query = tf.concat(
                    [self.sent_level_ctx[0, :, :config.sent_rnn_state_size],   # the last state of bwd
                     self.sent_level_ctx[-1, :, config.sent_rnn_state_size:]], # the last state of fwd
                     axis=1)
            else:
                sent_level_query = tf.concat(
                    [self.sent_level_ctx[-1, :, :config.sent_rnn_state_size],  # the last state of fwd 
                     self.sent_level_ctx[0, :, config.sent_rnn_state_size:]],  # the last state of bwd
                     axis=1)

        with tf.variable_scope("sent-attention"):
            # sent_attn [batch_size, state_size * 2]
            self.sent_attn_layer = MyAttentionStep(
                context=self.sent_level_ctx, 
                context_state_size=2*config.sent_rnn_state_size,
                context_mask=sent_input_mask,
                state_size=2*config.sent_rnn_state_size,
                hidden_size=2*config.sent_rnn_state_size,
                use_layer_norm=config.rnn_layer_normalization,
                dropout_context=dropout_hidden,
                dropout_state=dropout_hidden)
            
            # now batch channel first
            self.sent_attn_context, _ = self.sent_attn_layer.forward(sent_level_query)
        
        with tf.variable_scope("fully-connect-1"):
            self.fc_layer_1 = layers.FeedForwardLayer(in_size=2*config.sent_rnn_state_size, out_size=config.sec_repr_size, batch_size=batch_size)
            self.sec_repr = self.fc_layer_1.forward(self.sent_attn_context)

        with tf.variable_scope("fully-connect-2"):
            self.fc_layer_2 = layers.FeedForwardLayer(in_size=config.sec_repr_size, out_size=9, batch_size=batch_size)
            # logits: [1, batch_size, class_num]
            self.logits = tf.expand_dims(self.fc_layer_2.forward(self.sec_repr), axis=0, name='logits')
            self._preds = tf.argmax(tf.squeeze(self.logits, axis=0), axis=1, name='preds')

        with tf.variable_scope("loss"):
            # we can view the output label as a one-token sentence
            self.loss_layer = layers.Masked_cross_entropy_loss(
                self.inputs.y, self.inputs.y_mask, config.label_smoothing,
                training=self.inputs.training)
            self._loss_per_sec = self.loss_layer.forward(self.logits)
            self._loss = tf.reduce_mean(self._loss_per_sec, keepdims=False)

    @property
    def loss_per_sec(self):
        return self._loss_per_sec

    @property
    def loss(self):
        return self._loss
    
    @property
    def preds(self):
        return self._preds

class MemNet(object):
    pass