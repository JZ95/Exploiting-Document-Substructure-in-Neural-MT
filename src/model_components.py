import tensorflow as tf
import layers

class Encoder(object):
    """ Construct a bi-directional GRU encoder.
    """
    def __init__(self, input_size, state_size, batch_size, 
                 use_layer_norm,
                 dropout_input, dropout_state,
                 stack_depth, transition_depth):

        with tf.variable_scope("forward-stack"):
            self.forward_encoder = layers.GRUStack(
                input_size=input_size,
                state_size=state_size,
                batch_size=batch_size,
                use_layer_norm=use_layer_norm,
                dropout_input=dropout_input,
                dropout_state=dropout_state,
                stack_depth=stack_depth,
                transition_depth=transition_depth,
                alternating=True,
                residual_connections=True,
                first_residual_output=1)

        with tf.variable_scope("backward-stack"):
            self.backward_encoder = layers.GRUStack(
                input_size=input_size,
                state_size=state_size,
                batch_size=batch_size,
                use_layer_norm=use_layer_norm,
                dropout_input=dropout_input,
                dropout_state=dropout_state,
                stack_depth=stack_depth,
                transition_depth=transition_depth,
                alternating=True,
                reverse_alternation=True,
                residual_connections=True,
                first_residual_output=1)

    def get_context(self, x, x_mask):
        """ 
        args:
            x: shape - [factors, seqLen, batch_size], type - numpy.array
            x_mask: shape - [seqLen, batch_size], type - numpy.array
        returns:
            concat_states: shape - [time_step, batch_size, state_size * 2]
        """
        with tf.variable_scope("forward-stack"):
            fwd_states = self.forward_encoder.forward(x, x_mask)

        with tf.variable_scope("backward-stack"):
            bwd_states = self.backward_encoder.forward(x, x_mask)

        stack_depth = len(self.forward_encoder.grus)
        if stack_depth % 2 == 0:
            concat_states = tf.concat([bwd_states, fwd_states], axis=2)
        else:
            concat_states = tf.concat([fwd_states, bwd_states], axis=2)
        return concat_states


class WordLevelEncoder(Encoder):
    def __init__(self, config, sent_num,
                 dropout_source, dropout_embedding, dropout_hidden):
        self.dropout_source = dropout_source

        with tf.variable_scope("embedding"):
            self.emb_layer = layers.EmbeddingLayer(config.source_vocab_sizes,
                                                   config.dim_per_factor)

        super(WordLevelEncoder, self).__init__(config.embedding_size, config.word_rnn_state_size, sent_num,
                                               config.rnn_layer_normalization,
                                               dropout_embedding, dropout_hidden,
                                               config.word_rnn_enc_depth, config.word_rnn_enc_transition_depth)

    def get_context(self, x, x_mask):
        """ 
        args:
            x: shape - [factors, seqLen, batch_size], type - numpy.array
            x_mask: shape - [seqLen, batch_size], type - numpy.array
        returns:
            concat_states: shape - [time_step, batch_size, state_size * 2]
            embs: shape - [seqLen, batch_size, embed_size]
        """
        with tf.variable_scope("embedding"):
            embs = self.emb_layer.forward(x)
            if self.dropout_source != None:
                embs = self.dropout_source(embs)
        
        return super(WordLevelEncoder, self).get_context(embs, x_mask), embs


class SentLevelEncoder(Encoder):
    def __init__(self, config, sec_num, dropout_hidden):
        super(SentLevelEncoder, self).__init__(2*config.word_rnn_state_size, config.sent_rnn_state_size, sec_num,
                                               config.rnn_layer_normalization,
                                               dropout_hidden, dropout_hidden,
                                               config.sent_rnn_enc_depth, config.sent_rnn_enc_transition_depth)


class VanillaDecoder(object):
    def __init__(self, config, context, x_embs, x_mask,
                 dropout_target, dropout_embedding, dropout_hidden,
                 encoder_embedding_layer=None):

        self._prepare_decoder(config, context, x_embs, x_mask,
                 dropout_target, dropout_embedding, dropout_hidden,
                 encoder_embedding_layer)
        
        batch_size = tf.shape(x_mask)[1]
        with tf.variable_scope("next_word_predictor"):
            W = None
            if config.tie_decoder_embeddings:
                W = self.y_emb_layer.get_embeddings(factor=0)
                W = tf.transpose(W)
            self.predictor = VanillaPredictor(config, batch_size,
                                              dropout_embedding, dropout_hidden,
                                              hidden_to_logits_W=W)

    def _prepare_decoder(self, config, context, x_embs, x_mask,
                 dropout_target, dropout_embedding, dropout_hidden,
                 encoder_embedding_layer=None):
        self.dropout_target = dropout_target
        batch_size = tf.shape(x_mask)[1]

        with tf.variable_scope("initial_state_constructor"):
            context_sum = tf.reduce_sum(
                            context * tf.expand_dims(x_mask, axis=2),
                            axis=0)
            context_mean = tf.div_no_nan(
                                context_sum, 
                                tf.expand_dims(tf.reduce_sum(x_mask, axis=0), 
                                axis=1))
            self.init_state_layer = layers.FeedForwardLayer(
                in_size=config.word_rnn_state_size * 2,
                out_size=config.word_rnn_state_size,
                batch_size=batch_size,
                use_layer_norm=config.rnn_layer_normalization,
                dropout_input=dropout_hidden)
            self.init_state = self.init_state_layer.forward(context_mean)
            self.x_embs = x_embs

            self.translation_maxlen = config.translation_maxlen
            self.embedding_size = config.target_embedding_size
            self.state_size = config.word_rnn_state_size
            self.target_vocab_size = config.target_vocab_size

        with tf.variable_scope("embedding"):
            if encoder_embedding_layer == None:
                self.y_emb_layer = layers.EmbeddingLayer(
                    vocabulary_sizes=[config.target_vocab_size],
                    dim_per_factor=[config.target_embedding_size])
            else:
                self.y_emb_layer = encoder_embedding_layer

        with tf.variable_scope("base"):
            with tf.variable_scope("gru0"):
                self.grustep1 = layers.GRUStep(
                    input_size=config.target_embedding_size,
                    state_size=config.word_rnn_state_size,
                    batch_size=batch_size,
                    use_layer_norm=config.rnn_layer_normalization,
                    dropout_input=dropout_embedding,
                    dropout_state=dropout_hidden)
            with tf.variable_scope("attention"):
                self.attstep = layers.AttentionStep(
                    context=context,
                    context_state_size=2*config.word_rnn_state_size,
                    context_mask=x_mask,
                    state_size=config.word_rnn_state_size,
                    hidden_size=2*config.word_rnn_state_size,
                    use_layer_norm=config.rnn_layer_normalization,
                    dropout_context=dropout_hidden,
                    dropout_state=dropout_hidden)
            self.grustep2 = layers.DeepTransitionGRUStep(
                input_size=2*config.word_rnn_state_size,
                state_size=config.word_rnn_state_size,
                batch_size=batch_size,
                use_layer_norm=config.rnn_layer_normalization,
                dropout_input=dropout_hidden,
                dropout_state=dropout_hidden,
                transition_depth=config.rnn_dec_base_transition_depth-1,
                var_scope_fn=lambda i: "gru{0}".format(i+1))

        with tf.variable_scope("high"):
            if config.rnn_dec_depth == 1:
                self.high_gru_stack = None
            else:
                self.high_gru_stack = layers.GRUStack(
                    input_size=config.word_rnn_state_size,
                    state_size=config.word_rnn_state_size,
                    batch_size=batch_size,
                    use_layer_norm=config.rnn_layer_normalization,
                    dropout_input=dropout_hidden,
                    dropout_state=dropout_hidden,
                    stack_depth=config.rnn_dec_depth-1,
                    transition_depth=config.rnn_dec_high_transition_depth,
                    context_state_size=(2*config.word_rnn_state_size if config.rnn_dec_deep_context else 0),
                    residual_connections=True,
                    first_residual_output=0)

        if config.rnn_lexical_model:
            with tf.variable_scope("lexical"):
                self.lexical_layer = layers.LexicalModel(
                    in_size=config.embedding_size,
                    out_size=config.embedding_size,
                    batch_size=batch_size,
                    use_layer_norm=config.rnn_layer_normalization,
                    dropout_embedding=dropout_embedding,
                    dropout_hidden=dropout_hidden)
        else:
            self.lexical_layer = None

    def _prepare_logits(self, y):
        with tf.variable_scope("y_embeddings_layer"):
            y_but_last = tf.slice(y, [0,0], [tf.shape(y)[0]-1, -1])
            y_embs = self.y_emb_layer.forward(y_but_last, factor=0)
            if self.dropout_target != None:
                y_embs = self.dropout_target(y_embs)
            y_embs = tf.pad(y_embs,
                            mode='CONSTANT',
                            paddings=[[1,0],[0,0],[0,0]]) # prepend zeros

        init_attended_context = tf.zeros([tf.shape(self.init_state)[0], self.state_size*2])
        init_att_alphas = tf.zeros([tf.shape(self.x_embs)[0], tf.shape(self.x_embs)[1]])
        init_state_att_ctx = (self.init_state, init_attended_context, init_att_alphas)
        gates_x, proposal_x = self.grustep1.precompute_from_x(y_embs)
        
        def step_fn(prev, x):
            prev_state = prev[0]
            prev_att_ctx = prev[1]
            prev_lexical_state = prev[2]
            gates_x2d = x[0]
            proposal_x2d = x[1]
            state = self.grustep1.forward(
                        prev_state,
                        gates_x=gates_x2d,
                        proposal_x=proposal_x2d)
            att_ctx, att_alphas = self.attstep.forward(state)
            state = self.grustep2.forward(state, att_ctx)
            return (state, att_ctx, att_alphas)

        layer = layers.RecurrentLayer(initial_state=init_state_att_ctx,
                                      step_fn=step_fn)
        states, attended_states, attention_weights = layer.forward((gates_x, proposal_x))

        if self.high_gru_stack != None:
            states = self.high_gru_stack.forward(
                states,
                context_layer=(attended_states if self.high_gru_stack.context_state_size > 0 else None),
                init_state=self.init_state)

        if self.lexical_layer is not None:
            lexical_states = self.lexical_layer.forward(self.x_embs, attention_weights, multi_step=True)
        else:
            lexical_states = None
        
        return y_embs, states, attended_states, lexical_states

    def score(self, y):
        y_embs, states, attended_states, lexical_states = self._prepare_logits(y)
        logits = self.predictor.get_logits(y_embs, states, attended_states, lexical_states, multi_step=True)
        return logits


class AdvancedDecoder(VanillaDecoder):
    def __init__(self, config, context, x_embs, x_mask,
                 dropout_target, dropout_embedding, dropout_hidden,
                 encoder_embedding_layer=None):
        self._prepare_decoder(config, context, x_embs, x_mask,
                              dropout_target, dropout_embedding, dropout_hidden,
                              encoder_embedding_layer)
        batch_size = tf.shape(x_mask)[1]
        with tf.variable_scope("next_word_predictor"):
            W = None
            if config.tie_decoder_embeddings:
                W = self.y_emb_layer.get_embeddings(factor=0)
                W = tf.transpose(W)
            self.predictor = AdvancedPredictor(config, batch_size,
                                               dropout_embedding, dropout_hidden,
                                               hidden_to_logits_W=W)

    def score(self, y, sec_repr):
        y_embs, states, attended_states, lexical_states = self._prepare_logits(y)
        logits = self.predictor.get_logits(y_embs, states, attended_states, lexical_states, sec_repr, multi_step=True)
        return logits


class VanillaPredictor(object):
    def __init__(self, config, batch_size, dropout_embedding, dropout_hidden, hidden_to_logits_W=None):
        self.config = config

        with tf.variable_scope("prev_emb_to_hidden"):
            self.prev_emb_to_hidden = layers.FeedForwardLayer(
                in_size=config.target_embedding_size,
                out_size=config.target_embedding_size,
                batch_size=batch_size,
                non_linearity=lambda y: y,
                use_layer_norm=config.rnn_layer_normalization,
                dropout_input=dropout_embedding)

        with tf.variable_scope("state_to_hidden"):
            self.state_to_hidden = layers.FeedForwardLayer(
                in_size=config.word_rnn_state_size,
                out_size=config.target_embedding_size,
                batch_size=batch_size,
                non_linearity=lambda y: y,
                use_layer_norm=config.rnn_layer_normalization,
                dropout_input=dropout_hidden)

        with tf.variable_scope("attended_context_to_hidden"):
            self.att_ctx_to_hidden = layers.FeedForwardLayer(
                in_size=2 * config.word_rnn_state_size,
                out_size=config.target_embedding_size,
                batch_size=batch_size,
                non_linearity=lambda y: y,
                use_layer_norm=config.rnn_layer_normalization,
                dropout_input=dropout_hidden)

        if config.output_hidden_activation == 'prelu':
            with tf.variable_scope("hidden_prelu"):
                self.hidden_prelu = layers.PReLU(in_size=config.target_embedding_size)

        with tf.variable_scope("hidden_to_logits"):
            self.hidden_to_logits = layers.FeedForwardLayer(
                in_size=config.target_embedding_size,
                out_size=config.target_vocab_size,
                batch_size=batch_size,
                non_linearity=lambda y: y,
                W=hidden_to_logits_W,
                dropout_input=dropout_embedding)

        if config.rnn_lexical_model:
            with tf.variable_scope("lexical_to_logits"):
                self.lexical_to_logits = layers.FeedForwardLayer(
                                in_size=config.target_embedding_size,
                                out_size=config.target_vocab_size,
                                batch_size=batch_size,
                                non_linearity=lambda y: y,
                                dropout_input=dropout_embedding)

    def _compute_hidden(self, y_embs, states, attended_states, multi_step):
        with tf.variable_scope("prev_emb_to_hidden"):
            hidden_emb = self.prev_emb_to_hidden.forward(y_embs, input_is_3d=multi_step)
            
        with tf.variable_scope("state_to_hidden"):
            hidden_state = self.state_to_hidden.forward(states, input_is_3d=multi_step)

        with tf.variable_scope("attended_context_to_hidden"):
            hidden_att_ctx = self.att_ctx_to_hidden.forward(attended_states,input_is_3d=multi_step)
        
        return hidden_emb + hidden_state + hidden_att_ctx

    def _apply_nonlinearity(self, x):
        if self.config.output_hidden_activation == 'tanh':
            return tf.tanh(x)
        elif self.config.output_hidden_activation == 'relu':
            return tf.nn.relu(x)
        elif self.config.output_hidden_activation == 'prelu':
            return self.hidden_prelu.forward(x)
        elif self.config.output_hidden_activation == 'linear':
            return x
        else:
            assert False, 'Unknown output activation function "%s"' % self.config.output_hidden_activation


    def get_logits(self, y_embs, states, attended_states, lexical_states, multi_step=True):
        hidden = self._compute_hidden(y_embs, states, attended_states, multi_step)
        hidden = self._apply_nonlinearity(hidden)

        with tf.variable_scope("hidden_to_logits"):
            logits = self.hidden_to_logits.forward(hidden, input_is_3d=multi_step)

        if self.config.rnn_lexical_model:
            with tf.variable_scope("lexical_to_logits"):
                logits += self.lexical_to_logits.forward(lexical_states, input_is_3d=multi_step)

        return logits


class AdvancedPredictor(VanillaPredictor):
    def __init__(self, config, batch_size, dropout_embedding, dropout_hidden, hidden_to_logits_W=None):
        self.config = config
        super(AdvancedPredictor, self).__init__(config, batch_size, dropout_embedding, dropout_hidden, hidden_to_logits_W)

        with tf.variable_scope("clf_state_to_hidden"):
            self.clf_state_to_hidden = layers.FeedForwardLayer(
                in_size=config.sec_repr_size,
                out_size=config.target_embedding_size,
                batch_size=batch_size,
                non_linearity=lambda y: y,
                use_layer_norm=config.rnn_layer_normalization,
                dropout_input=dropout_hidden)
    
    def get_logits(self, y_embs, states, attended_states, lexical_states, clf_states, multi_step=True):
        """ add additional text classifier representation for each section into the computation for logits
        """
        hidden = self._compute_hidden(y_embs, states, attended_states, multi_step)

        with tf.variable_scope("clf_state_to_hidden"):
            hidden_clf_state = self.clf_state_to_hidden.forward(clf_states, input_is_3d=False)
        
        hidden = hidden + tf.nn.sigmoid(hidden) * hidden_clf_state
        hidden = self._apply_nonlinearity(hidden)

        with tf.variable_scope("hidden_to_logits"):
            logits = self.hidden_to_logits.forward(hidden, input_is_3d=multi_step)

        if self.config.rnn_lexical_model:
            with tf.variable_scope("lexical_to_logits"):
                logits += self.lexical_to_logits.forward(lexical_states, input_is_3d=multi_step)

        return logits
