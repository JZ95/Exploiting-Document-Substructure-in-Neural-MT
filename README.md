# Exploiting Predictable Document Sub-structure in NMT

## Intro


## Model Architecutre
The model consists of a Seq2Seq Attentional NMT and HAN (Hierarchical Attention Network) text classifier. The hidden state within the HAN model can get involeved in the computation of logits for predicting the next target word in NMT, thus helping widen the context for NMT.
![](/imgs/model-arch.png)

## Training strategy
1. Train the baseline NMT (i.e., sent-independent NMT)
2. Train text clf (reusing the word embedding and word-level encoder in the baseline-NMT)
3. Finetuning the context-aware NMT on top of CLF model under both NMT and CLF loss function

## Usage


## Performance


## Nematus
This project is based on nematus v0.4, specifically the [commit 75a168d247e50a746a717be0ac514e7c314246d3](https://github.com/EdinburghNLP/nematus/tree/75a168d247e50a746a717be0ac514e7c314246d3). The support for Transformer Model, Server Translator, MAP training, rescore, ensemble is removed for simplicity.