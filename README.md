Exploiting Predictable Document Sub-structure in NMT
-------

Intro
-------

Model Architecutre
-------

Training strategy
-------
1. train baseline NMT (sent-independent)
2. train text clf (reusing the word embedding and Encoder in the baseline-NMT)
3. Finetuning the sub-topic informed NMT

Why not runing multi-task training (i.e. NMT and TEXT CLF) at once?
Primary Reason: GPU memory limitation


Usage
-------
```
cd nematus
python setup.py develop
```

NOTES
-------
This project uses Nematus v0.4, specifically the [commit 75a168d247e50a746a717be0ac514e7c314246d3](https://github.com/EdinburghNLP/nematus/tree/75a168d247e50a746a717be0ac514e7c314246d3). You are free to replace the dir `nematus` with the lateset version, but the compatibility is not guaranteed.